{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d769166f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/starsdliu/Library/CloudStorage/OneDrive-JohnsHopkins/Hopkins Academics/Classes/Machine Learning-Deep Learning/Final Project/Scripts/Ensemble'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62137f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 (main, Jul  5 2023, 08:54:11) [Clang 14.0.6 ]\n",
      "1.24.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f60f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.0.0\r\n",
      "aiobotocore @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_71xswk40o_/croot/aiobotocore_1682537536268/work\r\n",
      "aiofiles @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_f56ag8l7kr/croot/aiofiles_1683773599608/work\r\n",
      "aiohttp @ file:///Users/cbousseau/work/recipes/ci_py311/aiohttp_1677926054700/work\r\n",
      "aioitertools @ file:///tmp/build/80754af9/aioitertools_1607109665762/work\r\n",
      "aiosignal @ file:///tmp/build/80754af9/aiosignal_1637843061372/work\r\n",
      "aiosqlite @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_3d75lecab1/croot/aiosqlite_1683773918307/work\r\n",
      "alabaster @ file:///home/ktietz/src/ci/alabaster_1611921544520/work\r\n",
      "anaconda-anon-usage @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_6dviz_3v0e/croot/anaconda-anon-usage_1695305101164/work\r\n",
      "anaconda-catalogs @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_e8tmw882qa/croot/anaconda-catalogs_1685727305051/work\r\n",
      "anaconda-client==1.12.0\r\n",
      "anaconda-cloud-auth @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_c3pmk6onxv/croot/anaconda-cloud-auth_1694462077833/work\r\n",
      "anaconda-navigator @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1cm5bladz0/croot/anaconda-navigator_1695236701001/work\r\n",
      "anaconda-project @ file:///Users/cbousseau/work/recipes/ci_py311/anaconda-project_1677964558977/work\r\n",
      "anyio @ file:///Users/cbousseau/work/recipes/ci_py311/anyio_1677917528418/work/dist\r\n",
      "appdirs==1.4.4\r\n",
      "applaunchservices @ file:///Users/cbousseau/work/recipes/ci_py311/applaunchservices_1677955996025/work\r\n",
      "appnope @ file:///Users/cbousseau/work/recipes/ci_py311/appnope_1677917710869/work\r\n",
      "appscript @ file:///Users/cbousseau/work/recipes/ci_py311/appscript_1677956964648/work\r\n",
      "argon2-cffi @ file:///opt/conda/conda-bld/argon2-cffi_1645000214183/work\r\n",
      "argon2-cffi-bindings @ file:///Users/cbousseau/work/recipes/ci_py311/argon2-cffi-bindings_1677915727169/work\r\n",
      "arrow @ file:///Users/cbousseau/work/recipes/ci_py311/arrow_1677931434012/work\r\n",
      "astroid @ file:///Users/cbousseau/work/recipes/ci_py311/astroid_1677926110661/work\r\n",
      "astropy @ file:///Users/cbousseau/work/recipes/ci_py311_2/astropy_1678994125362/work\r\n",
      "asttokens @ file:///opt/conda/conda-bld/asttokens_1646925590279/work\r\n",
      "astunparse==1.6.3\r\n",
      "async-timeout @ file:///Users/cbousseau/work/recipes/ci_py311/async-timeout_1677925030615/work\r\n",
      "atomicwrites==1.4.0\r\n",
      "attrs @ file:///Users/cbousseau/work/recipes/ci_py311/attrs_1677906550284/work\r\n",
      "Automat @ file:///tmp/build/80754af9/automat_1600298431173/work\r\n",
      "autopep8 @ file:///opt/conda/conda-bld/autopep8_1650463822033/work\r\n",
      "Babel @ file:///Users/cbousseau/work/recipes/ci_py311/babel_1677920677615/work\r\n",
      "backcall @ file:///home/ktietz/src/ci/backcall_1611930011877/work\r\n",
      "backports.functools-lru-cache @ file:///tmp/build/80754af9/backports.functools_lru_cache_1618170165463/work\r\n",
      "backports.tempfile @ file:///home/linux1/recipes/ci/backports.tempfile_1610991236607/work\r\n",
      "backports.weakref==1.0.post1\r\n",
      "bcrypt @ file:///Users/cbousseau/work/recipes/ci_py311/bcrypt_1677931459811/work\r\n",
      "beautifulsoup4 @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_fa78jvo_0n/croot/beautifulsoup4-split_1681493044306/work\r\n",
      "binaryornot @ file:///tmp/build/80754af9/binaryornot_1617751525010/work\r\n",
      "black @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_29yzn6blpk/croot/black_1680737263429/work\r\n",
      "bleach @ file:///opt/conda/conda-bld/bleach_1641577558959/work\r\n",
      "bokeh @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_7552pi2mmm/croot/bokeh_1690546093040/work\r\n",
      "boltons @ file:///Users/cbousseau/work/recipes/ci_py311/boltons_1677965141748/work\r\n",
      "botocore @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_45ad_8onrk/croot/botocore_1682528014542/work\r\n",
      "Bottleneck @ file:///Users/cbousseau/work/recipes/ci_py311/bottleneck_1677925122241/work\r\n",
      "brotlipy==0.7.0\r\n",
      "cachetools==5.3.2\r\n",
      "certifi @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_3bweqb8byn/croot/certifi_1690232244779/work/certifi\r\n",
      "cffi @ file:///Users/cbousseau/work/recipes/ci_py311/cffi_1677903595907/work\r\n",
      "chardet @ file:///Users/cbousseau/work/recipes/ci_py311/chardet_1677931647221/work\r\n",
      "charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work\r\n",
      "click @ file:///Users/cbousseau/work/recipes/ci_py311/click_1677920889666/work\r\n",
      "cloudpickle @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_da31odypvn/croot/cloudpickle_1683040013858/work\r\n",
      "clyent==1.2.2\r\n",
      "colorama @ file:///Users/cbousseau/work/recipes/ci_py311/colorama_1677925183444/work\r\n",
      "colorcet @ file:///Users/cbousseau/work/recipes/ci_py311/colorcet_1677936559489/work\r\n",
      "comm @ file:///Users/cbousseau/work/recipes/ci_py311/comm_1677919149446/work\r\n",
      "conda @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_47t5xq9oq5/croot/conda_1694544078292/work\r\n",
      "conda-build @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_f467bc822c/croot/conda-build_1690381801495/work\r\n",
      "conda-content-trust @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5324skqvu9/croot/conda-content-trust_1693490622873/work\r\n",
      "conda-libmamba-solver @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_48qgqxi1ys/croot/conda-libmamba-solver_1685032355439/work/src\r\n",
      "conda-pack @ file:///tmp/build/80754af9/conda-pack_1611163042455/work\r\n",
      "conda-package-handling @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_fc4cx8vjhj/croot/conda-package-handling_1690999937094/work\r\n",
      "conda-repo-cli==1.0.41\r\n",
      "conda-token @ file:///Users/paulyim/miniconda3/envs/c3i/conda-bld/conda-token_1662660369760/work\r\n",
      "conda-verify==3.4.2\r\n",
      "conda_index @ file:///Users/cbousseau/work/recipes/ci_py311/conda-index_1677970042779/work\r\n",
      "conda_package_streaming @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_aecpaup22q/croot/conda-package-streaming_1690987978274/work\r\n",
      "constantly==15.1.0\r\n",
      "contourpy @ file:///Users/cbousseau/work/recipes/ci_py311/contourpy_1677925208456/work\r\n",
      "cookiecutter @ file:///opt/conda/conda-bld/cookiecutter_1649151442564/work\r\n",
      "cryptography @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_a3xvfhudbc/croot/cryptography_1689373681671/work\r\n",
      "cssselect==1.1.0\r\n",
      "cycler @ file:///tmp/build/80754af9/cycler_1637851556182/work\r\n",
      "cytoolz @ file:///Users/cbousseau/work/recipes/ci_py311/cytoolz_1677931761799/work\r\n",
      "dask @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_f3jad8spw3/croot/dask-core_1686782923467/work\r\n",
      "datasets @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_5eqct1blor/croot/datasets_1684482935720/work\r\n",
      "datashader @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_8f3fxewux9/croot/datashader_1689587619365/work\r\n",
      "datashape==0.5.4\r\n",
      "debugpy @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_563_nwtkoc/croot/debugpy_1690905063850/work\r\n",
      "decorator @ file:///opt/conda/conda-bld/decorator_1643638310831/work\r\n",
      "defusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work\r\n",
      "diff-match-patch @ file:///Users/ktietz/demo/mc3/conda-bld/diff-match-patch_1630511840874/work\r\n",
      "dill @ file:///Users/cbousseau/work/recipes/ci_py311/dill_1677926161443/work\r\n",
      "distlib==0.3.7\r\n",
      "distributed @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_46b2z6ud_6/croot/distributed_1686866053959/work\r\n",
      "dm-tree==0.1.8\r\n",
      "docstring-to-markdown @ file:///Users/cbousseau/work/recipes/ci_py311/docstring-to-markdown_1677931891483/work\r\n",
      "docutils @ file:///Users/cbousseau/work/recipes/ci_py311/docutils_1677907269557/work\r\n",
      "entrypoints @ file:///Users/cbousseau/work/recipes/ci_py311/entrypoints_1677911798787/work\r\n",
      "et-xmlfile==1.1.0\r\n",
      "executing @ file:///opt/conda/conda-bld/executing_1646925071911/work\r\n",
      "fastjsonschema @ file:///Users/cbousseau/work/recipes/ci_py311_2/python-fastjsonschema_1678996913062/work\r\n",
      "filelock==3.13.1\r\n",
      "flake8 @ file:///Users/cbousseau/work/recipes/ci_py311/flake8_1677931981927/work\r\n",
      "Flask @ file:///Users/cbousseau/work/recipes/ci_py311/flask_1677937489551/work\r\n",
      "flatbuffers==23.5.26\r\n",
      "fonttools==4.25.0\r\n",
      "frozenlist @ file:///Users/cbousseau/work/recipes/ci_py311/frozenlist_1677923867353/work\r\n",
      "fsspec @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b41fwm88q_/croot/fsspec_1682526071956/work\r\n",
      "future @ file:///Users/cbousseau/work/recipes/ci_py311_2/future_1678994664110/work\r\n",
      "gast==0.5.4\r\n",
      "gensim @ file:///Users/cbousseau/work/recipes/ci_py311/gensim_1677971806459/work\r\n",
      "glob2 @ file:///home/linux1/recipes/ci/glob2_1610991677669/work\r\n",
      "gmpy2 @ file:///Users/cbousseau/work/recipes/ci_py311/gmpy2_1677937751357/work\r\n",
      "google-auth==2.25.1\r\n",
      "google-auth-oauthlib==1.1.0\r\n",
      "google-pasta==0.2.0\r\n",
      "graphviz @ file:///Users/cbousseau/work/recipes/ci_py311/python-graphviz_1678001933790/work\r\n",
      "greenlet @ file:///Users/cbousseau/work/recipes/ci_py311/greenlet_1677926210411/work\r\n",
      "grpcio==1.59.3\r\n",
      "h5py @ file:///Users/cbousseau/work/recipes/ci_py311/h5py_1677937901660/work\r\n",
      "HeapDict @ file:///Users/ktietz/demo/mc3/conda-bld/heapdict_1630598515714/work\r\n",
      "holoviews @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_f0kn6h75hh/croot/holoviews_1690477580363/work\r\n",
      "huggingface-hub @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_a3pjdbppt5/croot/huggingface_hub_1686693687731/work\r\n",
      "hvplot @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_30omxxom8t/croot/hvplot_1686021066828/work\r\n",
      "hyperlink @ file:///tmp/build/80754af9/hyperlink_1610130746837/work\r\n",
      "idna @ file:///Users/cbousseau/work/recipes/ci_py311/idna_1677906072337/work\r\n",
      "imagecodecs @ file:///Users/cbousseau/work/recipes/ci_py311_2/imagecodecs_1678994839331/work\r\n",
      "imageio @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_c1sh4q1483/croot/imageio_1687264958491/work\r\n",
      "imagesize @ file:///Users/cbousseau/work/recipes/ci_py311/imagesize_1677932611633/work\r\n",
      "imbalanced-learn @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_26_yvb9nba/croot/imbalanced-learn_1685020915768/work\r\n",
      "importlib-metadata @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_84cgjb624t/croot/importlib-metadata_1678997087058/work\r\n",
      "incremental @ file:///tmp/build/80754af9/incremental_1636629750599/work\r\n",
      "inflection==0.5.1\r\n",
      "iniconfig @ file:///home/linux1/recipes/ci/iniconfig_1610983019677/work\r\n",
      "intake @ file:///Users/cbousseau/work/recipes/ci_py311_2/intake_1678994948878/work\r\n",
      "intervaltree @ file:///Users/ktietz/demo/mc3/conda-bld/intervaltree_1630511889664/work\r\n",
      "ipykernel @ file:///Users/cbousseau/work/recipes/ci_py311/ipykernel_1677921035781/work\r\n",
      "ipython @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_2bkl1d0l7u/croot/ipython_1680701883396/work\r\n",
      "ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work\r\n",
      "ipywidgets @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_f07ugy1hvo/croot/ipywidgets_1679394821999/work\r\n",
      "isort @ file:///tmp/build/80754af9/isort_1628603791788/work\r\n",
      "itemadapter @ file:///tmp/build/80754af9/itemadapter_1626442940632/work\r\n",
      "itemloaders @ file:///opt/conda/conda-bld/itemloaders_1646805235997/work\r\n",
      "itsdangerous @ file:///tmp/build/80754af9/itsdangerous_1621432558163/work\r\n",
      "jaraco.classes @ file:///tmp/build/80754af9/jaraco.classes_1620983179379/work\r\n",
      "jedi @ file:///Users/cbousseau/work/recipes/ci_py311_2/jedi_1678994967789/work\r\n",
      "jellyfish @ file:///Users/cbousseau/work/recipes/ci_py311/jellyfish_1677959705446/work\r\n",
      "Jinja2 @ file:///Users/cbousseau/work/recipes/ci_py311/jinja2_1677916113134/work\r\n",
      "jinja2-time @ file:///opt/conda/conda-bld/jinja2-time_1649251842261/work\r\n",
      "jmespath @ file:///Users/ktietz/demo/mc3/conda-bld/jmespath_1630583964805/work\r\n",
      "joblib @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_a357ltg47g/croot/joblib_1685113093574/work\r\n",
      "json5 @ file:///tmp/build/80754af9/json5_1624432770122/work\r\n",
      "jsonpatch @ file:///tmp/build/80754af9/jsonpatch_1615747632069/work\r\n",
      "jsonpointer==2.1\r\n",
      "jsonschema @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_2a92785vxi/croot/jsonschema_1678983423459/work\r\n",
      "jupyter @ file:///Users/cbousseau/work/recipes/ci_py311/jupyter_1677932849424/work\r\n",
      "jupyter-console @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_62liw5pns2/croot/jupyter_console_1679999641189/work\r\n",
      "jupyter-events @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_23dk3_r5iy/croot/jupyter_events_1684268066707/work\r\n",
      "jupyter-server @ file:///Users/cbousseau/work/recipes/ci_py311/jupyter_server_1677919944873/work\r\n",
      "jupyter-ydoc @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1djmqkjwof/croot/jupyter_ydoc_1683747243427/work\r\n",
      "jupyter_client @ file:///Users/cbousseau/work/recipes/ci_py311/jupyter_client_1677914181590/work\r\n",
      "jupyter_core @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_d1sy1hlz9t/croot/jupyter_core_1679906585151/work\r\n",
      "jupyter_server_fileid @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_a5j1mo_1cs/croot/jupyter_server_fileid_1684273608144/work\r\n",
      "jupyter_server_ydoc @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_47q6o0w705/croot/jupyter_server_ydoc_1686767400324/work\r\n",
      "jupyterlab @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_a5omaxlzc0/croot/jupyterlab_1686179674589/work\r\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\r\n",
      "jupyterlab-widgets @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_65wwn9cwab/croot/jupyterlab_widgets_1679055283460/work\r\n",
      "jupyterlab_server @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_0fmfqwbrd7/croot/jupyterlab_server_1680792517631/work\r\n",
      "keras==2.15.0\r\n",
      "keyring @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_8bd22k84zo/croot/keyring_1678999224442/work\r\n",
      "kiwisolver @ file:///Users/cbousseau/work/recipes/ci_py311/kiwisolver_1677925326358/work\r\n",
      "lazy-object-proxy @ file:///Users/cbousseau/work/recipes/ci_py311/lazy-object-proxy_1677925379420/work\r\n",
      "lazy_loader @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_9c4sl77tg1/croot/lazy_loader_1687264096938/work\r\n",
      "libarchive-c @ file:///tmp/build/80754af9/python-libarchive-c_1617780486945/work\r\n",
      "libclang==16.0.6\r\n",
      "libmambapy @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_ba7rom_bbt/croot/mamba-split_1680092817644/work/libmambapy\r\n",
      "linkify-it-py @ file:///Users/cbousseau/work/recipes/ci_py311/linkify-it-py_1677973036983/work\r\n",
      "llvmlite @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_fcgbla6sbr/croot/llvmlite_1683555144762/work\r\n",
      "lmdb @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_6fumkuh_c0/croot/python-lmdb_1682522347231/work\r\n",
      "locket @ file:///Users/cbousseau/work/recipes/ci_py311/locket_1677925419801/work\r\n",
      "lxml @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_bbjv2ox7t8/croot/lxml_1679646469466/work\r\n",
      "lz4 @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_f0mtitgo6y/croot/lz4_1686063770247/work\r\n",
      "Markdown @ file:///Users/cbousseau/work/recipes/ci_py311/markdown_1677932925449/work\r\n",
      "markdown-it-py @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_43l_4ajkho/croot/markdown-it-py_1684279912406/work\r\n",
      "MarkupSafe @ file:///Users/cbousseau/work/recipes/ci_py311/markupsafe_1677914270710/work\r\n",
      "matplotlib @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_05rfi32zzb/croot/matplotlib-suite_1679593473526/work\r\n",
      "matplotlib-inline @ file:///Users/cbousseau/work/recipes/ci_py311/matplotlib-inline_1677918241899/work\r\n",
      "mccabe @ file:///opt/conda/conda-bld/mccabe_1644221741721/work\r\n",
      "mdit-py-plugins @ file:///Users/cbousseau/work/recipes/ci_py311/mdit-py-plugins_1677995322132/work\r\n",
      "mdurl @ file:///Users/cbousseau/work/recipes/ci_py311/mdurl_1677942260967/work\r\n",
      "mistune @ file:///Users/cbousseau/work/recipes/ci_py311/mistune_1677916714667/work\r\n",
      "ml-dtypes==0.2.0\r\n",
      "more-itertools @ file:///tmp/build/80754af9/more-itertools_1637733554872/work\r\n",
      "mpmath @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_17iu6a8a3m/croot/mpmath_1690848269369/work\r\n",
      "msgpack @ file:///Users/cbousseau/work/recipes/ci_py311/msgpack-python_1677909260136/work\r\n",
      "multidict @ file:///Users/cbousseau/work/recipes/ci_py311/multidict_1677923908690/work\r\n",
      "multipledispatch @ file:///Users/cbousseau/work/recipes/ci_py311/multipledispatch_1677960800437/work\r\n",
      "multiprocess @ file:///Users/cbousseau/work/recipes/ci_py311/multiprocess_1677942297511/work\r\n",
      "munkres==1.1.4\r\n",
      "mypy-extensions==0.4.3\r\n",
      "namex==0.0.7\r\n",
      "navigator-updater==0.4.0\r\n",
      "nbclassic @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_d6oy9w0m3l/croot/nbclassic_1681756176477/work\r\n",
      "nbclient @ file:///Users/cbousseau/work/recipes/ci_py311/nbclient_1677916908988/work\r\n",
      "nbconvert @ file:///Users/cbousseau/work/recipes/ci_py311/nbconvert_1677918402558/work\r\n",
      "nbformat @ file:///Users/cbousseau/work/recipes/ci_py311/nbformat_1677914501406/work\r\n",
      "nest-asyncio @ file:///Users/cbousseau/work/recipes/ci_py311/nest-asyncio_1677912430289/work\r\n",
      "networkx @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b9af3smw_7/croot/networkx_1690562010704/work\r\n",
      "nltk @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_ebiuq9880w/croot/nltk_1688114154971/work\r\n",
      "notebook @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_37zamp3vxi/croot/notebook_1690984825122/work\r\n",
      "notebook_shim @ file:///Users/cbousseau/work/recipes/ci_py311/notebook-shim_1677921216909/work\r\n",
      "numba @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_bar0txilh4/croot/numba_1684245494553/work\r\n",
      "numexpr @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_76yyu1p9jk/croot/numexpr_1683221830860/work\r\n",
      "numpy @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_f9f5xs2fx0/croot/numpy_and_numpy_base_1682520577456/work\r\n",
      "numpydoc @ file:///Users/cbousseau/work/recipes/ci_py311/numpydoc_1677960919550/work\r\n",
      "oauthlib==3.2.2\r\n",
      "openpyxl==3.0.10\r\n",
      "opt-einsum==3.3.0\r\n",
      "packaging @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_e946luvhc3/croot/packaging_1678965323926/work\r\n",
      "pandas==1.5.3\r\n",
      "pandocfilters @ file:///opt/conda/conda-bld/pandocfilters_1643405455980/work\r\n",
      "panel @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_fckdqsngkv/croot/panel_1690822225257/work\r\n",
      "param @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_cfp588uo3i/croot/param_1684915323490/work\r\n",
      "parsel @ file:///Users/cbousseau/work/recipes/ci_py311/parsel_1678068232273/work\r\n",
      "parso @ file:///opt/conda/conda-bld/parso_1641458642106/work\r\n",
      "partd @ file:///opt/conda/conda-bld/partd_1647245470509/work\r\n",
      "pathlib @ file:///Users/ktietz/demo/mc3/conda-bld/pathlib_1629713961906/work\r\n",
      "pathspec @ file:///Users/cbousseau/work/recipes/ci_py311_2/pathspec_1678995598596/work\r\n",
      "patsy==0.5.3\r\n",
      "pep8==1.7.1\r\n",
      "pexpect @ file:///tmp/build/80754af9/pexpect_1605563209008/work\r\n",
      "pickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\r\n",
      "Pillow==9.4.0\r\n",
      "pkce @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_da285fiplp/croot/pkce_1690384839054/work\r\n",
      "pkginfo @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_d1oq9rhye6/croot/pkginfo_1679431178842/work\r\n",
      "platformdirs==3.11.0\r\n",
      "plotly @ file:///Users/cbousseau/work/recipes/ci_py311/plotly_1677953301864/work\r\n",
      "pluggy @ file:///Users/cbousseau/work/recipes/ci_py311/pluggy_1677906980825/work\r\n",
      "ply==3.11\r\n",
      "pooch @ file:///tmp/build/80754af9/pooch_1623324770023/work\r\n",
      "poyo @ file:///tmp/build/80754af9/poyo_1617751526755/work\r\n",
      "prometheus-client @ file:///Users/cbousseau/work/recipes/ci_py311_2/prometheus_client_1678996808082/work\r\n",
      "prompt-toolkit @ file:///Users/cbousseau/work/recipes/ci_py311/prompt-toolkit_1677918689663/work\r\n",
      "Protego @ file:///tmp/build/80754af9/protego_1598657180827/work\r\n",
      "protobuf==4.23.4\r\n",
      "psutil @ file:///Users/cbousseau/work/recipes/ci_py311_2/psutil_1678995687212/work\r\n",
      "ptyprocess @ file:///tmp/build/80754af9/ptyprocess_1609355006118/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\r\n",
      "pure-eval @ file:///opt/conda/conda-bld/pure_eval_1646925070566/work\r\n",
      "py-cpuinfo @ file:///Users/ktietz/demo/mc3/conda-bld/py-cpuinfo_1629480366017/work\r\n",
      "pyarrow==11.0.0\r\n",
      "pyasn1 @ file:///Users/ktietz/demo/mc3/conda-bld/pyasn1_1629708007385/work\r\n",
      "pyasn1-modules==0.2.8\r\n",
      "pycodestyle @ file:///Users/cbousseau/work/recipes/ci_py311/pycodestyle_1677927047034/work\r\n",
      "pycosat @ file:///Users/cbousseau/work/recipes/ci_py311/pycosat_1677933552468/work\r\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\r\n",
      "pyct @ file:///Users/cbousseau/work/recipes/ci_py311/pyct_1677933596803/work\r\n",
      "pycurl==7.45.2\r\n",
      "pydantic @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_35534f0j5u/croot/pydantic_1686125721649/work\r\n",
      "PyDispatcher==2.0.5\r\n",
      "pydocstyle @ file:///Users/cbousseau/work/recipes/ci_py311/pydocstyle_1677933616104/work\r\n",
      "pyerfa @ file:///Users/cbousseau/work/recipes/ci_py311/pyerfa_1677933632816/work\r\n",
      "pyflakes @ file:///Users/cbousseau/work/recipes/ci_py311/pyflakes_1677927066386/work\r\n",
      "Pygments @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_29bs9f_dh9/croot/pygments_1684279974747/work\r\n",
      "PyJWT @ file:///Users/cbousseau/work/recipes/ci_py311/pyjwt_1677933681463/work\r\n",
      "pylint @ file:///Users/cbousseau/work/recipes/ci_py311/pylint_1677933699245/work\r\n",
      "pylint-venv @ file:///Users/cbousseau/work/recipes/ci_py311/pylint-venv_1677961443839/work\r\n",
      "pyls-spyder==0.4.0\r\n",
      "pyobjc-core @ file:///Users/cbousseau/work/recipes/ci_py311/pyobjc-core_1678112643033/work\r\n",
      "pyobjc-framework-Cocoa @ file:///Users/cbousseau/work/recipes/ci_py311/pyobjc-framework-cocoa_1678112805655/work\r\n",
      "pyobjc-framework-CoreServices @ file:///Users/cbousseau/work/recipes/ci_py311/pyobjc-framework-coreservices_1678113537167/work\r\n",
      "pyobjc-framework-FSEvents @ file:///Users/cbousseau/work/recipes/ci_py311/pyobjc-framework-fsevents_1678112996782/work\r\n",
      "pyodbc @ file:///Users/cbousseau/work/recipes/ci_py311/pyodbc_1678001241548/work\r\n",
      "pyOpenSSL @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_b8whqav6qm/croot/pyopenssl_1690223428943/work\r\n",
      "pyparsing @ file:///Users/cbousseau/work/recipes/ci_py311/pyparsing_1677910832141/work\r\n",
      "PyQt5-sip==12.11.0\r\n",
      "pyrsistent @ file:///Users/cbousseau/work/recipes/ci_py311/pyrsistent_1677909782145/work\r\n",
      "PySocks @ file:///Users/cbousseau/work/recipes/ci_py311/pysocks_1677906386870/work\r\n",
      "pytest @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_75ehl8i878/croot/pytest_1690474711033/work\r\n",
      "python-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work\r\n",
      "python-dotenv @ file:///Users/cbousseau/work/recipes/ci_py311/python-dotenv_1677924884036/work\r\n",
      "python-json-logger @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_c3baq2ko4j/croot/python-json-logger_1683823815343/work\r\n",
      "python-lsp-black @ file:///Users/cbousseau/work/recipes/ci_py311/python-lsp-black_1677961743861/work\r\n",
      "python-lsp-jsonrpc==1.0.0\r\n",
      "python-lsp-server @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_73tk9oa5lj/croot/python-lsp-server_1681930403042/work\r\n",
      "python-slugify @ file:///tmp/build/80754af9/python-slugify_1620405669636/work\r\n",
      "python-snappy @ file:///Users/cbousseau/work/recipes/ci_py311/python-snappy_1677954153933/work\r\n",
      "pytoolconfig @ file:///Users/cbousseau/work/recipes/ci_py311/pytoolconfig_1677952331058/work\r\n",
      "pytz @ file:///Users/cbousseau/work/recipes/ci_py311/pytz_1677920497588/work\r\n",
      "pyviz-comms @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_54a2e0jluq/croot/pyviz_comms_1685030719530/work\r\n",
      "PyWavelets @ file:///Users/cbousseau/work/recipes/ci_py311/pywavelets_1677934749412/work\r\n",
      "PyYAML @ file:///Users/cbousseau/work/recipes/ci_py311/pyyaml_1677927153992/work\r\n",
      "pyzmq @ file:///Users/cbousseau/work/recipes/ci_py311/pyzmq_1677912759914/work\r\n",
      "QDarkStyle @ file:///tmp/build/80754af9/qdarkstyle_1617386714626/work\r\n",
      "qstylizer @ file:///Users/cbousseau/work/recipes/ci_py311/qstylizer_1678072198813/work/dist/qstylizer-0.2.2-py2.py3-none-any.whl\r\n",
      "QtAwesome @ file:///Users/cbousseau/work/recipes/ci_py311/qtawesome_1677961781784/work\r\n",
      "qtconsole @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_86g4aht18r/croot/qtconsole_1681394233851/work\r",
      "\r\n",
      "QtPy @ file:///Users/cbousseau/work/recipes/ci_py311/qtpy_1677925820115/work\r\n",
      "queuelib==1.5.0\r\n",
      "regex @ file:///Users/cbousseau/work/recipes/ci_py311/regex_1677961821876/work\r\n",
      "requests @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_54zi68h2nb/croot/requests_1690400233316/work\r\n",
      "requests-file @ file:///Users/ktietz/demo/mc3/conda-bld/requests-file_1629455781986/work\r\n",
      "requests-oauthlib==1.3.1\r\n",
      "requests-toolbelt @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_3fee1fr2ex/croot/requests-toolbelt_1690874011813/work\r\n",
      "responses @ file:///tmp/build/80754af9/responses_1619800270522/work\r\n",
      "rfc3339-validator @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_76ae5cu30h/croot/rfc3339-validator_1683077051957/work\r\n",
      "rfc3986-validator @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_d0l5zd97kt/croot/rfc3986-validator_1683058998431/work\r\n",
      "rich==13.7.0\r\n",
      "rope @ file:///Users/cbousseau/work/recipes/ci_py311/rope_1677934821109/work\r\n",
      "rsa==4.9\r\n",
      "Rtree @ file:///Users/cbousseau/work/recipes/ci_py311/rtree_1677961892694/work\r\n",
      "ruamel-yaml-conda @ file:///Users/cbousseau/work/recipes/ci_py311/ruamel_yaml_1677961911260/work\r\n",
      "ruamel.yaml @ file:///Users/cbousseau/work/recipes/ci_py311/ruamel.yaml_1677934845850/work\r\n",
      "s3fs @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_94qu8izf0w/croot/s3fs_1682551484893/work\r\n",
      "sacremoses @ file:///tmp/build/80754af9/sacremoses_1633107328213/work\r\n",
      "scikit-image @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_12rg0hdzgk/croot/scikit-image_1682528304529/work\r\n",
      "scikit-learn @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_75x_mwrnoj/croot/scikit-learn_1690978919744/work\r\n",
      "scipy==1.10.1\r\n",
      "Scrapy @ file:///Users/cbousseau/work/recipes/ci_py311/scrapy_1678002824834/work\r\n",
      "seaborn @ file:///Users/cbousseau/work/recipes/ci_py311/seaborn_1677961968762/work\r\n",
      "Send2Trash @ file:///tmp/build/80754af9/send2trash_1632406701022/work\r\n",
      "service-identity @ file:///Users/ktietz/demo/mc3/conda-bld/service_identity_1629460757137/work\r\n",
      "sip @ file:///Users/cbousseau/work/recipes/ci_py311/sip_1677923661665/work\r\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\r\n",
      "smart-open @ file:///Users/cbousseau/work/recipes/ci_py311/smart_open_1677955621457/work\r\n",
      "sniffio @ file:///Users/cbousseau/work/recipes/ci_py311/sniffio_1677917188478/work\r\n",
      "snowballstemmer @ file:///tmp/build/80754af9/snowballstemmer_1637937080595/work\r\n",
      "sortedcontainers @ file:///tmp/build/80754af9/sortedcontainers_1623949099177/work\r\n",
      "soupsieve @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_cfrx9hmp3g/croot/soupsieve_1680518480034/work\r\n",
      "Sphinx @ file:///Users/cbousseau/work/recipes/ci_py311/sphinx_1677955655588/work\r\n",
      "sphinxcontrib-applehelp @ file:///home/ktietz/src/ci/sphinxcontrib-applehelp_1611920841464/work\r\n",
      "sphinxcontrib-devhelp @ file:///home/ktietz/src/ci/sphinxcontrib-devhelp_1611920923094/work\r\n",
      "sphinxcontrib-htmlhelp @ file:///tmp/build/80754af9/sphinxcontrib-htmlhelp_1623945626792/work\r\n",
      "sphinxcontrib-jsmath @ file:///home/ktietz/src/ci/sphinxcontrib-jsmath_1611920942228/work\r\n",
      "sphinxcontrib-qthelp @ file:///home/ktietz/src/ci/sphinxcontrib-qthelp_1611921055322/work\r\n",
      "sphinxcontrib-serializinghtml @ file:///tmp/build/80754af9/sphinxcontrib-serializinghtml_1624451540180/work\r\n",
      "spyder @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_f02k_edsgq/croot/spyder_1681934090757/work\r\n",
      "spyder-kernels @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_18_uk_uzcm/croot/spyder-kernels_1681309301436/work\r\n",
      "SQLAlchemy @ file:///Users/cbousseau/work/recipes/ci_py311/sqlalchemy_1677934876727/work\r\n",
      "stack-data @ file:///opt/conda/conda-bld/stack_data_1646927590127/work\r\n",
      "statsmodels @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_d39rlzrllo/croot/statsmodels_1689937269798/work\r\n",
      "sympy @ file:///Users/cbousseau/work/recipes/ci_py311_2/sympy_1678995976952/work\r\n",
      "tables @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_2d3fzevm9d/croot/pytables_1685123231215/work\r\n",
      "tabulate @ file:///Users/cbousseau/work/recipes/ci_py311/tabulate_1678003852126/work\r\n",
      "TBB==0.2\r\n",
      "tblib @ file:///Users/ktietz/demo/mc3/conda-bld/tblib_1629402031467/work\r\n",
      "tenacity @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_0ew5sfng29/croot/tenacity_1682972282256/work\r\n",
      "tensorboard==2.15.1\r\n",
      "tensorboard-data-server==0.7.2\r\n",
      "tensorflow==2.15.0\r\n",
      "tensorflow-estimator==2.15.0\r\n",
      "tensorflow-io-gcs-filesystem==0.34.0\r\n",
      "tensorflow-macos==2.15.0\r\n",
      "termcolor==2.4.0\r\n",
      "terminado @ file:///Users/cbousseau/work/recipes/ci_py311/terminado_1677918849903/work\r\n",
      "text-unidecode @ file:///Users/ktietz/demo/mc3/conda-bld/text-unidecode_1629401354553/work\r\n",
      "textdistance @ file:///tmp/build/80754af9/textdistance_1612461398012/work\r\n",
      "threadpoolctl @ file:///Users/ktietz/demo/mc3/conda-bld/threadpoolctl_1629802263681/work\r\n",
      "three-merge @ file:///tmp/build/80754af9/three-merge_1607553261110/work\r\n",
      "tifffile @ file:///tmp/build/80754af9/tifffile_1627275862826/work\r\n",
      "tinycss2 @ file:///Users/cbousseau/work/recipes/ci_py311/tinycss2_1677917352983/work\r\n",
      "tldextract @ file:///opt/conda/conda-bld/tldextract_1646638314385/work\r\n",
      "tokenizers @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_123g4rizbe/croot/tokenizers_1687191917415/work\r\n",
      "toml @ file:///tmp/build/80754af9/toml_1616166611790/work\r\n",
      "tomlkit @ file:///Users/cbousseau/work/recipes/ci_py311/tomlkit_1677911131859/work\r\n",
      "toolz @ file:///Users/cbousseau/work/recipes/ci_py311/toolz_1677925870232/work\r\n",
      "torch==2.1.0\r\n",
      "torchvision==0.16.0\r\n",
      "tornado @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_28d93aezp2/croot/tornado_1690848278715/work\r\n",
      "tqdm @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_ac7zic_tin/croot/tqdm_1679561870178/work\r\n",
      "traitlets @ file:///Users/cbousseau/work/recipes/ci_py311/traitlets_1677911650502/work\r\n",
      "transformers @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_aecnjml9j7/croot/transformers_1684843867688/work\r\n",
      "Twisted @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_3c_lnc4s5c/croot/twisted_1683796895946/work\r\n",
      "typing_extensions @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1fdywrbp_3/croot/typing_extensions_1690297474455/work\r\n",
      "uc-micro-py @ file:///Users/cbousseau/work/recipes/ci_py311/uc-micro-py_1677963537430/work\r\n",
      "ujson @ file:///Users/cbousseau/work/recipes/ci_py311/ujson_1677927397272/work\r\n",
      "Unidecode @ file:///tmp/build/80754af9/unidecode_1614712377438/work\r\n",
      "urllib3 @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_e9usuw__8i/croot/urllib3_1686166332689/work\r\n",
      "virtualenv==20.24.6\r\n",
      "w3lib @ file:///Users/ktietz/demo/mc3/conda-bld/w3lib_1629359764703/work\r\n",
      "watchdog @ file:///Users/cbousseau/work/recipes/ci_py311/watchdog_1677963700938/work\r\n",
      "wcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work\r\n",
      "webencodings==0.5.1\r\n",
      "websocket-client @ file:///Users/cbousseau/work/recipes/ci_py311/websocket-client_1677918996745/work\r\n",
      "Werkzeug @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_fc9kcczuwd/croot/werkzeug_1679489745296/work\r\n",
      "whatthepatch @ file:///Users/cbousseau/work/recipes/ci_py311/whatthepatch_1677934976505/work\r\n",
      "widgetsnbextension @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_cd_nvw5x1_/croot/widgetsnbextension_1679313872684/work\r\n",
      "wrapt @ file:///Users/cbousseau/work/recipes/ci_py311/wrapt_1677925966862/work\r\n",
      "wurlitzer @ file:///Users/cbousseau/work/recipes/ci_py311/wurlitzer_1677955854875/work\r\n",
      "xarray @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_a14bvvrzzp/croot/xarray_1689041477812/work\r\n",
      "xgboost==2.0.0\r\n",
      "xlwings @ file:///Users/cbousseau/work/recipes/ci_py311_2/xlwings_1678996173448/work\r\n",
      "xxhash @ file:///Users/cbousseau/work/recipes/ci_py311/python-xxhash_1677954188023/work\r\n",
      "xyzservices @ file:///Users/cbousseau/work/recipes/ci_py311/xyzservices_1677927443768/work\r\n",
      "y-py @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_58d555zc6u/croot/y-py_1683555409055/work\r\n",
      "yapf @ file:///tmp/build/80754af9/yapf_1615749224965/work\r\n",
      "yarl @ file:///Users/cbousseau/work/recipes/ci_py311/yarl_1677926011607/work\r\n",
      "ypy-websocket @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_e638ipunz1/croot/ypy-websocket_1684192343550/work\r\n",
      "zict @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_06f3lix4ji/croot/zict_1682698748419/work\r\n",
      "zipp @ file:///Users/cbousseau/work/recipes/ci_py311/zipp_1677907997878/work\r\n",
      "zope.interface @ file:///Users/cbousseau/work/recipes/ci_py311/zope.interface_1678055276546/work\r\n",
      "zstandard @ file:///Users/cbousseau/work/recipes/ci_py311_2/zstandard_1678996192313/work\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9853478f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6567e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from scipy import stats\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import model_zoo\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Tuple, Sequence\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "#squeezenet\n",
    "from torchvision.transforms._presets import ImageClassification\n",
    "from torchvision.utils import _log_api_usage_once\n",
    "from torchvision.models._api import register_model, Weights, WeightsEnum\n",
    "from torchvision.models._meta import _IMAGENET_CATEGORIES\n",
    "from torchvision.models._utils import _ovewrite_named_param, handle_legacy_interface\n",
    "\n",
    "#unique to mobilenet\n",
    "from torchvision.ops.misc import Conv2dNormActivation\n",
    "from torchvision.models._utils import _make_divisible\n",
    "\n",
    "#time functions\n",
    "import time\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299920e",
   "metadata": {},
   "source": [
    "# Import Custom Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35278493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "add_path = '/Users/starsdliu/Library/CloudStorage/OneDrive-JohnsHopkins' + \\\n",
    "            '/Hopkins Academics/Classes/Machine Learning-Deep Learning/Final Project/Scripts'\n",
    "sys.path.insert(0, add_path)\n",
    "#models\n",
    "from SqueezeNet.snet import *\n",
    "from MobileNet_V2.mnet import *\n",
    "from ResNet.rnet import *\n",
    "from InceptionNet.inet import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e30458b",
   "metadata": {},
   "source": [
    "# Additional Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db19442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View image\n",
    "def imshow(img):\n",
    "    # img = img / 2 + 0.5     # unnormalize if your images were normalized\n",
    "    img = img.cpu()\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_performance(train_acc, test_acc, train_loss, test_loss, epochs):\n",
    "\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))  # Adjust the size as needed\n",
    "    fig.suptitle('Training and Testing Performance')\n",
    "\n",
    "    # train acc\n",
    "    axes[0].plot(epochs_range, train_acc, label='Training Accuracy', marker='o')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Training Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # test acc\n",
    "    axes[1].plot(epochs_range, test_acc, label='Testing Accuracy', color='orange', marker='o')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_title('Testing Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # train loss\n",
    "    axes[2].plot(epochs_range, train_loss, label='Training Loss', marker='o', linestyle='--')\n",
    "    axes[2].set_xlabel('Epochs')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].set_title('Training Loss')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "\n",
    "    # test loss\n",
    "    axes[3].plot(epochs_range, test_loss, label='Testing Loss', color='orange', marker='o', linestyle='--')\n",
    "    axes[3].set_xlabel('Epochs')\n",
    "    axes[3].set_title('Testing Loss')\n",
    "    axes[3].legend()\n",
    "    axes[3].grid(True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce53603",
   "metadata": {},
   "source": [
    "# Additional Directory Paths Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4622ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['InceptionNet', 'ResNet', 'SqueezeNet', 'MobileNet_V2']\n",
      "There are 4 models: InceptionNet, ResNet, SqueezeNet, MobileNet_V2.\n"
     ]
    }
   ],
   "source": [
    "# Base directory where the model directories are located\n",
    "base_dir = '../'\n",
    "\n",
    "# Get a list of all subdirectories in the base directory\n",
    "model_directories = next(os.walk(base_dir))[1]\n",
    "model_directories = model_directories[1:-1]\n",
    "print(model_directories)\n",
    "# Dictionary to store the loaded models\n",
    "loaded_models = {}\n",
    "\n",
    "# Loop through each subdirectory and load the model\n",
    "for model_dir in model_directories:\n",
    "    # Path to the 'Saved Models' directory\n",
    "    saved_models_path = os.path.join(base_dir, model_dir, 'savedModels')\n",
    "\n",
    "    # Check if 'Saved Models' directory exists\n",
    "    if os.path.isdir(saved_models_path):\n",
    "        # List all '.pth' files in the 'Saved Models' directory\n",
    "        model_files = [f for f in os.listdir(saved_models_path) if f.endswith('.pth')]\n",
    "\n",
    "        if model_files:\n",
    "            # Load the model (assuming you want the first '.pth' file found)\n",
    "            model_file_path = os.path.join(saved_models_path, model_files[0])\n",
    "            loaded_models[model_dir] = torch.load(model_file_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Now loaded_models dictionary contains all the loaded models\n",
    "print(f'There are {len(loaded_models)} models: {\", \".join(list(loaded_models.keys()))}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141ddf5",
   "metadata": {},
   "source": [
    "# GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d25b1883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_boole = torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if gpu_boole else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "387559db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['InceptionNet', 'ResNet', 'SqueezeNet', 'MobileNet_V2'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_models.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f151fa",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17e47232",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=65, bias=True)\n",
      "Loaded: InceptionNet\n",
      "Linear(in_features=512, out_features=65, bias=True)\n",
      "Loaded: ResNet\n",
      "Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Conv2d(512, 65, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      ")\n",
      "Loaded: SqueezeNet\n",
      "Sequential(\n",
      "  (0): Dropout(p=0.2, inplace=False)\n",
      "  (1): Linear(in_features=1280, out_features=65, bias=True)\n",
      ")\n",
      "Loaded: MobileNet_V2\n"
     ]
    }
   ],
   "source": [
    "num_classes = 65\n",
    "\n",
    "for modelName in loaded_models:\n",
    "    if modelName == 'SqueezeNet':\n",
    "        #Initialize\n",
    "        snet = squeezenet1_0(pretrained = False)\n",
    "        # Replace the final layer\n",
    "        snet.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "        #Load model dictionary\n",
    "        snet.load_state_dict(loaded_models[modelName])\n",
    "        #to GPU device\n",
    "        snet = snet.to(device)\n",
    "        print(snet.classifier)\n",
    "        \n",
    "    elif modelName == 'MobileNet_V2':\n",
    "        #Initialize\n",
    "        mnet = mobilenet_v2(pretrained = False)\n",
    "        # Replace the final layer\n",
    "        num_input_features = mnet.classifier[1].in_features\n",
    "        mnet.classifier[1] = nn.Linear(num_input_features, num_classes)\n",
    "        #Load model dictionary\n",
    "        mnet.load_state_dict(loaded_models[modelName])\n",
    "        #to GPU device\n",
    "        mnet = mnet.to(device)\n",
    "        print(mnet.classifier)\n",
    "                \n",
    "    elif modelName == 'InceptionNet':\n",
    "        #Initialize\n",
    "        inet = inception(pretrained = False)\n",
    "        # Replace the final layer\n",
    "        num_auglogit_features = inet.AuxLogits.fc.in_features\n",
    "        inet.AuxLogits.fc = nn.Linear(num_auglogit_features, num_classes)\n",
    "        num_input_features = inet.fc.in_features\n",
    "        inet.fc = nn.Linear(num_input_features, num_classes)\n",
    "        #Load model dictionary\n",
    "        inet.load_state_dict(loaded_models[modelName])\n",
    "        #to GPU device\n",
    "        inet = inet.to(device)\n",
    "        print(inet.fc)\n",
    "        \n",
    "    elif modelName == 'ResNet':\n",
    "        #Initialize\n",
    "        rnet = resnet18(pretrained = False)\n",
    "        # Replace the final layer\n",
    "        num_input_features = rnet.class_classifier.in_features\n",
    "        rnet.class_classifier = nn.Linear(num_input_features, num_classes)\n",
    "        #Load model dictionary\n",
    "        rnet.load_state_dict(loaded_models[modelName])\n",
    "        #to GPU device\n",
    "        rnet = rnet.to(device)\n",
    "        print(rnet.class_classifier)\n",
    "\n",
    "    print(f'Loaded: {modelName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520da63",
   "metadata": {},
   "source": [
    "# Load Target Source Data: Real-World Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6889dccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 87 and total sample size: 4350\n"
     ]
    }
   ],
   "source": [
    "# Data loaders\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "#     transforms.RandomHorizontalFlip(p=0.5),\n",
    "#     transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),\n",
    "#     transforms.RandomGrayscale(0.1),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "#     transforms.Lambda(lambda x: x.float()),\n",
    "# ])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.Lambda(lambda x: x.float()),\n",
    "])\n",
    "\n",
    "directory = '../../OfficeHomeDataset_10072016'\n",
    "# concatenated_datasets = []\n",
    "# total_size = 0\n",
    "\n",
    "# for name in [\"Art\",\"Clipart\",\"Product\"]:\n",
    "#     dataset = datasets.ImageFolder(\n",
    "#         f'{directory}/{name}',\n",
    "#         transform=train_transform\n",
    "#     )\n",
    "#     total_size += len(dataset)\n",
    "#     concatenated_datasets.append(dataset)\n",
    "\n",
    "# combined_dataset = ConcatDataset(concatenated_datasets)\n",
    "# indices = list(range(total_size))\n",
    "# random.shuffle(indices)\n",
    "# random_subset = Subset(combined_dataset, indices)\n",
    "# source_loader = DataLoader(random_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "target_dataset = datasets.ImageFolder(\n",
    "    f'{directory}/Real World',\n",
    "    transform=val_transform)\n",
    "target_loader = DataLoader(target_dataset , batch_size=50, shuffle=True)\n",
    "\n",
    "np.random.seed(610)\n",
    "\n",
    "# Get the total number of samples in the dataset\n",
    "total_samples = len(target_loader.dataset)\n",
    "\n",
    "# Generate a random selection of indices for half of the dataset\n",
    "total_size = int(np.round(total_samples * 0.9985))\n",
    "selected_indices = np.random.choice(total_samples, size=total_size, replace=False)\n",
    "\n",
    "# Create a Subset of the original dataset using the selected indices\n",
    "subset_dataset = Subset(target_loader.dataset, selected_indices)\n",
    "\n",
    "# Create a new DataLoader with the subset\n",
    "subset_data_loader = DataLoader(subset_dataset, batch_size=target_loader.batch_size, shuffle=False)\n",
    "print(f'Number of batches: {len(subset_data_loader)} and total sample size: {len(subset_data_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09c88f",
   "metadata": {},
   "source": [
    "# Forward Pass through data for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f4ffe24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['InceptionNet', 'ResNet', 'SqueezeNet', 'MobileNet_V2'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "497e703d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/starsdliu/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch execution time:  1.0896079540252686 seconds\n",
      "Next 5 batches execution time:  1.0896871089935303 seconds\n",
      "Next 5 batches execution time:  5.319436073303223 seconds\n",
      "Next 5 batches execution time:  4.666278123855591 seconds\n",
      "Next 5 batches execution time:  5.324376106262207 seconds\n",
      "Next 5 batches execution time:  5.541074275970459 seconds\n",
      "Next 5 batches execution time:  5.784801006317139 seconds\n",
      "Next 5 batches execution time:  6.04597806930542 seconds\n",
      "Next 5 batches execution time:  5.781419038772583 seconds\n",
      "Next 5 batches execution time:  4.988924980163574 seconds\n",
      "Next 5 batches execution time:  5.755149841308594 seconds\n",
      "Next 5 batches execution time:  4.906090021133423 seconds\n",
      "Next 5 batches execution time:  4.546435832977295 seconds\n",
      "Next 5 batches execution time:  4.907794952392578 seconds\n",
      "Next 5 batches execution time:  4.708524942398071 seconds\n",
      "Next 5 batches execution time:  5.5316033363342285 seconds\n",
      "Next 5 batches execution time:  5.002686977386475 seconds\n",
      "Next 5 batches execution time:  4.732933759689331 seconds\n",
      "Next 5 batches execution time:  5.114546060562134 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/starsdliu/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch execution time:  0.9605381488800049 seconds\n",
      "Next 5 batches execution time:  0.9606053829193115 seconds\n",
      "Next 5 batches execution time:  4.9277613162994385 seconds\n",
      "Next 5 batches execution time:  4.595412015914917 seconds\n",
      "Next 5 batches execution time:  5.065808057785034 seconds\n",
      "Next 5 batches execution time:  5.125703811645508 seconds\n",
      "Next 5 batches execution time:  5.32752799987793 seconds\n",
      "Next 5 batches execution time:  5.407053232192993 seconds\n",
      "Next 5 batches execution time:  5.3987531661987305 seconds\n",
      "Next 5 batches execution time:  4.56702995300293 seconds\n",
      "Next 5 batches execution time:  5.507785081863403 seconds\n",
      "Next 5 batches execution time:  4.734426975250244 seconds\n",
      "Next 5 batches execution time:  4.6050028800964355 seconds\n",
      "Next 5 batches execution time:  5.0218422412872314 seconds\n",
      "Next 5 batches execution time:  4.764357328414917 seconds\n",
      "Next 5 batches execution time:  5.384819030761719 seconds\n",
      "Next 5 batches execution time:  4.739837884902954 seconds\n",
      "Next 5 batches execution time:  4.582748174667358 seconds\n",
      "Next 5 batches execution time:  4.759531736373901 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/starsdliu/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch execution time:  1.1318511962890625 seconds\n",
      "Next 5 batches execution time:  1.131917953491211 seconds\n",
      "Next 5 batches execution time:  5.041935920715332 seconds\n",
      "Next 5 batches execution time:  4.662607908248901 seconds\n",
      "Next 5 batches execution time:  5.20816707611084 seconds\n",
      "Next 5 batches execution time:  4.915511131286621 seconds\n",
      "Next 5 batches execution time:  5.242824077606201 seconds\n",
      "Next 5 batches execution time:  5.3976709842681885 seconds\n",
      "Next 5 batches execution time:  5.317471027374268 seconds\n",
      "Next 5 batches execution time:  4.5647218227386475 seconds\n",
      "Next 5 batches execution time:  5.477139949798584 seconds\n",
      "Next 5 batches execution time:  4.645182847976685 seconds\n",
      "Next 5 batches execution time:  4.262573957443237 seconds\n",
      "Next 5 batches execution time:  4.643371105194092 seconds\n",
      "Next 5 batches execution time:  4.448462724685669 seconds\n",
      "Next 5 batches execution time:  5.349560976028442 seconds\n",
      "Next 5 batches execution time:  4.5827929973602295 seconds\n",
      "Next 5 batches execution time:  4.53303599357605 seconds\n",
      "Next 5 batches execution time:  4.839696168899536 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/starsdliu/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch execution time:  1.0139050483703613 seconds\n",
      "Next 5 batches execution time:  1.0139758586883545 seconds\n",
      "Next 5 batches execution time:  5.095124959945679 seconds\n",
      "Next 5 batches execution time:  4.871720790863037 seconds\n",
      "Next 5 batches execution time:  5.022112131118774 seconds\n",
      "Next 5 batches execution time:  4.926438808441162 seconds\n",
      "Next 5 batches execution time:  5.243192672729492 seconds\n",
      "Next 5 batches execution time:  5.297950983047485 seconds\n",
      "Next 5 batches execution time:  5.345341205596924 seconds\n",
      "Next 5 batches execution time:  4.6888439655303955 seconds\n",
      "Next 5 batches execution time:  5.598867177963257 seconds\n",
      "Next 5 batches execution time:  4.767737865447998 seconds\n",
      "Next 5 batches execution time:  4.412515878677368 seconds\n",
      "Next 5 batches execution time:  4.745900869369507 seconds\n",
      "Next 5 batches execution time:  4.5986647605896 seconds\n",
      "Next 5 batches execution time:  5.347089767456055 seconds\n",
      "Next 5 batches execution time:  4.802919864654541 seconds\n",
      "Next 5 batches execution time:  4.625339984893799 seconds\n",
      "Next 5 batches execution time:  4.8974950313568115 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_list = [inet, rnet, snet, mnet]\n",
    "# List to store predictions for each image\n",
    "\n",
    "df_holder = pd.DataFrame()\n",
    "models_metrics_holder = {}\n",
    "\n",
    "for model_idx, model in enumerate(model_list):\n",
    "    # Dictionaries to store metrics for each model\n",
    "#     model_losses = {}\n",
    "#     model_accuracies = {}\n",
    "#     model_f1_scores = {}\n",
    "#     model_aucs = {}\n",
    "\n",
    "    all_predictions = []\n",
    "    \n",
    "    modelName = list(loaded_models.keys())[model_idx]\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Initialize metrics\n",
    "    epoch_test_loss = 0.0\n",
    "    epoch_test_acc = 0.0\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    all_outputs_proba = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for it, batch in enumerate(subset_data_loader):\n",
    "        \n",
    "#         print(it)\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            class_logit = model(inputs)\n",
    "            test_loss = criterion(class_logit, labels)\n",
    "            _, cls_pred = class_logit.max(dim=1)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            epoch_test_loss += test_loss.item()\n",
    "            epoch_test_acc += torch.sum(cls_pred == labels.data).item()\n",
    "\n",
    "            # For F1 score and AUC, store all outputs and labels\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            \n",
    "            #auroc\n",
    "            probabilities = torch.softmax(class_logit, dim=1).cpu().numpy()\n",
    "            all_outputs_proba.extend(probabilities)\n",
    "            \n",
    "            #f1 score\n",
    "            all_outputs.extend(cls_pred.cpu().numpy())\n",
    "\n",
    "            # Store predictions for each image\n",
    "            predictions = class_logit.cpu().numpy()\n",
    "            for i in range(probabilities.shape[0]):  # Loop over images in the batch\n",
    "                all_predictions.append({'true_label': labels[i].item()})\n",
    "\n",
    "                for class_idx in range(probabilities.shape[1]):  # Loop over classes\n",
    "                    column_name = f'{modelName}_{model_idx + 1}_class_{class_idx + 1}'\n",
    "                    all_predictions[it * len(inputs) + i][column_name] = probabilities[i, class_idx]\n",
    "\n",
    "        if it == 0:\n",
    "            end_time = time.time()\n",
    "            print(\"First batch execution time: \", end_time - start_time, \"seconds\")\n",
    "#             break\n",
    "            \n",
    "        if (it % 5 == 0):\n",
    "            end_time = time.time()\n",
    "            print(\"Next 5 batches execution time: \", end_time - start_time, \"seconds\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    # Calculate and store average loss and accuracy\n",
    "    num_samples = len(subset_data_loader.dataset)\n",
    "    model_loss = epoch_test_loss / num_samples\n",
    "    model_accuracies = epoch_test_acc / num_samples\n",
    "\n",
    "    # Calculate and store F1 score and AUC\n",
    "    # Convert all_targets to one-hot encoded format\n",
    "    y_true_one_hot = label_binarize(all_targets, classes=range(num_classes))\n",
    "\n",
    "    # Ensure all_outputs_proba is a numpy array\n",
    "    all_outputs_proba = np.array(all_outputs_proba)\n",
    "    f1score = f1_score(all_targets, all_outputs, average='weighted')\n",
    "    auroc = roc_auc_score(y_true_one_hot, all_outputs_proba, multi_class='ovr')\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    df = pd.DataFrame(all_predictions)\n",
    "    df_holder = pd.concat([df_holder, df], axis = 1)\n",
    "    # Convert metrics to JSON (if needed)\n",
    "    combined_metrics = {\n",
    "        'losses': model_loss,\n",
    "        'accuracies': model_accuracies,\n",
    "        'f1_scores': f1score,\n",
    "        'aucs': auroc\n",
    "    }\n",
    "    models_metrics_holder[modelName] = combined_metrics\n",
    "\n",
    "#save csv\n",
    "csv_file_path = './Model Results/snet_mnet_inet_rnet_predictions_labels_full.csv'\n",
    "df_holder.to_csv(csv_file_path, index=False)\n",
    "\n",
    "#save json\n",
    "json_file_path = './Model Results/snet_mnet_inet_rnet_models_metrics_full.json'\n",
    "with open(json_file_path, 'w') as file:\n",
    "    json.dump(models_metrics_holder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e37b9770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>SqueezeNet_1_class_1</th>\n",
       "      <th>SqueezeNet_1_class_2</th>\n",
       "      <th>SqueezeNet_1_class_3</th>\n",
       "      <th>SqueezeNet_1_class_4</th>\n",
       "      <th>SqueezeNet_1_class_5</th>\n",
       "      <th>SqueezeNet_1_class_6</th>\n",
       "      <th>SqueezeNet_1_class_7</th>\n",
       "      <th>SqueezeNet_1_class_8</th>\n",
       "      <th>SqueezeNet_1_class_9</th>\n",
       "      <th>...</th>\n",
       "      <th>MobileNet_V2_2_class_56</th>\n",
       "      <th>MobileNet_V2_2_class_57</th>\n",
       "      <th>MobileNet_V2_2_class_58</th>\n",
       "      <th>MobileNet_V2_2_class_59</th>\n",
       "      <th>MobileNet_V2_2_class_60</th>\n",
       "      <th>MobileNet_V2_2_class_61</th>\n",
       "      <th>MobileNet_V2_2_class_62</th>\n",
       "      <th>MobileNet_V2_2_class_63</th>\n",
       "      <th>MobileNet_V2_2_class_64</th>\n",
       "      <th>MobileNet_V2_2_class_65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>7.262563e-10</td>\n",
       "      <td>2.498528e-12</td>\n",
       "      <td>2.975894e-12</td>\n",
       "      <td>5.339139e-13</td>\n",
       "      <td>2.320832e-12</td>\n",
       "      <td>1.710723e-11</td>\n",
       "      <td>1.857067e-12</td>\n",
       "      <td>2.197785e-12</td>\n",
       "      <td>4.356368e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.223780e-07</td>\n",
       "      <td>1.766251e-07</td>\n",
       "      <td>3.727825e-10</td>\n",
       "      <td>3.734709e-09</td>\n",
       "      <td>7.203157e-08</td>\n",
       "      <td>5.545787e-09</td>\n",
       "      <td>2.515807e-08</td>\n",
       "      <td>1.021777e-07</td>\n",
       "      <td>6.160776e-08</td>\n",
       "      <td>2.210914e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>5.276681e-07</td>\n",
       "      <td>1.501765e-12</td>\n",
       "      <td>2.381423e-09</td>\n",
       "      <td>7.190358e-14</td>\n",
       "      <td>1.051137e-13</td>\n",
       "      <td>4.978161e-12</td>\n",
       "      <td>4.373954e-13</td>\n",
       "      <td>9.998670e-01</td>\n",
       "      <td>1.048098e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>9.050146e-07</td>\n",
       "      <td>1.253019e-04</td>\n",
       "      <td>4.477683e-05</td>\n",
       "      <td>7.737304e-05</td>\n",
       "      <td>1.824193e-06</td>\n",
       "      <td>1.730803e-04</td>\n",
       "      <td>1.259049e-08</td>\n",
       "      <td>3.632041e-06</td>\n",
       "      <td>3.792483e-06</td>\n",
       "      <td>4.162863e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>1.808928e-04</td>\n",
       "      <td>1.800425e-03</td>\n",
       "      <td>1.503360e-04</td>\n",
       "      <td>3.704068e-02</td>\n",
       "      <td>1.165294e-03</td>\n",
       "      <td>3.865846e-04</td>\n",
       "      <td>3.679395e-04</td>\n",
       "      <td>7.975586e-04</td>\n",
       "      <td>3.718957e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>3.995181e-05</td>\n",
       "      <td>4.432637e-04</td>\n",
       "      <td>1.994744e-03</td>\n",
       "      <td>2.892468e-02</td>\n",
       "      <td>1.874315e-02</td>\n",
       "      <td>9.240152e-03</td>\n",
       "      <td>2.608575e-03</td>\n",
       "      <td>3.587585e-04</td>\n",
       "      <td>1.729638e-04</td>\n",
       "      <td>2.334915e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8.362006e-05</td>\n",
       "      <td>4.662251e-05</td>\n",
       "      <td>1.358395e-01</td>\n",
       "      <td>3.964900e-05</td>\n",
       "      <td>4.118890e-05</td>\n",
       "      <td>5.945881e-03</td>\n",
       "      <td>6.279061e-01</td>\n",
       "      <td>1.925077e-04</td>\n",
       "      <td>4.927367e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>7.329953e-03</td>\n",
       "      <td>5.770333e-04</td>\n",
       "      <td>3.681877e-04</td>\n",
       "      <td>6.223977e-03</td>\n",
       "      <td>3.169568e-04</td>\n",
       "      <td>2.465677e-03</td>\n",
       "      <td>1.331138e-03</td>\n",
       "      <td>3.494845e-04</td>\n",
       "      <td>3.974282e-01</td>\n",
       "      <td>9.930512e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>2.600521e-06</td>\n",
       "      <td>2.690635e-06</td>\n",
       "      <td>4.031113e-06</td>\n",
       "      <td>4.464919e-05</td>\n",
       "      <td>6.762485e-07</td>\n",
       "      <td>1.058991e-02</td>\n",
       "      <td>2.383457e-06</td>\n",
       "      <td>6.834535e-07</td>\n",
       "      <td>1.856201e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.291772e-04</td>\n",
       "      <td>9.482015e-06</td>\n",
       "      <td>9.991421e-06</td>\n",
       "      <td>2.754351e-05</td>\n",
       "      <td>7.820977e-04</td>\n",
       "      <td>4.904593e-03</td>\n",
       "      <td>1.390067e-04</td>\n",
       "      <td>2.840934e-03</td>\n",
       "      <td>1.216361e-04</td>\n",
       "      <td>4.244205e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>6</td>\n",
       "      <td>1.653436e-09</td>\n",
       "      <td>2.351998e-07</td>\n",
       "      <td>3.472163e-09</td>\n",
       "      <td>3.388399e-10</td>\n",
       "      <td>1.137893e-10</td>\n",
       "      <td>4.627875e-06</td>\n",
       "      <td>9.999915e-01</td>\n",
       "      <td>4.279730e-09</td>\n",
       "      <td>1.429219e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.974962e-05</td>\n",
       "      <td>2.613673e-07</td>\n",
       "      <td>4.650745e-07</td>\n",
       "      <td>2.484209e-07</td>\n",
       "      <td>4.874250e-09</td>\n",
       "      <td>1.562865e-09</td>\n",
       "      <td>9.991079e-08</td>\n",
       "      <td>3.729173e-08</td>\n",
       "      <td>2.617291e-02</td>\n",
       "      <td>1.909165e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>21</td>\n",
       "      <td>3.680644e-03</td>\n",
       "      <td>3.905456e-01</td>\n",
       "      <td>2.883671e-04</td>\n",
       "      <td>6.569243e-05</td>\n",
       "      <td>5.253129e-05</td>\n",
       "      <td>1.502243e-02</td>\n",
       "      <td>2.964745e-02</td>\n",
       "      <td>1.017704e-02</td>\n",
       "      <td>2.788324e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.676223e-01</td>\n",
       "      <td>1.715256e-04</td>\n",
       "      <td>2.886083e-03</td>\n",
       "      <td>1.363019e-07</td>\n",
       "      <td>7.178323e-06</td>\n",
       "      <td>2.857074e-04</td>\n",
       "      <td>4.080899e-03</td>\n",
       "      <td>7.621653e-04</td>\n",
       "      <td>3.375111e-02</td>\n",
       "      <td>7.631975e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>36</td>\n",
       "      <td>2.493720e-04</td>\n",
       "      <td>4.805806e-05</td>\n",
       "      <td>1.726472e-03</td>\n",
       "      <td>7.085905e-05</td>\n",
       "      <td>3.730945e-05</td>\n",
       "      <td>1.366549e-04</td>\n",
       "      <td>1.370465e-04</td>\n",
       "      <td>4.228563e-04</td>\n",
       "      <td>4.388458e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>9.199030e-08</td>\n",
       "      <td>1.114061e-06</td>\n",
       "      <td>3.346800e-05</td>\n",
       "      <td>6.640473e-07</td>\n",
       "      <td>9.402354e-08</td>\n",
       "      <td>3.096250e-06</td>\n",
       "      <td>1.271762e-06</td>\n",
       "      <td>5.899246e-07</td>\n",
       "      <td>3.273698e-07</td>\n",
       "      <td>1.960391e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>9</td>\n",
       "      <td>5.002904e-06</td>\n",
       "      <td>5.207412e-07</td>\n",
       "      <td>1.835346e-06</td>\n",
       "      <td>1.636948e-07</td>\n",
       "      <td>9.024228e-06</td>\n",
       "      <td>3.555494e-05</td>\n",
       "      <td>3.863099e-08</td>\n",
       "      <td>1.278523e-07</td>\n",
       "      <td>1.645629e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>8.633162e-04</td>\n",
       "      <td>7.405291e-04</td>\n",
       "      <td>8.604111e-05</td>\n",
       "      <td>1.674609e-04</td>\n",
       "      <td>1.211314e-04</td>\n",
       "      <td>2.499685e-05</td>\n",
       "      <td>5.173074e-05</td>\n",
       "      <td>4.500997e-04</td>\n",
       "      <td>4.772073e-05</td>\n",
       "      <td>1.203780e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>63</td>\n",
       "      <td>8.723522e-03</td>\n",
       "      <td>2.035603e-01</td>\n",
       "      <td>4.776013e-04</td>\n",
       "      <td>1.250037e-02</td>\n",
       "      <td>1.003050e-03</td>\n",
       "      <td>2.208260e-03</td>\n",
       "      <td>3.396436e-02</td>\n",
       "      <td>4.223077e-03</td>\n",
       "      <td>1.187753e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.540072e-03</td>\n",
       "      <td>2.606308e-04</td>\n",
       "      <td>6.949893e-06</td>\n",
       "      <td>1.141677e-02</td>\n",
       "      <td>2.308521e-04</td>\n",
       "      <td>3.864342e-03</td>\n",
       "      <td>2.421903e-06</td>\n",
       "      <td>1.779990e-02</td>\n",
       "      <td>1.758284e-02</td>\n",
       "      <td>6.809386e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4350 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      true_label  SqueezeNet_1_class_1  SqueezeNet_1_class_2  \\\n",
       "0              9          7.262563e-10          2.498528e-12   \n",
       "1              7          5.276681e-07          1.501765e-12   \n",
       "2             18          1.808928e-04          1.800425e-03   \n",
       "3              6          8.362006e-05          4.662251e-05   \n",
       "4             31          2.600521e-06          2.690635e-06   \n",
       "...          ...                   ...                   ...   \n",
       "4345           6          1.653436e-09          2.351998e-07   \n",
       "4346          21          3.680644e-03          3.905456e-01   \n",
       "4347          36          2.493720e-04          4.805806e-05   \n",
       "4348           9          5.002904e-06          5.207412e-07   \n",
       "4349          63          8.723522e-03          2.035603e-01   \n",
       "\n",
       "      SqueezeNet_1_class_3  SqueezeNet_1_class_4  SqueezeNet_1_class_5  \\\n",
       "0             2.975894e-12          5.339139e-13          2.320832e-12   \n",
       "1             2.381423e-09          7.190358e-14          1.051137e-13   \n",
       "2             1.503360e-04          3.704068e-02          1.165294e-03   \n",
       "3             1.358395e-01          3.964900e-05          4.118890e-05   \n",
       "4             4.031113e-06          4.464919e-05          6.762485e-07   \n",
       "...                    ...                   ...                   ...   \n",
       "4345          3.472163e-09          3.388399e-10          1.137893e-10   \n",
       "4346          2.883671e-04          6.569243e-05          5.253129e-05   \n",
       "4347          1.726472e-03          7.085905e-05          3.730945e-05   \n",
       "4348          1.835346e-06          1.636948e-07          9.024228e-06   \n",
       "4349          4.776013e-04          1.250037e-02          1.003050e-03   \n",
       "\n",
       "      SqueezeNet_1_class_6  SqueezeNet_1_class_7  SqueezeNet_1_class_8  \\\n",
       "0             1.710723e-11          1.857067e-12          2.197785e-12   \n",
       "1             4.978161e-12          4.373954e-13          9.998670e-01   \n",
       "2             3.865846e-04          3.679395e-04          7.975586e-04   \n",
       "3             5.945881e-03          6.279061e-01          1.925077e-04   \n",
       "4             1.058991e-02          2.383457e-06          6.834535e-07   \n",
       "...                    ...                   ...                   ...   \n",
       "4345          4.627875e-06          9.999915e-01          4.279730e-09   \n",
       "4346          1.502243e-02          2.964745e-02          1.017704e-02   \n",
       "4347          1.366549e-04          1.370465e-04          4.228563e-04   \n",
       "4348          3.555494e-05          3.863099e-08          1.278523e-07   \n",
       "4349          2.208260e-03          3.396436e-02          4.223077e-03   \n",
       "\n",
       "      SqueezeNet_1_class_9  ...  MobileNet_V2_2_class_56  \\\n",
       "0             4.356368e-12  ...             1.223780e-07   \n",
       "1             1.048098e-06  ...             9.050146e-07   \n",
       "2             3.718957e-04  ...             3.995181e-05   \n",
       "3             4.927367e-04  ...             7.329953e-03   \n",
       "4             1.856201e-06  ...             1.291772e-04   \n",
       "...                    ...  ...                      ...   \n",
       "4345          1.429219e-10  ...             5.974962e-05   \n",
       "4346          2.788324e-04  ...             2.676223e-01   \n",
       "4347          4.388458e-05  ...             9.199030e-08   \n",
       "4348          1.645629e-07  ...             8.633162e-04   \n",
       "4349          1.187753e-03  ...             1.540072e-03   \n",
       "\n",
       "      MobileNet_V2_2_class_57  MobileNet_V2_2_class_58  \\\n",
       "0                1.766251e-07             3.727825e-10   \n",
       "1                1.253019e-04             4.477683e-05   \n",
       "2                4.432637e-04             1.994744e-03   \n",
       "3                5.770333e-04             3.681877e-04   \n",
       "4                9.482015e-06             9.991421e-06   \n",
       "...                       ...                      ...   \n",
       "4345             2.613673e-07             4.650745e-07   \n",
       "4346             1.715256e-04             2.886083e-03   \n",
       "4347             1.114061e-06             3.346800e-05   \n",
       "4348             7.405291e-04             8.604111e-05   \n",
       "4349             2.606308e-04             6.949893e-06   \n",
       "\n",
       "      MobileNet_V2_2_class_59  MobileNet_V2_2_class_60  \\\n",
       "0                3.734709e-09             7.203157e-08   \n",
       "1                7.737304e-05             1.824193e-06   \n",
       "2                2.892468e-02             1.874315e-02   \n",
       "3                6.223977e-03             3.169568e-04   \n",
       "4                2.754351e-05             7.820977e-04   \n",
       "...                       ...                      ...   \n",
       "4345             2.484209e-07             4.874250e-09   \n",
       "4346             1.363019e-07             7.178323e-06   \n",
       "4347             6.640473e-07             9.402354e-08   \n",
       "4348             1.674609e-04             1.211314e-04   \n",
       "4349             1.141677e-02             2.308521e-04   \n",
       "\n",
       "      MobileNet_V2_2_class_61  MobileNet_V2_2_class_62  \\\n",
       "0                5.545787e-09             2.515807e-08   \n",
       "1                1.730803e-04             1.259049e-08   \n",
       "2                9.240152e-03             2.608575e-03   \n",
       "3                2.465677e-03             1.331138e-03   \n",
       "4                4.904593e-03             1.390067e-04   \n",
       "...                       ...                      ...   \n",
       "4345             1.562865e-09             9.991079e-08   \n",
       "4346             2.857074e-04             4.080899e-03   \n",
       "4347             3.096250e-06             1.271762e-06   \n",
       "4348             2.499685e-05             5.173074e-05   \n",
       "4349             3.864342e-03             2.421903e-06   \n",
       "\n",
       "      MobileNet_V2_2_class_63  MobileNet_V2_2_class_64  \\\n",
       "0                1.021777e-07             6.160776e-08   \n",
       "1                3.632041e-06             3.792483e-06   \n",
       "2                3.587585e-04             1.729638e-04   \n",
       "3                3.494845e-04             3.974282e-01   \n",
       "4                2.840934e-03             1.216361e-04   \n",
       "...                       ...                      ...   \n",
       "4345             3.729173e-08             2.617291e-02   \n",
       "4346             7.621653e-04             3.375111e-02   \n",
       "4347             5.899246e-07             3.273698e-07   \n",
       "4348             4.500997e-04             4.772073e-05   \n",
       "4349             1.779990e-02             1.758284e-02   \n",
       "\n",
       "      MobileNet_V2_2_class_65  \n",
       "0                2.210914e-10  \n",
       "1                4.162863e-07  \n",
       "2                2.334915e-04  \n",
       "3                9.930512e-04  \n",
       "4                4.244205e-04  \n",
       "...                       ...  \n",
       "4345             1.909165e-10  \n",
       "4346             7.631975e-07  \n",
       "4347             1.960391e-05  \n",
       "4348             1.203780e-05  \n",
       "4349             6.809386e-07  \n",
       "\n",
       "[4350 rows x 131 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test = pd.read_csv(csv_file_path)\n",
    "# df_test = df_test.drop(columns=[\"true_label.1\"])\n",
    "# csv_file_path = './Model Results/snet_mnet_predictions_labels_full.csv'\n",
    "# df_test.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# csv_file_path = './Model Results/snet_mnet_predictions_labels.csv'\n",
    "# df_holder.to_csv(csv_file_path, index=False)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1720ab",
   "metadata": {},
   "source": [
    "# Validate the dataframe and performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352cc7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>InceptionNet_1_class_1</th>\n",
       "      <th>InceptionNet_1_class_2</th>\n",
       "      <th>InceptionNet_1_class_3</th>\n",
       "      <th>InceptionNet_1_class_4</th>\n",
       "      <th>InceptionNet_1_class_5</th>\n",
       "      <th>InceptionNet_1_class_6</th>\n",
       "      <th>InceptionNet_1_class_7</th>\n",
       "      <th>InceptionNet_1_class_8</th>\n",
       "      <th>InceptionNet_1_class_9</th>\n",
       "      <th>...</th>\n",
       "      <th>MobileNet_V2_4_class_56</th>\n",
       "      <th>MobileNet_V2_4_class_57</th>\n",
       "      <th>MobileNet_V2_4_class_58</th>\n",
       "      <th>MobileNet_V2_4_class_59</th>\n",
       "      <th>MobileNet_V2_4_class_60</th>\n",
       "      <th>MobileNet_V2_4_class_61</th>\n",
       "      <th>MobileNet_V2_4_class_62</th>\n",
       "      <th>MobileNet_V2_4_class_63</th>\n",
       "      <th>MobileNet_V2_4_class_64</th>\n",
       "      <th>MobileNet_V2_4_class_65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>5.271711e-08</td>\n",
       "      <td>8.214375e-09</td>\n",
       "      <td>9.422983e-09</td>\n",
       "      <td>1.073778e-09</td>\n",
       "      <td>3.919455e-07</td>\n",
       "      <td>9.068081e-07</td>\n",
       "      <td>1.051537e-07</td>\n",
       "      <td>7.904253e-09</td>\n",
       "      <td>8.876940e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.223780e-07</td>\n",
       "      <td>1.766251e-07</td>\n",
       "      <td>3.727825e-10</td>\n",
       "      <td>3.734709e-09</td>\n",
       "      <td>7.203157e-08</td>\n",
       "      <td>5.545787e-09</td>\n",
       "      <td>2.515807e-08</td>\n",
       "      <td>1.021777e-07</td>\n",
       "      <td>6.160776e-08</td>\n",
       "      <td>2.210914e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>5.970387e-06</td>\n",
       "      <td>1.712181e-07</td>\n",
       "      <td>2.003469e-06</td>\n",
       "      <td>1.324439e-06</td>\n",
       "      <td>1.319423e-06</td>\n",
       "      <td>1.367447e-06</td>\n",
       "      <td>1.114959e-06</td>\n",
       "      <td>9.275686e-01</td>\n",
       "      <td>6.915659e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>9.050146e-07</td>\n",
       "      <td>1.253019e-04</td>\n",
       "      <td>4.477683e-05</td>\n",
       "      <td>7.737304e-05</td>\n",
       "      <td>1.824193e-06</td>\n",
       "      <td>1.730803e-04</td>\n",
       "      <td>1.259049e-08</td>\n",
       "      <td>3.632041e-06</td>\n",
       "      <td>3.792483e-06</td>\n",
       "      <td>4.162863e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>8.319048e-04</td>\n",
       "      <td>2.812022e-05</td>\n",
       "      <td>7.891122e-05</td>\n",
       "      <td>1.513799e-02</td>\n",
       "      <td>1.710256e-03</td>\n",
       "      <td>4.854570e-05</td>\n",
       "      <td>2.351476e-05</td>\n",
       "      <td>1.711687e-05</td>\n",
       "      <td>1.635728e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>3.995181e-05</td>\n",
       "      <td>4.432637e-04</td>\n",
       "      <td>1.994744e-03</td>\n",
       "      <td>2.892468e-02</td>\n",
       "      <td>1.874315e-02</td>\n",
       "      <td>9.240152e-03</td>\n",
       "      <td>2.608575e-03</td>\n",
       "      <td>3.587585e-04</td>\n",
       "      <td>1.729638e-04</td>\n",
       "      <td>2.334915e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>5.056936e-13</td>\n",
       "      <td>2.460811e-12</td>\n",
       "      <td>2.693100e-13</td>\n",
       "      <td>1.241311e-13</td>\n",
       "      <td>2.725491e-12</td>\n",
       "      <td>6.051094e-12</td>\n",
       "      <td>9.999771e-01</td>\n",
       "      <td>2.680090e-13</td>\n",
       "      <td>1.796287e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>7.329953e-03</td>\n",
       "      <td>5.770333e-04</td>\n",
       "      <td>3.681877e-04</td>\n",
       "      <td>6.223977e-03</td>\n",
       "      <td>3.169568e-04</td>\n",
       "      <td>2.465677e-03</td>\n",
       "      <td>1.331138e-03</td>\n",
       "      <td>3.494845e-04</td>\n",
       "      <td>3.974282e-01</td>\n",
       "      <td>9.930512e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>1.593175e-09</td>\n",
       "      <td>2.795414e-10</td>\n",
       "      <td>1.250389e-10</td>\n",
       "      <td>4.804293e-08</td>\n",
       "      <td>8.671065e-10</td>\n",
       "      <td>1.413309e-09</td>\n",
       "      <td>3.889157e-09</td>\n",
       "      <td>2.460848e-11</td>\n",
       "      <td>2.990766e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.291772e-04</td>\n",
       "      <td>9.482015e-06</td>\n",
       "      <td>9.991421e-06</td>\n",
       "      <td>2.754351e-05</td>\n",
       "      <td>7.820977e-04</td>\n",
       "      <td>4.904593e-03</td>\n",
       "      <td>1.390067e-04</td>\n",
       "      <td>2.840934e-03</td>\n",
       "      <td>1.216361e-04</td>\n",
       "      <td>4.244205e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>6</td>\n",
       "      <td>1.001821e-07</td>\n",
       "      <td>5.556457e-09</td>\n",
       "      <td>1.118992e-07</td>\n",
       "      <td>1.858495e-09</td>\n",
       "      <td>2.039189e-08</td>\n",
       "      <td>2.832041e-06</td>\n",
       "      <td>9.998986e-01</td>\n",
       "      <td>4.310100e-09</td>\n",
       "      <td>1.668221e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>5.974962e-05</td>\n",
       "      <td>2.613673e-07</td>\n",
       "      <td>4.650745e-07</td>\n",
       "      <td>2.484209e-07</td>\n",
       "      <td>4.874250e-09</td>\n",
       "      <td>1.562865e-09</td>\n",
       "      <td>9.991079e-08</td>\n",
       "      <td>3.729173e-08</td>\n",
       "      <td>2.617291e-02</td>\n",
       "      <td>1.909165e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>21</td>\n",
       "      <td>1.762407e-02</td>\n",
       "      <td>8.314757e-06</td>\n",
       "      <td>1.775505e-06</td>\n",
       "      <td>5.172735e-06</td>\n",
       "      <td>1.821291e-03</td>\n",
       "      <td>4.952103e-07</td>\n",
       "      <td>7.500716e-05</td>\n",
       "      <td>1.121382e-05</td>\n",
       "      <td>7.612852e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>2.676223e-01</td>\n",
       "      <td>1.715256e-04</td>\n",
       "      <td>2.886083e-03</td>\n",
       "      <td>1.363019e-07</td>\n",
       "      <td>7.178323e-06</td>\n",
       "      <td>2.857074e-04</td>\n",
       "      <td>4.080899e-03</td>\n",
       "      <td>7.621653e-04</td>\n",
       "      <td>3.375111e-02</td>\n",
       "      <td>7.631975e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>36</td>\n",
       "      <td>1.896000e-07</td>\n",
       "      <td>5.306263e-09</td>\n",
       "      <td>5.572599e-06</td>\n",
       "      <td>1.579984e-09</td>\n",
       "      <td>3.392033e-09</td>\n",
       "      <td>1.266143e-07</td>\n",
       "      <td>9.430685e-09</td>\n",
       "      <td>5.280787e-10</td>\n",
       "      <td>1.736073e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>9.199030e-08</td>\n",
       "      <td>1.114061e-06</td>\n",
       "      <td>3.346800e-05</td>\n",
       "      <td>6.640473e-07</td>\n",
       "      <td>9.402354e-08</td>\n",
       "      <td>3.096250e-06</td>\n",
       "      <td>1.271762e-06</td>\n",
       "      <td>5.899246e-07</td>\n",
       "      <td>3.273698e-07</td>\n",
       "      <td>1.960391e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>9</td>\n",
       "      <td>3.715895e-05</td>\n",
       "      <td>2.376575e-06</td>\n",
       "      <td>1.784199e-04</td>\n",
       "      <td>5.689888e-07</td>\n",
       "      <td>1.495891e-05</td>\n",
       "      <td>8.724782e-05</td>\n",
       "      <td>2.644503e-05</td>\n",
       "      <td>4.857977e-05</td>\n",
       "      <td>5.239484e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.633162e-04</td>\n",
       "      <td>7.405291e-04</td>\n",
       "      <td>8.604111e-05</td>\n",
       "      <td>1.674609e-04</td>\n",
       "      <td>1.211314e-04</td>\n",
       "      <td>2.499685e-05</td>\n",
       "      <td>5.173074e-05</td>\n",
       "      <td>4.500997e-04</td>\n",
       "      <td>4.772073e-05</td>\n",
       "      <td>1.203780e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>63</td>\n",
       "      <td>5.388100e-04</td>\n",
       "      <td>1.319834e-02</td>\n",
       "      <td>1.321735e-03</td>\n",
       "      <td>5.462120e-06</td>\n",
       "      <td>1.691211e-04</td>\n",
       "      <td>3.649561e-01</td>\n",
       "      <td>1.366114e-04</td>\n",
       "      <td>3.035181e-05</td>\n",
       "      <td>1.750476e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.540072e-03</td>\n",
       "      <td>2.606308e-04</td>\n",
       "      <td>6.949893e-06</td>\n",
       "      <td>1.141677e-02</td>\n",
       "      <td>2.308521e-04</td>\n",
       "      <td>3.864342e-03</td>\n",
       "      <td>2.421903e-06</td>\n",
       "      <td>1.779990e-02</td>\n",
       "      <td>1.758284e-02</td>\n",
       "      <td>6.809386e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4350 rows × 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      true_label  InceptionNet_1_class_1  InceptionNet_1_class_2  \\\n",
       "0              9            5.271711e-08            8.214375e-09   \n",
       "1              7            5.970387e-06            1.712181e-07   \n",
       "2             18            8.319048e-04            2.812022e-05   \n",
       "3              6            5.056936e-13            2.460811e-12   \n",
       "4             31            1.593175e-09            2.795414e-10   \n",
       "...          ...                     ...                     ...   \n",
       "4345           6            1.001821e-07            5.556457e-09   \n",
       "4346          21            1.762407e-02            8.314757e-06   \n",
       "4347          36            1.896000e-07            5.306263e-09   \n",
       "4348           9            3.715895e-05            2.376575e-06   \n",
       "4349          63            5.388100e-04            1.319834e-02   \n",
       "\n",
       "      InceptionNet_1_class_3  InceptionNet_1_class_4  InceptionNet_1_class_5  \\\n",
       "0               9.422983e-09            1.073778e-09            3.919455e-07   \n",
       "1               2.003469e-06            1.324439e-06            1.319423e-06   \n",
       "2               7.891122e-05            1.513799e-02            1.710256e-03   \n",
       "3               2.693100e-13            1.241311e-13            2.725491e-12   \n",
       "4               1.250389e-10            4.804293e-08            8.671065e-10   \n",
       "...                      ...                     ...                     ...   \n",
       "4345            1.118992e-07            1.858495e-09            2.039189e-08   \n",
       "4346            1.775505e-06            5.172735e-06            1.821291e-03   \n",
       "4347            5.572599e-06            1.579984e-09            3.392033e-09   \n",
       "4348            1.784199e-04            5.689888e-07            1.495891e-05   \n",
       "4349            1.321735e-03            5.462120e-06            1.691211e-04   \n",
       "\n",
       "      InceptionNet_1_class_6  InceptionNet_1_class_7  InceptionNet_1_class_8  \\\n",
       "0               9.068081e-07            1.051537e-07            7.904253e-09   \n",
       "1               1.367447e-06            1.114959e-06            9.275686e-01   \n",
       "2               4.854570e-05            2.351476e-05            1.711687e-05   \n",
       "3               6.051094e-12            9.999771e-01            2.680090e-13   \n",
       "4               1.413309e-09            3.889157e-09            2.460848e-11   \n",
       "...                      ...                     ...                     ...   \n",
       "4345            2.832041e-06            9.998986e-01            4.310100e-09   \n",
       "4346            4.952103e-07            7.500716e-05            1.121382e-05   \n",
       "4347            1.266143e-07            9.430685e-09            5.280787e-10   \n",
       "4348            8.724782e-05            2.644503e-05            4.857977e-05   \n",
       "4349            3.649561e-01            1.366114e-04            3.035181e-05   \n",
       "\n",
       "      InceptionNet_1_class_9  ...  MobileNet_V2_4_class_56  \\\n",
       "0               8.876940e-10  ...             1.223780e-07   \n",
       "1               6.915659e-02  ...             9.050146e-07   \n",
       "2               1.635728e-03  ...             3.995181e-05   \n",
       "3               1.796287e-12  ...             7.329953e-03   \n",
       "4               2.990766e-09  ...             1.291772e-04   \n",
       "...                      ...  ...                      ...   \n",
       "4345            1.668221e-09  ...             5.974962e-05   \n",
       "4346            7.612852e-08  ...             2.676223e-01   \n",
       "4347            1.736073e-10  ...             9.199030e-08   \n",
       "4348            5.239484e-06  ...             8.633162e-04   \n",
       "4349            1.750476e-05  ...             1.540072e-03   \n",
       "\n",
       "      MobileNet_V2_4_class_57  MobileNet_V2_4_class_58  \\\n",
       "0                1.766251e-07             3.727825e-10   \n",
       "1                1.253019e-04             4.477683e-05   \n",
       "2                4.432637e-04             1.994744e-03   \n",
       "3                5.770333e-04             3.681877e-04   \n",
       "4                9.482015e-06             9.991421e-06   \n",
       "...                       ...                      ...   \n",
       "4345             2.613673e-07             4.650745e-07   \n",
       "4346             1.715256e-04             2.886083e-03   \n",
       "4347             1.114061e-06             3.346800e-05   \n",
       "4348             7.405291e-04             8.604111e-05   \n",
       "4349             2.606308e-04             6.949893e-06   \n",
       "\n",
       "      MobileNet_V2_4_class_59  MobileNet_V2_4_class_60  \\\n",
       "0                3.734709e-09             7.203157e-08   \n",
       "1                7.737304e-05             1.824193e-06   \n",
       "2                2.892468e-02             1.874315e-02   \n",
       "3                6.223977e-03             3.169568e-04   \n",
       "4                2.754351e-05             7.820977e-04   \n",
       "...                       ...                      ...   \n",
       "4345             2.484209e-07             4.874250e-09   \n",
       "4346             1.363019e-07             7.178323e-06   \n",
       "4347             6.640473e-07             9.402354e-08   \n",
       "4348             1.674609e-04             1.211314e-04   \n",
       "4349             1.141677e-02             2.308521e-04   \n",
       "\n",
       "      MobileNet_V2_4_class_61  MobileNet_V2_4_class_62  \\\n",
       "0                5.545787e-09             2.515807e-08   \n",
       "1                1.730803e-04             1.259049e-08   \n",
       "2                9.240152e-03             2.608575e-03   \n",
       "3                2.465677e-03             1.331138e-03   \n",
       "4                4.904593e-03             1.390067e-04   \n",
       "...                       ...                      ...   \n",
       "4345             1.562865e-09             9.991079e-08   \n",
       "4346             2.857074e-04             4.080899e-03   \n",
       "4347             3.096250e-06             1.271762e-06   \n",
       "4348             2.499685e-05             5.173074e-05   \n",
       "4349             3.864342e-03             2.421903e-06   \n",
       "\n",
       "      MobileNet_V2_4_class_63  MobileNet_V2_4_class_64  \\\n",
       "0                1.021777e-07             6.160776e-08   \n",
       "1                3.632041e-06             3.792483e-06   \n",
       "2                3.587585e-04             1.729638e-04   \n",
       "3                3.494845e-04             3.974282e-01   \n",
       "4                2.840934e-03             1.216361e-04   \n",
       "...                       ...                      ...   \n",
       "4345             3.729173e-08             2.617291e-02   \n",
       "4346             7.621653e-04             3.375111e-02   \n",
       "4347             5.899246e-07             3.273698e-07   \n",
       "4348             4.500997e-04             4.772073e-05   \n",
       "4349             1.779990e-02             1.758284e-02   \n",
       "\n",
       "      MobileNet_V2_4_class_65  \n",
       "0                2.210914e-10  \n",
       "1                4.162863e-07  \n",
       "2                2.334915e-04  \n",
       "3                9.930512e-04  \n",
       "4                4.244205e-04  \n",
       "...                       ...  \n",
       "4345             1.909165e-10  \n",
       "4346             7.631975e-07  \n",
       "4347             1.960391e-05  \n",
       "4348             1.203780e-05  \n",
       "4349             6.809386e-07  \n",
       "\n",
       "[4350 rows x 264 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_path = './Model Results/snet_mnet_inet_rnet_predictions_labels_full.csv'\n",
    "df_test = pd.read_csv(csv_file_path)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2a0bd7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'InceptionNet': {'losses': 0.03594856326607452,\n",
       "  'accuracies': 0.7110344827586207,\n",
       "  'f1_scores': 0.7107268872316282,\n",
       "  'aucs': 0.9746678787677775},\n",
       " 'ResNet': {'losses': 0.027736404229854714,\n",
       "  'accuracies': 0.7349425287356322,\n",
       "  'f1_scores': 0.7318349079033682,\n",
       "  'aucs': 0.9822914140797735},\n",
       " 'SqueezeNet': {'losses': 0.03397794493313493,\n",
       "  'accuracies': 0.6174712643678161,\n",
       "  'f1_scores': 0.6145330043588136,\n",
       "  'aucs': 0.9644666666276738},\n",
       " 'MobileNet_V2': {'losses': 0.022019906187879627,\n",
       "  'accuracies': 0.6988505747126437,\n",
       "  'f1_scores': 0.6950701804527387,\n",
       "  'aucs': 0.9829742778345268}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "json_file_path = './Model Results/snet_mnet_inet_rnet_models_metrics_full.json'\n",
    "with open(json_file_path, 'r') as file:\n",
    "    models_metrics_holder_test = json.load(file)\n",
    "models_metrics_holder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c6bf16",
   "metadata": {},
   "source": [
    "# Save the models' accuracies for weighting later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f77af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "snet_acc = models_metrics_holder_test['SqueezeNet']['accuracies']\n",
    "mnet_acc = models_metrics_holder_test['MobileNet_V2']['accuracies']\n",
    "rnet_acc = models_metrics_holder_test['ResNet']['accuracies']\n",
    "inet_acc = models_metrics_holder_test['InceptionNet']['accuracies']\n",
    "model_acc_sum = snet_acc + mnet_acc + rnet_acc + inet_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57908aae",
   "metadata": {},
   "source": [
    "# Voting Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d399550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_voting(*predictions):\n",
    "    # Taking argmax to get predicted class for each model's prediction\n",
    "    votes = [np.argmax(pred, axis=1) for pred in predictions]\n",
    "    # Transpose to get a list of predictions for each instance\n",
    "    votes = np.array(votes).T\n",
    "    # Use mode to find the most common prediction\n",
    "    final_pred, _ = stats.mode(votes, axis=1)\n",
    "    return final_pred.flatten()\n",
    "\n",
    "def soft_voting(*predictions):\n",
    "    # Averaging probabilities across models\n",
    "    avg_probs = np.mean(np.array(predictions), axis=0)\n",
    "    # Choosing the class with the highest average probability\n",
    "    final_pred = np.argmax(avg_probs, axis=1)\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a969d919",
   "metadata": {},
   "source": [
    "# Number of matches should give us the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "debb7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_test.iloc[:, :66]\n",
    "df2 = df_test.iloc[:, [0] + list(range(67, 132))]\n",
    "df3 = df_test.iloc[:, [0] + list(range(133, 198))]\n",
    "df4 = df_test.iloc[:, [0] + list(range(199, 264))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ee54a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>MobileNet_V2_4_class_1</th>\n",
       "      <th>MobileNet_V2_4_class_2</th>\n",
       "      <th>MobileNet_V2_4_class_3</th>\n",
       "      <th>MobileNet_V2_4_class_4</th>\n",
       "      <th>MobileNet_V2_4_class_5</th>\n",
       "      <th>MobileNet_V2_4_class_6</th>\n",
       "      <th>MobileNet_V2_4_class_7</th>\n",
       "      <th>MobileNet_V2_4_class_8</th>\n",
       "      <th>MobileNet_V2_4_class_9</th>\n",
       "      <th>...</th>\n",
       "      <th>MobileNet_V2_4_class_56</th>\n",
       "      <th>MobileNet_V2_4_class_57</th>\n",
       "      <th>MobileNet_V2_4_class_58</th>\n",
       "      <th>MobileNet_V2_4_class_59</th>\n",
       "      <th>MobileNet_V2_4_class_60</th>\n",
       "      <th>MobileNet_V2_4_class_61</th>\n",
       "      <th>MobileNet_V2_4_class_62</th>\n",
       "      <th>MobileNet_V2_4_class_63</th>\n",
       "      <th>MobileNet_V2_4_class_64</th>\n",
       "      <th>MobileNet_V2_4_class_65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>1.298897e-09</td>\n",
       "      <td>9.091097e-09</td>\n",
       "      <td>9.581078e-08</td>\n",
       "      <td>7.214933e-10</td>\n",
       "      <td>2.465350e-08</td>\n",
       "      <td>4.576903e-08</td>\n",
       "      <td>1.869161e-08</td>\n",
       "      <td>3.776105e-08</td>\n",
       "      <td>8.248176e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.223780e-07</td>\n",
       "      <td>1.766251e-07</td>\n",
       "      <td>3.727825e-10</td>\n",
       "      <td>3.734709e-09</td>\n",
       "      <td>7.203157e-08</td>\n",
       "      <td>5.545787e-09</td>\n",
       "      <td>2.515807e-08</td>\n",
       "      <td>1.021777e-07</td>\n",
       "      <td>6.160776e-08</td>\n",
       "      <td>2.210914e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>5.064653e-06</td>\n",
       "      <td>6.767095e-08</td>\n",
       "      <td>1.798815e-06</td>\n",
       "      <td>1.820694e-07</td>\n",
       "      <td>4.347245e-06</td>\n",
       "      <td>1.915279e-07</td>\n",
       "      <td>1.147045e-08</td>\n",
       "      <td>9.443328e-01</td>\n",
       "      <td>4.586118e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>9.050146e-07</td>\n",
       "      <td>1.253019e-04</td>\n",
       "      <td>4.477683e-05</td>\n",
       "      <td>7.737304e-05</td>\n",
       "      <td>1.824193e-06</td>\n",
       "      <td>1.730803e-04</td>\n",
       "      <td>1.259049e-08</td>\n",
       "      <td>3.632041e-06</td>\n",
       "      <td>3.792483e-06</td>\n",
       "      <td>4.162863e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_label  MobileNet_V2_4_class_1  MobileNet_V2_4_class_2  \\\n",
       "0           9            1.298897e-09            9.091097e-09   \n",
       "1           7            5.064653e-06            6.767095e-08   \n",
       "\n",
       "   MobileNet_V2_4_class_3  MobileNet_V2_4_class_4  MobileNet_V2_4_class_5  \\\n",
       "0            9.581078e-08            7.214933e-10            2.465350e-08   \n",
       "1            1.798815e-06            1.820694e-07            4.347245e-06   \n",
       "\n",
       "   MobileNet_V2_4_class_6  MobileNet_V2_4_class_7  MobileNet_V2_4_class_8  \\\n",
       "0            4.576903e-08            1.869161e-08            3.776105e-08   \n",
       "1            1.915279e-07            1.147045e-08            9.443328e-01   \n",
       "\n",
       "   MobileNet_V2_4_class_9  ...  MobileNet_V2_4_class_56  \\\n",
       "0            8.248176e-09  ...             1.223780e-07   \n",
       "1            4.586118e-03  ...             9.050146e-07   \n",
       "\n",
       "   MobileNet_V2_4_class_57  MobileNet_V2_4_class_58  MobileNet_V2_4_class_59  \\\n",
       "0             1.766251e-07             3.727825e-10             3.734709e-09   \n",
       "1             1.253019e-04             4.477683e-05             7.737304e-05   \n",
       "\n",
       "   MobileNet_V2_4_class_60  MobileNet_V2_4_class_61  MobileNet_V2_4_class_62  \\\n",
       "0             7.203157e-08             5.545787e-09             2.515807e-08   \n",
       "1             1.824193e-06             1.730803e-04             1.259049e-08   \n",
       "\n",
       "   MobileNet_V2_4_class_63  MobileNet_V2_4_class_64  MobileNet_V2_4_class_65  \n",
       "0             1.021777e-07             6.160776e-08             2.210914e-10  \n",
       "1             3.632041e-06             3.792483e-06             4.162863e-07  \n",
       "\n",
       "[2 rows x 66 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56c058f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3093, 67)\n",
      "(3197, 67)\n",
      "(2686, 67)\n",
      "(3040, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/87f8l4qn6lg35t626mq1bgx40000gn/T/ipykernel_35939/436439709.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['max_index'] = df2.apply(find_max_index, axis=1)\n",
      "/var/folders/_t/87f8l4qn6lg35t626mq1bgx40000gn/T/ipykernel_35939/436439709.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['max_index'] = df3.apply(find_max_index, axis=1)\n",
      "/var/folders/_t/87f8l4qn6lg35t626mq1bgx40000gn/T/ipykernel_35939/436439709.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['max_index'] = df4.apply(find_max_index, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# max_col_index_first_row_excluding_first_column = df.iloc[0, 1:].idxmax()\n",
    "# print(max_col_index_first_row_excluding_first_column[-2:])\n",
    "# df.iloc[0]\n",
    "def find_max_index(row):\n",
    "    #find the index for _ and get numbers after that\n",
    "#     print(row)\n",
    "    max_col = row.iloc[1:].idxmax()\n",
    "    max_col = int(max_col.split('_')[-1])\n",
    "    return max_col - 1\n",
    "\n",
    "# Apply the function to each row and create a new column with the results\n",
    "df1['max_index'] = df1.apply(find_max_index, axis=1)\n",
    "df2['max_index'] = df2.apply(find_max_index, axis=1)\n",
    "df3['max_index'] = df3.apply(find_max_index, axis=1)\n",
    "df4['max_index'] = df4.apply(find_max_index, axis=1)\n",
    "print(df1[df1['max_index'] == df1['true_label']].shape)\n",
    "print(df2[df2['max_index'] == df2['true_label']].shape)\n",
    "print(df3[df3['max_index'] == df3['true_label']].shape)\n",
    "print(df4[df4['max_index'] == df4['true_label']].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be37844",
   "metadata": {},
   "source": [
    "# Create Pseudo-labels for unsupervised domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb46d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_test.iloc[:, :66]\n",
    "df2 = df_test.iloc[:, list(range(67, 132))]\n",
    "df3 = df_test.iloc[:, list(range(133, 198))]\n",
    "df4 = df_test.iloc[:, list(range(199, 264))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8fd071e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>InceptionNet_1_class_1</th>\n",
       "      <th>InceptionNet_1_class_2</th>\n",
       "      <th>InceptionNet_1_class_3</th>\n",
       "      <th>InceptionNet_1_class_4</th>\n",
       "      <th>InceptionNet_1_class_5</th>\n",
       "      <th>InceptionNet_1_class_6</th>\n",
       "      <th>InceptionNet_1_class_7</th>\n",
       "      <th>InceptionNet_1_class_8</th>\n",
       "      <th>InceptionNet_1_class_9</th>\n",
       "      <th>...</th>\n",
       "      <th>InceptionNet_1_class_56</th>\n",
       "      <th>InceptionNet_1_class_57</th>\n",
       "      <th>InceptionNet_1_class_58</th>\n",
       "      <th>InceptionNet_1_class_59</th>\n",
       "      <th>InceptionNet_1_class_60</th>\n",
       "      <th>InceptionNet_1_class_61</th>\n",
       "      <th>InceptionNet_1_class_62</th>\n",
       "      <th>InceptionNet_1_class_63</th>\n",
       "      <th>InceptionNet_1_class_64</th>\n",
       "      <th>InceptionNet_1_class_65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>5.271711e-08</td>\n",
       "      <td>8.214375e-09</td>\n",
       "      <td>9.422983e-09</td>\n",
       "      <td>1.073778e-09</td>\n",
       "      <td>3.919455e-07</td>\n",
       "      <td>9.068081e-07</td>\n",
       "      <td>1.051537e-07</td>\n",
       "      <td>7.904253e-09</td>\n",
       "      <td>8.876940e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>6.518616e-08</td>\n",
       "      <td>6.329334e-09</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>5.436562e-09</td>\n",
       "      <td>9.964816e-08</td>\n",
       "      <td>5.423522e-09</td>\n",
       "      <td>9.474314e-08</td>\n",
       "      <td>5.355669e-08</td>\n",
       "      <td>4.232184e-08</td>\n",
       "      <td>1.940623e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>5.970387e-06</td>\n",
       "      <td>1.712181e-07</td>\n",
       "      <td>2.003469e-06</td>\n",
       "      <td>1.324439e-06</td>\n",
       "      <td>1.319423e-06</td>\n",
       "      <td>1.367447e-06</td>\n",
       "      <td>1.114959e-06</td>\n",
       "      <td>9.275686e-01</td>\n",
       "      <td>6.915659e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.947686e-06</td>\n",
       "      <td>5.376736e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.300840e-07</td>\n",
       "      <td>1.443520e-07</td>\n",
       "      <td>1.877031e-04</td>\n",
       "      <td>1.826419e-07</td>\n",
       "      <td>1.019534e-05</td>\n",
       "      <td>1.243721e-06</td>\n",
       "      <td>1.239725e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_label  InceptionNet_1_class_1  InceptionNet_1_class_2  \\\n",
       "0           9            5.271711e-08            8.214375e-09   \n",
       "1           7            5.970387e-06            1.712181e-07   \n",
       "\n",
       "   InceptionNet_1_class_3  InceptionNet_1_class_4  InceptionNet_1_class_5  \\\n",
       "0            9.422983e-09            1.073778e-09            3.919455e-07   \n",
       "1            2.003469e-06            1.324439e-06            1.319423e-06   \n",
       "\n",
       "   InceptionNet_1_class_6  InceptionNet_1_class_7  InceptionNet_1_class_8  \\\n",
       "0            9.068081e-07            1.051537e-07            7.904253e-09   \n",
       "1            1.367447e-06            1.114959e-06            9.275686e-01   \n",
       "\n",
       "   InceptionNet_1_class_9  ...  InceptionNet_1_class_56  \\\n",
       "0            8.876940e-10  ...             6.518616e-08   \n",
       "1            6.915659e-02  ...             1.947686e-06   \n",
       "\n",
       "   InceptionNet_1_class_57  InceptionNet_1_class_58  InceptionNet_1_class_59  \\\n",
       "0             6.329334e-09                 0.000008             5.436562e-09   \n",
       "1             5.376736e-07                 0.000001             3.300840e-07   \n",
       "\n",
       "   InceptionNet_1_class_60  InceptionNet_1_class_61  InceptionNet_1_class_62  \\\n",
       "0             9.964816e-08             5.423522e-09             9.474314e-08   \n",
       "1             1.443520e-07             1.877031e-04             1.826419e-07   \n",
       "\n",
       "   InceptionNet_1_class_63  InceptionNet_1_class_64  InceptionNet_1_class_65  \n",
       "0             5.355669e-08             4.232184e-08             1.940623e-09  \n",
       "1             1.019534e-05             1.243721e-06             1.239725e-07  \n",
       "\n",
       "[2 rows x 66 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea6e8a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class_number(col_name):\n",
    "    return int(col_name.split('_')[-1])\n",
    "\n",
    "# Create a mapping from df2's columns to df1's columns\n",
    "df2_cols_to_df1 = {col: f\"InceptionNet_1_class_{extract_class_number(col)}\" for col in df2.columns}\n",
    "df3_cols_to_df1 = {col: f\"InceptionNet_1_class_{extract_class_number(col)}\" for col in df3.columns}\n",
    "df4_cols_to_df1 = {col: f\"InceptionNet_1_class_{extract_class_number(col)}\" for col in df4.columns}\n",
    "\n",
    "# Rename df2's columns to match df1's columns\n",
    "df2_renamed = df2.rename(columns=df2_cols_to_df1)\n",
    "df3_renamed = df3.rename(columns=df3_cols_to_df1)\n",
    "df4_renamed = df4.rename(columns=df4_cols_to_df1)\n",
    "\n",
    "# Perform element-wise addition\n",
    "combined_df = df1 * (inet_acc / model_acc_sum) + \\\n",
    "                df2_renamed * (rnet_acc / model_acc_sum) + \\\n",
    "                df3_renamed * (snet_acc / model_acc_sum) + \\\n",
    "                df4_renamed * (mnet_acc / model_acc_sum)\n",
    "\n",
    "# Step 4: Create the pseudo label column\n",
    "combined_df['pseudo_labels'] = combined_df.apply(find_max_index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0d981f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>ResNet_2_class_1</th>\n",
       "      <th>ResNet_2_class_2</th>\n",
       "      <th>ResNet_2_class_3</th>\n",
       "      <th>ResNet_2_class_4</th>\n",
       "      <th>ResNet_2_class_5</th>\n",
       "      <th>ResNet_2_class_6</th>\n",
       "      <th>ResNet_2_class_7</th>\n",
       "      <th>ResNet_2_class_8</th>\n",
       "      <th>ResNet_2_class_9</th>\n",
       "      <th>...</th>\n",
       "      <th>ResNet_2_class_56</th>\n",
       "      <th>ResNet_2_class_57</th>\n",
       "      <th>ResNet_2_class_58</th>\n",
       "      <th>ResNet_2_class_59</th>\n",
       "      <th>ResNet_2_class_60</th>\n",
       "      <th>ResNet_2_class_61</th>\n",
       "      <th>ResNet_2_class_62</th>\n",
       "      <th>ResNet_2_class_63</th>\n",
       "      <th>ResNet_2_class_64</th>\n",
       "      <th>ResNet_2_class_65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2.529406e-09</td>\n",
       "      <td>1.287810e-09</td>\n",
       "      <td>5.293993e-09</td>\n",
       "      <td>3.821620e-11</td>\n",
       "      <td>1.970488e-11</td>\n",
       "      <td>2.414438e-08</td>\n",
       "      <td>4.454451e-10</td>\n",
       "      <td>2.927175e-09</td>\n",
       "      <td>1.000635e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.476167e-07</td>\n",
       "      <td>4.642915e-11</td>\n",
       "      <td>6.350050e-10</td>\n",
       "      <td>2.944336e-10</td>\n",
       "      <td>1.625369e-09</td>\n",
       "      <td>1.150376e-10</td>\n",
       "      <td>2.834877e-08</td>\n",
       "      <td>1.329710e-08</td>\n",
       "      <td>4.743747e-10</td>\n",
       "      <td>5.960810e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>7.194324e-07</td>\n",
       "      <td>4.738232e-09</td>\n",
       "      <td>1.789827e-09</td>\n",
       "      <td>2.580729e-10</td>\n",
       "      <td>3.288950e-10</td>\n",
       "      <td>3.331287e-11</td>\n",
       "      <td>3.872326e-12</td>\n",
       "      <td>9.989772e-01</td>\n",
       "      <td>2.728004e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.556366e-10</td>\n",
       "      <td>1.587427e-09</td>\n",
       "      <td>7.847949e-11</td>\n",
       "      <td>2.515632e-08</td>\n",
       "      <td>7.313010e-12</td>\n",
       "      <td>1.016452e-03</td>\n",
       "      <td>1.542800e-10</td>\n",
       "      <td>1.048017e-09</td>\n",
       "      <td>2.495195e-11</td>\n",
       "      <td>1.008817e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_label  ResNet_2_class_1  ResNet_2_class_2  ResNet_2_class_3  \\\n",
       "0           9      2.529406e-09      1.287810e-09      5.293993e-09   \n",
       "1           7      7.194324e-07      4.738232e-09      1.789827e-09   \n",
       "\n",
       "   ResNet_2_class_4  ResNet_2_class_5  ResNet_2_class_6  ResNet_2_class_7  \\\n",
       "0      3.821620e-11      1.970488e-11      2.414438e-08      4.454451e-10   \n",
       "1      2.580729e-10      3.288950e-10      3.331287e-11      3.872326e-12   \n",
       "\n",
       "   ResNet_2_class_8  ResNet_2_class_9  ...  ResNet_2_class_56  \\\n",
       "0      2.927175e-09      1.000635e-10  ...       2.476167e-07   \n",
       "1      9.989772e-01      2.728004e-06  ...       3.556366e-10   \n",
       "\n",
       "   ResNet_2_class_57  ResNet_2_class_58  ResNet_2_class_59  ResNet_2_class_60  \\\n",
       "0       4.642915e-11       6.350050e-10       2.944336e-10       1.625369e-09   \n",
       "1       1.587427e-09       7.847949e-11       2.515632e-08       7.313010e-12   \n",
       "\n",
       "   ResNet_2_class_61  ResNet_2_class_62  ResNet_2_class_63  ResNet_2_class_64  \\\n",
       "0       1.150376e-10       2.834877e-08       1.329710e-08       4.743747e-10   \n",
       "1       1.016452e-03       1.542800e-10       1.048017e-09       2.495195e-11   \n",
       "\n",
       "   ResNet_2_class_65  \n",
       "0       5.960810e-12  \n",
       "1       1.008817e-11  \n",
       "\n",
       "[2 rows x 66 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df_test.iloc[:, :66]\n",
    "df2 = df_test.iloc[:, [0] + list(range(67, 132))]\n",
    "df3 = df_test.iloc[:, [0] + list(range(133, 198))]\n",
    "df4 = df_test.iloc[:, [0] + list(range(199, 264))]\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd6c5615",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3310, 67)\n",
      "(3310, 67)\n",
      "(3310, 67)\n",
      "(3310, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/87f8l4qn6lg35t626mq1bgx40000gn/T/ipykernel_35939/638631057.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['pseudo_label'] = combined_df['pseudo_labels']\n",
      "/var/folders/_t/87f8l4qn6lg35t626mq1bgx40000gn/T/ipykernel_35939/638631057.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['pseudo_label'] = combined_df['pseudo_labels']\n",
      "/var/folders/_t/87f8l4qn6lg35t626mq1bgx40000gn/T/ipykernel_35939/638631057.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['pseudo_label'] = combined_df['pseudo_labels']\n"
     ]
    }
   ],
   "source": [
    "df1['pseudo_label'] = combined_df['pseudo_labels']\n",
    "df2['pseudo_label'] = combined_df['pseudo_labels']\n",
    "df3['pseudo_label'] = combined_df['pseudo_labels']\n",
    "df4['pseudo_label'] = combined_df['pseudo_labels']\n",
    "\n",
    "print(df1[df1['pseudo_label'] == df1['true_label']].shape)\n",
    "print(df2[df2['pseudo_label'] == df2['true_label']].shape)\n",
    "print(df3[df3['pseudo_label'] == df3['true_label']].shape)\n",
    "print(df4[df4['pseudo_label'] == df4['true_label']].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d797fa",
   "metadata": {},
   "source": [
    "# Implementing Voting Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2f18409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_test.iloc[:, 1:66]\n",
    "df2 = df_test.iloc[:, list(range(67, 132))]\n",
    "df3 = df_test.iloc[:, list(range(133, 198))]\n",
    "df4 = df_test.iloc[:, list(range(199, 264))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "079f0ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InceptionNet_1_class_1</th>\n",
       "      <th>InceptionNet_1_class_2</th>\n",
       "      <th>InceptionNet_1_class_3</th>\n",
       "      <th>InceptionNet_1_class_4</th>\n",
       "      <th>InceptionNet_1_class_5</th>\n",
       "      <th>InceptionNet_1_class_6</th>\n",
       "      <th>InceptionNet_1_class_7</th>\n",
       "      <th>InceptionNet_1_class_8</th>\n",
       "      <th>InceptionNet_1_class_9</th>\n",
       "      <th>InceptionNet_1_class_10</th>\n",
       "      <th>...</th>\n",
       "      <th>InceptionNet_1_class_56</th>\n",
       "      <th>InceptionNet_1_class_57</th>\n",
       "      <th>InceptionNet_1_class_58</th>\n",
       "      <th>InceptionNet_1_class_59</th>\n",
       "      <th>InceptionNet_1_class_60</th>\n",
       "      <th>InceptionNet_1_class_61</th>\n",
       "      <th>InceptionNet_1_class_62</th>\n",
       "      <th>InceptionNet_1_class_63</th>\n",
       "      <th>InceptionNet_1_class_64</th>\n",
       "      <th>InceptionNet_1_class_65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.271711e-08</td>\n",
       "      <td>8.214375e-09</td>\n",
       "      <td>9.422983e-09</td>\n",
       "      <td>1.073778e-09</td>\n",
       "      <td>3.919455e-07</td>\n",
       "      <td>9.068081e-07</td>\n",
       "      <td>1.051537e-07</td>\n",
       "      <td>7.904253e-09</td>\n",
       "      <td>8.876940e-10</td>\n",
       "      <td>9.999390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.518616e-08</td>\n",
       "      <td>6.329334e-09</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>5.436562e-09</td>\n",
       "      <td>9.964816e-08</td>\n",
       "      <td>5.423522e-09</td>\n",
       "      <td>9.474314e-08</td>\n",
       "      <td>5.355669e-08</td>\n",
       "      <td>4.232184e-08</td>\n",
       "      <td>1.940623e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.970387e-06</td>\n",
       "      <td>1.712181e-07</td>\n",
       "      <td>2.003469e-06</td>\n",
       "      <td>1.324439e-06</td>\n",
       "      <td>1.319423e-06</td>\n",
       "      <td>1.367447e-06</td>\n",
       "      <td>1.114959e-06</td>\n",
       "      <td>9.275686e-01</td>\n",
       "      <td>6.915659e-02</td>\n",
       "      <td>6.998345e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.947686e-06</td>\n",
       "      <td>5.376736e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.300840e-07</td>\n",
       "      <td>1.443520e-07</td>\n",
       "      <td>1.877031e-04</td>\n",
       "      <td>1.826419e-07</td>\n",
       "      <td>1.019534e-05</td>\n",
       "      <td>1.243721e-06</td>\n",
       "      <td>1.239725e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   InceptionNet_1_class_1  InceptionNet_1_class_2  InceptionNet_1_class_3  \\\n",
       "0            5.271711e-08            8.214375e-09            9.422983e-09   \n",
       "1            5.970387e-06            1.712181e-07            2.003469e-06   \n",
       "\n",
       "   InceptionNet_1_class_4  InceptionNet_1_class_5  InceptionNet_1_class_6  \\\n",
       "0            1.073778e-09            3.919455e-07            9.068081e-07   \n",
       "1            1.324439e-06            1.319423e-06            1.367447e-06   \n",
       "\n",
       "   InceptionNet_1_class_7  InceptionNet_1_class_8  InceptionNet_1_class_9  \\\n",
       "0            1.051537e-07            7.904253e-09            8.876940e-10   \n",
       "1            1.114959e-06            9.275686e-01            6.915659e-02   \n",
       "\n",
       "   InceptionNet_1_class_10  ...  InceptionNet_1_class_56  \\\n",
       "0             9.999390e-01  ...             6.518616e-08   \n",
       "1             6.998345e-08  ...             1.947686e-06   \n",
       "\n",
       "   InceptionNet_1_class_57  InceptionNet_1_class_58  InceptionNet_1_class_59  \\\n",
       "0             6.329334e-09                 0.000008             5.436562e-09   \n",
       "1             5.376736e-07                 0.000001             3.300840e-07   \n",
       "\n",
       "   InceptionNet_1_class_60  InceptionNet_1_class_61  InceptionNet_1_class_62  \\\n",
       "0             9.964816e-08             5.423522e-09             9.474314e-08   \n",
       "1             1.443520e-07             1.877031e-04             1.826419e-07   \n",
       "\n",
       "   InceptionNet_1_class_63  InceptionNet_1_class_64  InceptionNet_1_class_65  \n",
       "0             5.355669e-08             4.232184e-08             1.940623e-09  \n",
       "1             1.019534e-05             1.243721e-06             1.239725e-07  \n",
       "\n",
       "[2 rows x 65 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a57b4746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/87f8l4qn6lg35t626mq1bgx40000gn/T/ipykernel_63722/2368454300.py:7: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  final_pred, _ = stats.mode(votes, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 9,  7, 48, ..., 36,  9,  1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hard Voting\n",
    "final_pred_hard = hard_voting(df1.values, df2.values, df3.values, df4.values)\n",
    "final_pred_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82e974dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>ResNet_2_class_1</th>\n",
       "      <th>ResNet_2_class_2</th>\n",
       "      <th>ResNet_2_class_3</th>\n",
       "      <th>ResNet_2_class_4</th>\n",
       "      <th>ResNet_2_class_5</th>\n",
       "      <th>ResNet_2_class_6</th>\n",
       "      <th>ResNet_2_class_7</th>\n",
       "      <th>ResNet_2_class_8</th>\n",
       "      <th>ResNet_2_class_9</th>\n",
       "      <th>...</th>\n",
       "      <th>ResNet_2_class_56</th>\n",
       "      <th>ResNet_2_class_57</th>\n",
       "      <th>ResNet_2_class_58</th>\n",
       "      <th>ResNet_2_class_59</th>\n",
       "      <th>ResNet_2_class_60</th>\n",
       "      <th>ResNet_2_class_61</th>\n",
       "      <th>ResNet_2_class_62</th>\n",
       "      <th>ResNet_2_class_63</th>\n",
       "      <th>ResNet_2_class_64</th>\n",
       "      <th>ResNet_2_class_65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2.529406e-09</td>\n",
       "      <td>1.287810e-09</td>\n",
       "      <td>5.293993e-09</td>\n",
       "      <td>3.821620e-11</td>\n",
       "      <td>1.970488e-11</td>\n",
       "      <td>2.414438e-08</td>\n",
       "      <td>4.454451e-10</td>\n",
       "      <td>2.927175e-09</td>\n",
       "      <td>1.000635e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.476167e-07</td>\n",
       "      <td>4.642915e-11</td>\n",
       "      <td>6.350050e-10</td>\n",
       "      <td>2.944336e-10</td>\n",
       "      <td>1.625369e-09</td>\n",
       "      <td>1.150376e-10</td>\n",
       "      <td>2.834877e-08</td>\n",
       "      <td>1.329710e-08</td>\n",
       "      <td>4.743747e-10</td>\n",
       "      <td>5.960810e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>7.194324e-07</td>\n",
       "      <td>4.738232e-09</td>\n",
       "      <td>1.789827e-09</td>\n",
       "      <td>2.580729e-10</td>\n",
       "      <td>3.288950e-10</td>\n",
       "      <td>3.331287e-11</td>\n",
       "      <td>3.872326e-12</td>\n",
       "      <td>9.989772e-01</td>\n",
       "      <td>2.728004e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.556366e-10</td>\n",
       "      <td>1.587427e-09</td>\n",
       "      <td>7.847949e-11</td>\n",
       "      <td>2.515632e-08</td>\n",
       "      <td>7.313010e-12</td>\n",
       "      <td>1.016452e-03</td>\n",
       "      <td>1.542800e-10</td>\n",
       "      <td>1.048017e-09</td>\n",
       "      <td>2.495195e-11</td>\n",
       "      <td>1.008817e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_label  ResNet_2_class_1  ResNet_2_class_2  ResNet_2_class_3  \\\n",
       "0           9      2.529406e-09      1.287810e-09      5.293993e-09   \n",
       "1           7      7.194324e-07      4.738232e-09      1.789827e-09   \n",
       "\n",
       "   ResNet_2_class_4  ResNet_2_class_5  ResNet_2_class_6  ResNet_2_class_7  \\\n",
       "0      3.821620e-11      1.970488e-11      2.414438e-08      4.454451e-10   \n",
       "1      2.580729e-10      3.288950e-10      3.331287e-11      3.872326e-12   \n",
       "\n",
       "   ResNet_2_class_8  ResNet_2_class_9  ...  ResNet_2_class_56  \\\n",
       "0      2.927175e-09      1.000635e-10  ...       2.476167e-07   \n",
       "1      9.989772e-01      2.728004e-06  ...       3.556366e-10   \n",
       "\n",
       "   ResNet_2_class_57  ResNet_2_class_58  ResNet_2_class_59  ResNet_2_class_60  \\\n",
       "0       4.642915e-11       6.350050e-10       2.944336e-10       1.625369e-09   \n",
       "1       1.587427e-09       7.847949e-11       2.515632e-08       7.313010e-12   \n",
       "\n",
       "   ResNet_2_class_61  ResNet_2_class_62  ResNet_2_class_63  ResNet_2_class_64  \\\n",
       "0       1.150376e-10       2.834877e-08       1.329710e-08       4.743747e-10   \n",
       "1       1.016452e-03       1.542800e-10       1.048017e-09       2.495195e-11   \n",
       "\n",
       "   ResNet_2_class_65  \n",
       "0       5.960810e-12  \n",
       "1       1.008817e-11  \n",
       "\n",
       "[2 rows x 66 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df_test.iloc[:, :66]\n",
    "df2 = df_test.iloc[:, [0] + list(range(67, 132))]\n",
    "df3 = df_test.iloc[:, [0] + list(range(133, 198))]\n",
    "df4 = df_test.iloc[:, [0] + list(range(199, 264))]\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "014a1c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3272, 67)\n",
      "(3272, 67)\n",
      "(3272, 67)\n",
      "(3272, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/87f8l4qn6lg35t626mq1bgx40000gn/T/ipykernel_63722/939233128.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['pseudo_label'] = final_pred_hard\n",
      "/var/folders/_t/87f8l4qn6lg35t626mq1bgx40000gn/T/ipykernel_63722/939233128.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['pseudo_label'] = final_pred_hard\n",
      "/var/folders/_t/87f8l4qn6lg35t626mq1bgx40000gn/T/ipykernel_63722/939233128.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['pseudo_label'] = final_pred_hard\n"
     ]
    }
   ],
   "source": [
    "df1['pseudo_label'] = final_pred_hard\n",
    "df2['pseudo_label'] = final_pred_hard\n",
    "df3['pseudo_label'] = final_pred_hard\n",
    "df4['pseudo_label'] = final_pred_hard\n",
    "\n",
    "print(df1[df1['pseudo_label'] == df1['true_label']].shape)\n",
    "print(df2[df2['pseudo_label'] == df2['true_label']].shape)\n",
    "print(df3[df3['pseudo_label'] == df3['true_label']].shape)\n",
    "print(df4[df4['pseudo_label'] == df4['true_label']].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df449d89",
   "metadata": {},
   "source": [
    "# Reshape and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7bfa67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>InceptionNet_1_class_1</th>\n",
       "      <th>InceptionNet_1_class_2</th>\n",
       "      <th>InceptionNet_1_class_3</th>\n",
       "      <th>InceptionNet_1_class_4</th>\n",
       "      <th>InceptionNet_1_class_5</th>\n",
       "      <th>InceptionNet_1_class_6</th>\n",
       "      <th>InceptionNet_1_class_7</th>\n",
       "      <th>InceptionNet_1_class_8</th>\n",
       "      <th>InceptionNet_1_class_9</th>\n",
       "      <th>...</th>\n",
       "      <th>MobileNet_V2_4_class_57</th>\n",
       "      <th>MobileNet_V2_4_class_58</th>\n",
       "      <th>MobileNet_V2_4_class_59</th>\n",
       "      <th>MobileNet_V2_4_class_60</th>\n",
       "      <th>MobileNet_V2_4_class_61</th>\n",
       "      <th>MobileNet_V2_4_class_62</th>\n",
       "      <th>MobileNet_V2_4_class_63</th>\n",
       "      <th>MobileNet_V2_4_class_64</th>\n",
       "      <th>MobileNet_V2_4_class_65</th>\n",
       "      <th>pseudo_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>5.271711e-08</td>\n",
       "      <td>8.214375e-09</td>\n",
       "      <td>9.422983e-09</td>\n",
       "      <td>1.073778e-09</td>\n",
       "      <td>3.919455e-07</td>\n",
       "      <td>9.068081e-07</td>\n",
       "      <td>1.051537e-07</td>\n",
       "      <td>7.904253e-09</td>\n",
       "      <td>8.876940e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.766251e-07</td>\n",
       "      <td>3.727825e-10</td>\n",
       "      <td>3.734709e-09</td>\n",
       "      <td>7.203157e-08</td>\n",
       "      <td>5.545787e-09</td>\n",
       "      <td>2.515807e-08</td>\n",
       "      <td>1.021777e-07</td>\n",
       "      <td>6.160776e-08</td>\n",
       "      <td>2.210914e-10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>5.970387e-06</td>\n",
       "      <td>1.712181e-07</td>\n",
       "      <td>2.003469e-06</td>\n",
       "      <td>1.324439e-06</td>\n",
       "      <td>1.319423e-06</td>\n",
       "      <td>1.367447e-06</td>\n",
       "      <td>1.114959e-06</td>\n",
       "      <td>9.275686e-01</td>\n",
       "      <td>6.915659e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.253019e-04</td>\n",
       "      <td>4.477683e-05</td>\n",
       "      <td>7.737304e-05</td>\n",
       "      <td>1.824193e-06</td>\n",
       "      <td>1.730803e-04</td>\n",
       "      <td>1.259049e-08</td>\n",
       "      <td>3.632041e-06</td>\n",
       "      <td>3.792483e-06</td>\n",
       "      <td>4.162863e-07</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>8.319048e-04</td>\n",
       "      <td>2.812022e-05</td>\n",
       "      <td>7.891122e-05</td>\n",
       "      <td>1.513799e-02</td>\n",
       "      <td>1.710256e-03</td>\n",
       "      <td>4.854570e-05</td>\n",
       "      <td>2.351476e-05</td>\n",
       "      <td>1.711687e-05</td>\n",
       "      <td>1.635728e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>4.432637e-04</td>\n",
       "      <td>1.994744e-03</td>\n",
       "      <td>2.892468e-02</td>\n",
       "      <td>1.874315e-02</td>\n",
       "      <td>9.240152e-03</td>\n",
       "      <td>2.608575e-03</td>\n",
       "      <td>3.587585e-04</td>\n",
       "      <td>1.729638e-04</td>\n",
       "      <td>2.334915e-04</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>5.056936e-13</td>\n",
       "      <td>2.460811e-12</td>\n",
       "      <td>2.693100e-13</td>\n",
       "      <td>1.241311e-13</td>\n",
       "      <td>2.725491e-12</td>\n",
       "      <td>6.051094e-12</td>\n",
       "      <td>9.999771e-01</td>\n",
       "      <td>2.680090e-13</td>\n",
       "      <td>1.796287e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>5.770333e-04</td>\n",
       "      <td>3.681877e-04</td>\n",
       "      <td>6.223977e-03</td>\n",
       "      <td>3.169568e-04</td>\n",
       "      <td>2.465677e-03</td>\n",
       "      <td>1.331138e-03</td>\n",
       "      <td>3.494845e-04</td>\n",
       "      <td>3.974282e-01</td>\n",
       "      <td>9.930512e-04</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>1.593175e-09</td>\n",
       "      <td>2.795414e-10</td>\n",
       "      <td>1.250389e-10</td>\n",
       "      <td>4.804293e-08</td>\n",
       "      <td>8.671065e-10</td>\n",
       "      <td>1.413309e-09</td>\n",
       "      <td>3.889157e-09</td>\n",
       "      <td>2.460848e-11</td>\n",
       "      <td>2.990766e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.482015e-06</td>\n",
       "      <td>9.991421e-06</td>\n",
       "      <td>2.754351e-05</td>\n",
       "      <td>7.820977e-04</td>\n",
       "      <td>4.904593e-03</td>\n",
       "      <td>1.390067e-04</td>\n",
       "      <td>2.840934e-03</td>\n",
       "      <td>1.216361e-04</td>\n",
       "      <td>4.244205e-04</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>6</td>\n",
       "      <td>1.001821e-07</td>\n",
       "      <td>5.556457e-09</td>\n",
       "      <td>1.118992e-07</td>\n",
       "      <td>1.858495e-09</td>\n",
       "      <td>2.039189e-08</td>\n",
       "      <td>2.832041e-06</td>\n",
       "      <td>9.998986e-01</td>\n",
       "      <td>4.310100e-09</td>\n",
       "      <td>1.668221e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.613673e-07</td>\n",
       "      <td>4.650745e-07</td>\n",
       "      <td>2.484209e-07</td>\n",
       "      <td>4.874250e-09</td>\n",
       "      <td>1.562865e-09</td>\n",
       "      <td>9.991079e-08</td>\n",
       "      <td>3.729173e-08</td>\n",
       "      <td>2.617291e-02</td>\n",
       "      <td>1.909165e-10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>21</td>\n",
       "      <td>1.762407e-02</td>\n",
       "      <td>8.314757e-06</td>\n",
       "      <td>1.775505e-06</td>\n",
       "      <td>5.172735e-06</td>\n",
       "      <td>1.821291e-03</td>\n",
       "      <td>4.952103e-07</td>\n",
       "      <td>7.500716e-05</td>\n",
       "      <td>1.121382e-05</td>\n",
       "      <td>7.612852e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.715256e-04</td>\n",
       "      <td>2.886083e-03</td>\n",
       "      <td>1.363019e-07</td>\n",
       "      <td>7.178323e-06</td>\n",
       "      <td>2.857074e-04</td>\n",
       "      <td>4.080899e-03</td>\n",
       "      <td>7.621653e-04</td>\n",
       "      <td>3.375111e-02</td>\n",
       "      <td>7.631975e-07</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>36</td>\n",
       "      <td>1.896000e-07</td>\n",
       "      <td>5.306263e-09</td>\n",
       "      <td>5.572599e-06</td>\n",
       "      <td>1.579984e-09</td>\n",
       "      <td>3.392033e-09</td>\n",
       "      <td>1.266143e-07</td>\n",
       "      <td>9.430685e-09</td>\n",
       "      <td>5.280787e-10</td>\n",
       "      <td>1.736073e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114061e-06</td>\n",
       "      <td>3.346800e-05</td>\n",
       "      <td>6.640473e-07</td>\n",
       "      <td>9.402354e-08</td>\n",
       "      <td>3.096250e-06</td>\n",
       "      <td>1.271762e-06</td>\n",
       "      <td>5.899246e-07</td>\n",
       "      <td>3.273698e-07</td>\n",
       "      <td>1.960391e-05</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>9</td>\n",
       "      <td>3.715895e-05</td>\n",
       "      <td>2.376575e-06</td>\n",
       "      <td>1.784199e-04</td>\n",
       "      <td>5.689888e-07</td>\n",
       "      <td>1.495891e-05</td>\n",
       "      <td>8.724782e-05</td>\n",
       "      <td>2.644503e-05</td>\n",
       "      <td>4.857977e-05</td>\n",
       "      <td>5.239484e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>7.405291e-04</td>\n",
       "      <td>8.604111e-05</td>\n",
       "      <td>1.674609e-04</td>\n",
       "      <td>1.211314e-04</td>\n",
       "      <td>2.499685e-05</td>\n",
       "      <td>5.173074e-05</td>\n",
       "      <td>4.500997e-04</td>\n",
       "      <td>4.772073e-05</td>\n",
       "      <td>1.203780e-05</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>63</td>\n",
       "      <td>5.388100e-04</td>\n",
       "      <td>1.319834e-02</td>\n",
       "      <td>1.321735e-03</td>\n",
       "      <td>5.462120e-06</td>\n",
       "      <td>1.691211e-04</td>\n",
       "      <td>3.649561e-01</td>\n",
       "      <td>1.366114e-04</td>\n",
       "      <td>3.035181e-05</td>\n",
       "      <td>1.750476e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.606308e-04</td>\n",
       "      <td>6.949893e-06</td>\n",
       "      <td>1.141677e-02</td>\n",
       "      <td>2.308521e-04</td>\n",
       "      <td>3.864342e-03</td>\n",
       "      <td>2.421903e-06</td>\n",
       "      <td>1.779990e-02</td>\n",
       "      <td>1.758284e-02</td>\n",
       "      <td>6.809386e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4350 rows × 262 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      true_label  InceptionNet_1_class_1  InceptionNet_1_class_2  \\\n",
       "0              9            5.271711e-08            8.214375e-09   \n",
       "1              7            5.970387e-06            1.712181e-07   \n",
       "2             18            8.319048e-04            2.812022e-05   \n",
       "3              6            5.056936e-13            2.460811e-12   \n",
       "4             31            1.593175e-09            2.795414e-10   \n",
       "...          ...                     ...                     ...   \n",
       "4345           6            1.001821e-07            5.556457e-09   \n",
       "4346          21            1.762407e-02            8.314757e-06   \n",
       "4347          36            1.896000e-07            5.306263e-09   \n",
       "4348           9            3.715895e-05            2.376575e-06   \n",
       "4349          63            5.388100e-04            1.319834e-02   \n",
       "\n",
       "      InceptionNet_1_class_3  InceptionNet_1_class_4  InceptionNet_1_class_5  \\\n",
       "0               9.422983e-09            1.073778e-09            3.919455e-07   \n",
       "1               2.003469e-06            1.324439e-06            1.319423e-06   \n",
       "2               7.891122e-05            1.513799e-02            1.710256e-03   \n",
       "3               2.693100e-13            1.241311e-13            2.725491e-12   \n",
       "4               1.250389e-10            4.804293e-08            8.671065e-10   \n",
       "...                      ...                     ...                     ...   \n",
       "4345            1.118992e-07            1.858495e-09            2.039189e-08   \n",
       "4346            1.775505e-06            5.172735e-06            1.821291e-03   \n",
       "4347            5.572599e-06            1.579984e-09            3.392033e-09   \n",
       "4348            1.784199e-04            5.689888e-07            1.495891e-05   \n",
       "4349            1.321735e-03            5.462120e-06            1.691211e-04   \n",
       "\n",
       "      InceptionNet_1_class_6  InceptionNet_1_class_7  InceptionNet_1_class_8  \\\n",
       "0               9.068081e-07            1.051537e-07            7.904253e-09   \n",
       "1               1.367447e-06            1.114959e-06            9.275686e-01   \n",
       "2               4.854570e-05            2.351476e-05            1.711687e-05   \n",
       "3               6.051094e-12            9.999771e-01            2.680090e-13   \n",
       "4               1.413309e-09            3.889157e-09            2.460848e-11   \n",
       "...                      ...                     ...                     ...   \n",
       "4345            2.832041e-06            9.998986e-01            4.310100e-09   \n",
       "4346            4.952103e-07            7.500716e-05            1.121382e-05   \n",
       "4347            1.266143e-07            9.430685e-09            5.280787e-10   \n",
       "4348            8.724782e-05            2.644503e-05            4.857977e-05   \n",
       "4349            3.649561e-01            1.366114e-04            3.035181e-05   \n",
       "\n",
       "      InceptionNet_1_class_9  ...  MobileNet_V2_4_class_57  \\\n",
       "0               8.876940e-10  ...             1.766251e-07   \n",
       "1               6.915659e-02  ...             1.253019e-04   \n",
       "2               1.635728e-03  ...             4.432637e-04   \n",
       "3               1.796287e-12  ...             5.770333e-04   \n",
       "4               2.990766e-09  ...             9.482015e-06   \n",
       "...                      ...  ...                      ...   \n",
       "4345            1.668221e-09  ...             2.613673e-07   \n",
       "4346            7.612852e-08  ...             1.715256e-04   \n",
       "4347            1.736073e-10  ...             1.114061e-06   \n",
       "4348            5.239484e-06  ...             7.405291e-04   \n",
       "4349            1.750476e-05  ...             2.606308e-04   \n",
       "\n",
       "      MobileNet_V2_4_class_58  MobileNet_V2_4_class_59  \\\n",
       "0                3.727825e-10             3.734709e-09   \n",
       "1                4.477683e-05             7.737304e-05   \n",
       "2                1.994744e-03             2.892468e-02   \n",
       "3                3.681877e-04             6.223977e-03   \n",
       "4                9.991421e-06             2.754351e-05   \n",
       "...                       ...                      ...   \n",
       "4345             4.650745e-07             2.484209e-07   \n",
       "4346             2.886083e-03             1.363019e-07   \n",
       "4347             3.346800e-05             6.640473e-07   \n",
       "4348             8.604111e-05             1.674609e-04   \n",
       "4349             6.949893e-06             1.141677e-02   \n",
       "\n",
       "      MobileNet_V2_4_class_60  MobileNet_V2_4_class_61  \\\n",
       "0                7.203157e-08             5.545787e-09   \n",
       "1                1.824193e-06             1.730803e-04   \n",
       "2                1.874315e-02             9.240152e-03   \n",
       "3                3.169568e-04             2.465677e-03   \n",
       "4                7.820977e-04             4.904593e-03   \n",
       "...                       ...                      ...   \n",
       "4345             4.874250e-09             1.562865e-09   \n",
       "4346             7.178323e-06             2.857074e-04   \n",
       "4347             9.402354e-08             3.096250e-06   \n",
       "4348             1.211314e-04             2.499685e-05   \n",
       "4349             2.308521e-04             3.864342e-03   \n",
       "\n",
       "      MobileNet_V2_4_class_62  MobileNet_V2_4_class_63  \\\n",
       "0                2.515807e-08             1.021777e-07   \n",
       "1                1.259049e-08             3.632041e-06   \n",
       "2                2.608575e-03             3.587585e-04   \n",
       "3                1.331138e-03             3.494845e-04   \n",
       "4                1.390067e-04             2.840934e-03   \n",
       "...                       ...                      ...   \n",
       "4345             9.991079e-08             3.729173e-08   \n",
       "4346             4.080899e-03             7.621653e-04   \n",
       "4347             1.271762e-06             5.899246e-07   \n",
       "4348             5.173074e-05             4.500997e-04   \n",
       "4349             2.421903e-06             1.779990e-02   \n",
       "\n",
       "      MobileNet_V2_4_class_64  MobileNet_V2_4_class_65  pseudo_label  \n",
       "0                6.160776e-08             2.210914e-10             9  \n",
       "1                3.792483e-06             4.162863e-07             7  \n",
       "2                1.729638e-04             2.334915e-04            48  \n",
       "3                3.974282e-01             9.930512e-04             6  \n",
       "4                1.216361e-04             4.244205e-04            15  \n",
       "...                       ...                      ...           ...  \n",
       "4345             2.617291e-02             1.909165e-10             6  \n",
       "4346             3.375111e-02             7.631975e-07            21  \n",
       "4347             3.273698e-07             1.960391e-05            36  \n",
       "4348             4.772073e-05             1.203780e-05             9  \n",
       "4349             1.758284e-02             6.809386e-07             1  \n",
       "\n",
       "[4350 rows x 262 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.drop(columns=[\"true_label\"])\n",
    "df3 = df3.drop(columns=[\"true_label\"])\n",
    "df4 = df4.drop(columns=[\"true_label\"])\n",
    "df1 = df1.drop(columns=[\"pseudo_label\"])\n",
    "df2 = df2.drop(columns=[\"pseudo_label\"])\n",
    "df3 = df3.drop(columns=[\"pseudo_label\"])\n",
    "\n",
    "df_pseudoLabel = pd.concat([df1, df2, df3, df4], axis = 1)\n",
    "csv_file_path = './Model Results/snet_mnet_inet_rnet_predictions_pseudolabels_full_voting.csv'\n",
    "df_pseudoLabel.to_csv(csv_file_path, index=False)\n",
    "df_pseudoLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f5507",
   "metadata": {},
   "source": [
    "# Build meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e6de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from scipy import stats\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import model_zoo\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Tuple, Sequence\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "#squeezenet\n",
    "from torchvision.transforms._presets import ImageClassification\n",
    "from torchvision.utils import _log_api_usage_once\n",
    "from torchvision.models._api import register_model, Weights, WeightsEnum\n",
    "from torchvision.models._meta import _IMAGENET_CATEGORIES\n",
    "from torchvision.models._utils import _ovewrite_named_param, handle_legacy_interface\n",
    "\n",
    "#unique to mobilenet\n",
    "from torchvision.ops.misc import Conv2dNormActivation\n",
    "from torchvision.models._utils import _make_divisible\n",
    "\n",
    "#time functions\n",
    "import time\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291f95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "class SingleLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        # Define a single fully connected layer\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the layer\n",
    "        x = self.fc(x)\n",
    "        # Apply a softmax activation function to get probabilities for each class\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "#*3, *2, 88, = 76%\n",
    "class MultiLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MultiLayerNN, self).__init__()\n",
    "        \n",
    "        # Initialize the layers\n",
    "        self.fc1 = nn.Linear(input_size, input_size*3)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(input_size*3, 65)  # Second hidden layer\n",
    "        self.fc3 = nn.Linear(65, 30)  # Second hidden layer\n",
    "        # Add more layers if needed\n",
    "        self.fc4 = nn.Linear(30, output_size)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the first layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Forward pass through the second layer with ReLU activation\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # Forward pass through the output layer\n",
    "        x = self.fc4(x)\n",
    "        # Apply a softmax activation function to get probabilities for each class\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9852970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>InceptionNet_1_class_1</th>\n",
       "      <th>InceptionNet_1_class_2</th>\n",
       "      <th>InceptionNet_1_class_3</th>\n",
       "      <th>InceptionNet_1_class_4</th>\n",
       "      <th>InceptionNet_1_class_5</th>\n",
       "      <th>InceptionNet_1_class_6</th>\n",
       "      <th>InceptionNet_1_class_7</th>\n",
       "      <th>InceptionNet_1_class_8</th>\n",
       "      <th>InceptionNet_1_class_9</th>\n",
       "      <th>...</th>\n",
       "      <th>MobileNet_V2_4_class_57</th>\n",
       "      <th>MobileNet_V2_4_class_58</th>\n",
       "      <th>MobileNet_V2_4_class_59</th>\n",
       "      <th>MobileNet_V2_4_class_60</th>\n",
       "      <th>MobileNet_V2_4_class_61</th>\n",
       "      <th>MobileNet_V2_4_class_62</th>\n",
       "      <th>MobileNet_V2_4_class_63</th>\n",
       "      <th>MobileNet_V2_4_class_64</th>\n",
       "      <th>MobileNet_V2_4_class_65</th>\n",
       "      <th>pseudo_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>5.271711e-08</td>\n",
       "      <td>8.214375e-09</td>\n",
       "      <td>9.422983e-09</td>\n",
       "      <td>1.073778e-09</td>\n",
       "      <td>3.919455e-07</td>\n",
       "      <td>9.068081e-07</td>\n",
       "      <td>1.051537e-07</td>\n",
       "      <td>7.904253e-09</td>\n",
       "      <td>8.876940e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.766251e-07</td>\n",
       "      <td>3.727825e-10</td>\n",
       "      <td>3.734709e-09</td>\n",
       "      <td>7.203157e-08</td>\n",
       "      <td>5.545787e-09</td>\n",
       "      <td>2.515807e-08</td>\n",
       "      <td>1.021777e-07</td>\n",
       "      <td>6.160776e-08</td>\n",
       "      <td>2.210914e-10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>5.970387e-06</td>\n",
       "      <td>1.712181e-07</td>\n",
       "      <td>2.003469e-06</td>\n",
       "      <td>1.324439e-06</td>\n",
       "      <td>1.319423e-06</td>\n",
       "      <td>1.367447e-06</td>\n",
       "      <td>1.114959e-06</td>\n",
       "      <td>9.275686e-01</td>\n",
       "      <td>6.915659e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.253019e-04</td>\n",
       "      <td>4.477683e-05</td>\n",
       "      <td>7.737304e-05</td>\n",
       "      <td>1.824193e-06</td>\n",
       "      <td>1.730803e-04</td>\n",
       "      <td>1.259049e-08</td>\n",
       "      <td>3.632041e-06</td>\n",
       "      <td>3.792483e-06</td>\n",
       "      <td>4.162863e-07</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 262 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_label  InceptionNet_1_class_1  InceptionNet_1_class_2  \\\n",
       "0           9            5.271711e-08            8.214375e-09   \n",
       "1           7            5.970387e-06            1.712181e-07   \n",
       "\n",
       "   InceptionNet_1_class_3  InceptionNet_1_class_4  InceptionNet_1_class_5  \\\n",
       "0            9.422983e-09            1.073778e-09            3.919455e-07   \n",
       "1            2.003469e-06            1.324439e-06            1.319423e-06   \n",
       "\n",
       "   InceptionNet_1_class_6  InceptionNet_1_class_7  InceptionNet_1_class_8  \\\n",
       "0            9.068081e-07            1.051537e-07            7.904253e-09   \n",
       "1            1.367447e-06            1.114959e-06            9.275686e-01   \n",
       "\n",
       "   InceptionNet_1_class_9  ...  MobileNet_V2_4_class_57  \\\n",
       "0            8.876940e-10  ...             1.766251e-07   \n",
       "1            6.915659e-02  ...             1.253019e-04   \n",
       "\n",
       "   MobileNet_V2_4_class_58  MobileNet_V2_4_class_59  MobileNet_V2_4_class_60  \\\n",
       "0             3.727825e-10             3.734709e-09             7.203157e-08   \n",
       "1             4.477683e-05             7.737304e-05             1.824193e-06   \n",
       "\n",
       "   MobileNet_V2_4_class_61  MobileNet_V2_4_class_62  MobileNet_V2_4_class_63  \\\n",
       "0             5.545787e-09             2.515807e-08             1.021777e-07   \n",
       "1             1.730803e-04             1.259049e-08             3.632041e-06   \n",
       "\n",
       "   MobileNet_V2_4_class_64  MobileNet_V2_4_class_65  pseudo_label  \n",
       "0             6.160776e-08             2.210914e-10             9  \n",
       "1             3.792483e-06             4.162863e-07             7  \n",
       "\n",
       "[2 rows x 262 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#snet_mnet_inet_rnet_predictions_pseudolabels_full_voting.csv\n",
    "#snet_mnet_inet_rnet_predictions_pseudolabels_full.csv\n",
    "csv_file_path = './Model Results/snet_mnet_inet_rnet_predictions_pseudolabels_full.csv'\n",
    "df_test = pd.read_csv(csv_file_path)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8779b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = df_test.iloc[:, 1:-1]\n",
    "pseudoLabel = df_test.iloc[:, -1]\n",
    "trueLabel = df_test.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad8bf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4350, 260)\n",
      "(4350,)\n",
      "(4350,)\n"
     ]
    }
   ],
   "source": [
    "print(Xs.shape)\n",
    "print(pseudoLabel.shape)\n",
    "print(trueLabel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b9e53",
   "metadata": {},
   "source": [
    "# Prepare DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2237b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size is 130 as there are predictions from 2 models for 65 classes\n",
    "input_size = 260\n",
    "# Output size is 65, corresponding to the final prediction across 65 classes\n",
    "output_size = 65\n",
    "\n",
    "# Create an instance of the neural network\n",
    "# ensemble_net = SingleLayerNN(input_size, output_size)\n",
    "ensemble_net = MultiLayerNN(input_size, output_size)\n",
    "\n",
    "# Convert the NumPy array to a PyTorch Tensor\n",
    "tensor_Xs = torch.tensor(Xs.to_numpy(), dtype=torch.float32)\n",
    "tensor_pseudoLabel = torch.tensor(pseudoLabel.to_numpy(), dtype=torch.float32).long()\n",
    "tensor_trueLabel = torch.tensor(trueLabel.to_numpy(), dtype=torch.float32).long()\n",
    "\n",
    "prediction_dataset_pseudoLabel = TensorDataset(tensor_Xs, tensor_pseudoLabel)\n",
    "prediction_dataset_trueLabel = TensorDataset(tensor_Xs, tensor_trueLabel)\n",
    "\n",
    "# Define batch size for DataLoader\n",
    "ensemble_batch_size = 500  # You can adjust this according to your requirements\n",
    "\n",
    "total_samples = len(prediction_dataset_pseudoLabel)\n",
    "train_size = int(0.7012 * total_samples)\n",
    "test_size = total_samples - train_size\n",
    "\n",
    "# Randomly split the dataset into train and test sets\n",
    "np.random.seed(610)\n",
    "train_dataset_pseudoLabel, test_dataset_pseudoLabel = random_split(prediction_dataset_pseudoLabel, [train_size, test_size])\n",
    "np.random.seed(610)\n",
    "train_dataset_trueLabel, test_dataset_trueLabel = random_split(prediction_dataset_trueLabel, [train_size, test_size])\n",
    "\n",
    "# Define batch size for DataLoader\n",
    "batch_size = 50  # Adjust this according to your requirements\n",
    "\n",
    "# Create DataLoaders for train and test sets\n",
    "\n",
    "#train with pseudo label\n",
    "train_loader_pseudoLabel = DataLoader(train_dataset_pseudoLabel, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#evaluate performance using true label\n",
    "test_loader_trueLabel = DataLoader(test_dataset_trueLabel, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6825707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(134780)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_dataset_pseudoLabel.dataset.tensors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c64258e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(134233)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_dataset_trueLabel.dataset.tensors[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bba680",
   "metadata": {},
   "source": [
    "# Training and testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "246e645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "3050\n",
      "26\n",
      "1300\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader_pseudoLabel))\n",
    "print(len(train_loader_pseudoLabel.dataset))\n",
    "print(len(test_loader_trueLabel))\n",
    "print(len(test_loader_trueLabel.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e0995",
   "metadata": {},
   "source": [
    "# Train an ensemble model (neural network) on top of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "445dce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000, Train Loss: 4.174183446852887, Train Accuracy: 1.9344262295081966%, Test Loss: 4.173968058366042, Test Accuracy: 1.8461538461538463%\n",
      "Saved the best model with validation loss:  4.173968058366042\n",
      "Epoch 2/2000, Train Loss: 4.173519587907635, Train Accuracy: 1.9344262295081966%, Test Loss: 4.173057207694421, Test Accuracy: 1.8461538461538463%\n",
      "Epoch 3/2000, Train Loss: 4.17165551420118, Train Accuracy: 1.9344262295081966%, Test Loss: 4.170204584415142, Test Accuracy: 2.3076923076923075%\n",
      "Saved the best model with validation loss:  4.170204584415142\n",
      "Epoch 4/2000, Train Loss: 4.163812387185018, Train Accuracy: 3.1475409836065573%, Test Loss: 4.15531613276555, Test Accuracy: 2.8461538461538463%\n",
      "Saved the best model with validation loss:  4.15531613276555\n",
      "Epoch 5/2000, Train Loss: 4.137827099346723, Train Accuracy: 3.901639344262295%, Test Loss: 4.1265378548548775, Test Accuracy: 5.923076923076923%\n",
      "Saved the best model with validation loss:  4.1265378548548775\n",
      "Epoch 6/2000, Train Loss: 4.093876268042893, Train Accuracy: 17.18032786885246%, Test Loss: 4.069185091898992, Test Accuracy: 22.076923076923077%\n",
      "Saved the best model with validation loss:  4.069185091898992\n",
      "Epoch 7/2000, Train Loss: 4.018716569806709, Train Accuracy: 27.9672131147541%, Test Loss: 3.996308390910809, Test Accuracy: 26.923076923076923%\n",
      "Saved the best model with validation loss:  3.996308390910809\n",
      "Epoch 8/2000, Train Loss: 3.939424690653066, Train Accuracy: 35.24590163934426%, Test Loss: 3.935233372908372, Test Accuracy: 35.46153846153846%\n",
      "Saved the best model with validation loss:  3.935233372908372\n",
      "Epoch 9/2000, Train Loss: 3.868475652131878, Train Accuracy: 43.83606557377049%, Test Loss: 3.8749280892885647, Test Accuracy: 40.23076923076923%\n",
      "Saved the best model with validation loss:  3.8749280892885647\n",
      "Epoch 10/2000, Train Loss: 3.8005598373100407, Train Accuracy: 48.19672131147541%, Test Loss: 3.8223187373234677, Test Accuracy: 41.69230769230769%\n",
      "Saved the best model with validation loss:  3.8223187373234677\n",
      "Epoch 11/2000, Train Loss: 3.741475953430426, Train Accuracy: 54.22950819672131%, Test Loss: 3.779265229518597, Test Accuracy: 50.53846153846154%\n",
      "Saved the best model with validation loss:  3.779265229518597\n",
      "Epoch 12/2000, Train Loss: 3.6923620388156078, Train Accuracy: 60.85245901639344%, Test Loss: 3.736766365858225, Test Accuracy: 53.23076923076923%\n",
      "Saved the best model with validation loss:  3.736766365858225\n",
      "Epoch 13/2000, Train Loss: 3.6429639253459993, Train Accuracy: 63.967213114754095%, Test Loss: 3.6976215839385986, Test Accuracy: 55.38461538461539%\n",
      "Saved the best model with validation loss:  3.6976215839385986\n",
      "Epoch 14/2000, Train Loss: 3.6010269806033275, Train Accuracy: 66.55737704918033%, Test Loss: 3.6698688085262594, Test Accuracy: 57.07692307692308%\n",
      "Saved the best model with validation loss:  3.6698688085262594\n",
      "Epoch 15/2000, Train Loss: 3.5725128416155205, Train Accuracy: 68.78688524590164%, Test Loss: 3.6510364825908956, Test Accuracy: 58.76923076923077%\n",
      "Saved the best model with validation loss:  3.6510364825908956\n",
      "Epoch 16/2000, Train Loss: 3.550578265893655, Train Accuracy: 70.42622950819673%, Test Loss: 3.6360118297430186, Test Accuracy: 60.46153846153846%\n",
      "Saved the best model with validation loss:  3.6360118297430186\n",
      "Epoch 17/2000, Train Loss: 3.5321410132236166, Train Accuracy: 72.52459016393442%, Test Loss: 3.620790325678312, Test Accuracy: 61.30769230769231%\n",
      "Saved the best model with validation loss:  3.620790325678312\n",
      "Epoch 18/2000, Train Loss: 3.5148865668500058, Train Accuracy: 73.31147540983606%, Test Loss: 3.6082314252853394, Test Accuracy: 62.30769230769231%\n",
      "Saved the best model with validation loss:  3.6082314252853394\n",
      "Epoch 19/2000, Train Loss: 3.4995345326720693, Train Accuracy: 74.52459016393442%, Test Loss: 3.5968009050075826, Test Accuracy: 63.07692307692308%\n",
      "Saved the best model with validation loss:  3.5968009050075826\n",
      "Epoch 20/2000, Train Loss: 3.483366114194276, Train Accuracy: 77.04918032786885%, Test Loss: 3.5840580555108876, Test Accuracy: 65.3076923076923%\n",
      "Saved the best model with validation loss:  3.5840580555108876\n",
      "Epoch 21/2000, Train Loss: 3.4606192932754265, Train Accuracy: 80.0%, Test Loss: 3.570687413215637, Test Accuracy: 66.46153846153847%\n",
      "Saved the best model with validation loss:  3.570687413215637\n",
      "Epoch 22/2000, Train Loss: 3.440762816882524, Train Accuracy: 81.54098360655738%, Test Loss: 3.560471846507146, Test Accuracy: 67.15384615384616%\n",
      "Saved the best model with validation loss:  3.560471846507146\n",
      "Epoch 23/2000, Train Loss: 3.4273999909885595, Train Accuracy: 82.22950819672131%, Test Loss: 3.551051213191106, Test Accuracy: 67.53846153846153%\n",
      "Saved the best model with validation loss:  3.551051213191106\n",
      "Epoch 24/2000, Train Loss: 3.4165677907036955, Train Accuracy: 82.45901639344262%, Test Loss: 3.543272385230431, Test Accuracy: 67.76923076923077%\n",
      "Saved the best model with validation loss:  3.543272385230431\n",
      "Epoch 25/2000, Train Loss: 3.408065932695983, Train Accuracy: 83.11475409836065%, Test Loss: 3.538917504824125, Test Accuracy: 68.23076923076923%\n",
      "Saved the best model with validation loss:  3.538917504824125\n",
      "Epoch 26/2000, Train Loss: 3.4003479128978293, Train Accuracy: 83.60655737704919%, Test Loss: 3.53477471608382, Test Accuracy: 68.15384615384616%\n",
      "Saved the best model with validation loss:  3.53477471608382\n",
      "Epoch 27/2000, Train Loss: 3.3945373003600072, Train Accuracy: 83.77049180327869%, Test Loss: 3.5326523780822754, Test Accuracy: 68.07692307692308%\n",
      "Saved the best model with validation loss:  3.5326523780822754\n",
      "Epoch 28/2000, Train Loss: 3.39002601045077, Train Accuracy: 83.9672131147541%, Test Loss: 3.5306028311069193, Test Accuracy: 68.07692307692308%\n",
      "Saved the best model with validation loss:  3.5306028311069193\n",
      "Epoch 29/2000, Train Loss: 3.3862875602284417, Train Accuracy: 83.93442622950819%, Test Loss: 3.5290489746974063, Test Accuracy: 68.07692307692308%\n",
      "Saved the best model with validation loss:  3.5290489746974063\n",
      "Epoch 30/2000, Train Loss: 3.382729678857522, Train Accuracy: 84.1311475409836%, Test Loss: 3.527773692057683, Test Accuracy: 68.15384615384616%\n",
      "Saved the best model with validation loss:  3.527773692057683\n",
      "Epoch 31/2000, Train Loss: 3.379867467723909, Train Accuracy: 84.22950819672131%, Test Loss: 3.5269868557269755, Test Accuracy: 68.0%\n",
      "Epoch 32/2000, Train Loss: 3.377342317925125, Train Accuracy: 84.32786885245902%, Test Loss: 3.5261504650115967, Test Accuracy: 68.07692307692308%\n",
      "Saved the best model with validation loss:  3.5261504650115967\n",
      "Epoch 33/2000, Train Loss: 3.3750774156851846, Train Accuracy: 84.55737704918033%, Test Loss: 3.5255878613545346, Test Accuracy: 68.0%\n",
      "Epoch 34/2000, Train Loss: 3.3730807734317465, Train Accuracy: 84.62295081967213%, Test Loss: 3.52501891209529, Test Accuracy: 68.0%\n",
      "Saved the best model with validation loss:  3.52501891209529\n",
      "Epoch 35/2000, Train Loss: 3.3711681600476875, Train Accuracy: 84.62295081967213%, Test Loss: 3.5241084740712094, Test Accuracy: 68.07692307692308%\n",
      "Epoch 36/2000, Train Loss: 3.3694969786972297, Train Accuracy: 84.65573770491804%, Test Loss: 3.5241358188482432, Test Accuracy: 68.15384615384616%\n",
      "Epoch 37/2000, Train Loss: 3.3677502186571964, Train Accuracy: 84.85245901639344%, Test Loss: 3.523333952977107, Test Accuracy: 68.15384615384616%\n",
      "Saved the best model with validation loss:  3.523333952977107\n",
      "Epoch 38/2000, Train Loss: 3.3663483604055937, Train Accuracy: 84.85245901639344%, Test Loss: 3.5229264314358053, Test Accuracy: 68.07692307692308%\n",
      "Epoch 39/2000, Train Loss: 3.3648014967558812, Train Accuracy: 84.95081967213115%, Test Loss: 3.522818611218379, Test Accuracy: 68.07692307692308%\n",
      "Epoch 40/2000, Train Loss: 3.3634993951828753, Train Accuracy: 85.01639344262296%, Test Loss: 3.522559569432185, Test Accuracy: 68.07692307692308%\n",
      "Epoch 41/2000, Train Loss: 3.3623239525028916, Train Accuracy: 85.04918032786885%, Test Loss: 3.5221913044269266, Test Accuracy: 68.07692307692308%\n",
      "Saved the best model with validation loss:  3.5221913044269266\n",
      "Epoch 42/2000, Train Loss: 3.3608696734318966, Train Accuracy: 85.31147540983606%, Test Loss: 3.5210965871810913, Test Accuracy: 68.46153846153847%\n",
      "Saved the best model with validation loss:  3.5210965871810913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/2000, Train Loss: 3.354158440574271, Train Accuracy: 86.1311475409836%, Test Loss: 3.518963932991028, Test Accuracy: 68.61538461538461%\n",
      "Saved the best model with validation loss:  3.518963932991028\n",
      "Epoch 44/2000, Train Loss: 3.3506160525024913, Train Accuracy: 86.45901639344262%, Test Loss: 3.517828446168166, Test Accuracy: 68.61538461538461%\n",
      "Saved the best model with validation loss:  3.517828446168166\n",
      "Epoch 45/2000, Train Loss: 3.3487391393692767, Train Accuracy: 86.52459016393442%, Test Loss: 3.5173769088891835, Test Accuracy: 68.61538461538461%\n",
      "Epoch 46/2000, Train Loss: 3.3470191408376224, Train Accuracy: 86.59016393442623%, Test Loss: 3.5168441167244544, Test Accuracy: 68.61538461538461%\n",
      "Epoch 47/2000, Train Loss: 3.3456284100892115, Train Accuracy: 86.59016393442623%, Test Loss: 3.5162807427919827, Test Accuracy: 68.76923076923077%\n",
      "Saved the best model with validation loss:  3.5162807427919827\n",
      "Epoch 48/2000, Train Loss: 3.3446312107023646, Train Accuracy: 86.55737704918033%, Test Loss: 3.5161633583215566, Test Accuracy: 68.61538461538461%\n",
      "Epoch 49/2000, Train Loss: 3.3436398193484447, Train Accuracy: 86.65573770491804%, Test Loss: 3.515735332782452, Test Accuracy: 68.76923076923077%\n",
      "Epoch 50/2000, Train Loss: 3.3427897242249034, Train Accuracy: 86.65573770491804%, Test Loss: 3.5154149257219753, Test Accuracy: 68.6923076923077%\n",
      "Epoch 51/2000, Train Loss: 3.341933191799727, Train Accuracy: 86.68852459016394%, Test Loss: 3.5153723863454966, Test Accuracy: 68.76923076923077%\n",
      "Epoch 52/2000, Train Loss: 3.3412815586465303, Train Accuracy: 86.75409836065573%, Test Loss: 3.5154376855263343, Test Accuracy: 68.84615384615384%\n",
      "Epoch 53/2000, Train Loss: 3.3405318182022845, Train Accuracy: 86.75409836065573%, Test Loss: 3.515023809212905, Test Accuracy: 68.84615384615384%\n",
      "Saved the best model with validation loss:  3.515023809212905\n",
      "Epoch 54/2000, Train Loss: 3.3399203214489046, Train Accuracy: 86.78688524590164%, Test Loss: 3.514802620961116, Test Accuracy: 68.84615384615384%\n",
      "Epoch 55/2000, Train Loss: 3.339364411400967, Train Accuracy: 86.81967213114754%, Test Loss: 3.5145020393224864, Test Accuracy: 68.84615384615384%\n",
      "Epoch 56/2000, Train Loss: 3.3387748608823684, Train Accuracy: 86.81967213114754%, Test Loss: 3.5143730548711924, Test Accuracy: 68.84615384615384%\n",
      "Epoch 57/2000, Train Loss: 3.3382655636208955, Train Accuracy: 86.85245901639344%, Test Loss: 3.5141108402839074, Test Accuracy: 68.92307692307692%\n",
      "Epoch 58/2000, Train Loss: 3.33779583211805, Train Accuracy: 86.81967213114754%, Test Loss: 3.5140152986233053, Test Accuracy: 68.84615384615384%\n",
      "Saved the best model with validation loss:  3.5140152986233053\n",
      "Epoch 59/2000, Train Loss: 3.3373749177964007, Train Accuracy: 86.81967213114754%, Test Loss: 3.51356289936946, Test Accuracy: 69.0%\n",
      "Epoch 60/2000, Train Loss: 3.3368208799205843, Train Accuracy: 86.88524590163935%, Test Loss: 3.5136604400781484, Test Accuracy: 68.84615384615384%\n",
      "Epoch 61/2000, Train Loss: 3.336441266732138, Train Accuracy: 86.95081967213115%, Test Loss: 3.513148206930894, Test Accuracy: 69.07692307692308%\n",
      "Epoch 62/2000, Train Loss: 3.3359988282938473, Train Accuracy: 86.95081967213115%, Test Loss: 3.5133203818247867, Test Accuracy: 68.92307692307692%\n",
      "Epoch 63/2000, Train Loss: 3.33554714624999, Train Accuracy: 86.98360655737704%, Test Loss: 3.513047456741333, Test Accuracy: 68.92307692307692%\n",
      "Epoch 64/2000, Train Loss: 3.335172875982816, Train Accuracy: 87.04918032786885%, Test Loss: 3.5127074168278623, Test Accuracy: 69.15384615384616%\n",
      "Saved the best model with validation loss:  3.5127074168278623\n",
      "Epoch 65/2000, Train Loss: 3.3347552526192588, Train Accuracy: 86.98360655737704%, Test Loss: 3.5128513207802405, Test Accuracy: 69.07692307692308%\n",
      "Epoch 66/2000, Train Loss: 3.334260577061137, Train Accuracy: 87.11475409836065%, Test Loss: 3.5124740600585938, Test Accuracy: 69.15384615384616%\n",
      "Epoch 67/2000, Train Loss: 3.333845193268823, Train Accuracy: 87.04918032786885%, Test Loss: 3.512236026617197, Test Accuracy: 69.15384615384616%\n",
      "Epoch 68/2000, Train Loss: 3.333470965995163, Train Accuracy: 87.11475409836065%, Test Loss: 3.5120698672074537, Test Accuracy: 69.23076923076923%\n",
      "Epoch 69/2000, Train Loss: 3.3331715747958324, Train Accuracy: 87.08196721311475%, Test Loss: 3.51196957551516, Test Accuracy: 69.15384615384616%\n",
      "Epoch 70/2000, Train Loss: 3.332880371906718, Train Accuracy: 87.14754098360656%, Test Loss: 3.511529876635625, Test Accuracy: 69.15384615384616%\n",
      "Saved the best model with validation loss:  3.511529876635625\n",
      "Epoch 71/2000, Train Loss: 3.3326817848643318, Train Accuracy: 87.11475409836065%, Test Loss: 3.511640869654142, Test Accuracy: 69.07692307692308%\n",
      "Epoch 72/2000, Train Loss: 3.3324448163392115, Train Accuracy: 87.14754098360656%, Test Loss: 3.5115398168563843, Test Accuracy: 69.46153846153847%\n",
      "Epoch 73/2000, Train Loss: 3.33223150597244, Train Accuracy: 87.14754098360656%, Test Loss: 3.511770587701064, Test Accuracy: 69.15384615384616%\n",
      "Epoch 74/2000, Train Loss: 3.331974506378174, Train Accuracy: 87.14754098360656%, Test Loss: 3.511464164807246, Test Accuracy: 69.3076923076923%\n",
      "Epoch 75/2000, Train Loss: 3.331811119298466, Train Accuracy: 87.14754098360656%, Test Loss: 3.51162326335907, Test Accuracy: 69.15384615384616%\n",
      "Epoch 76/2000, Train Loss: 3.3315451262427156, Train Accuracy: 87.18032786885246%, Test Loss: 3.5117891476704526, Test Accuracy: 69.15384615384616%\n",
      "Epoch 77/2000, Train Loss: 3.3313957003296397, Train Accuracy: 87.21311475409836%, Test Loss: 3.511521421946012, Test Accuracy: 69.15384615384616%\n",
      "Epoch 78/2000, Train Loss: 3.331046546091799, Train Accuracy: 87.21311475409836%, Test Loss: 3.5112958321204553, Test Accuracy: 69.3076923076923%\n",
      "Epoch 79/2000, Train Loss: 3.3308977455389304, Train Accuracy: 87.21311475409836%, Test Loss: 3.5116902589797974, Test Accuracy: 69.23076923076923%\n",
      "Epoch 80/2000, Train Loss: 3.3308085652648427, Train Accuracy: 87.21311475409836%, Test Loss: 3.5118366479873657, Test Accuracy: 69.07692307692308%\n",
      "Epoch 81/2000, Train Loss: 3.330662989225544, Train Accuracy: 87.24590163934427%, Test Loss: 3.5117326241273146, Test Accuracy: 69.07692307692308%\n",
      "Epoch 82/2000, Train Loss: 3.3304232378475, Train Accuracy: 87.24590163934427%, Test Loss: 3.511244856394254, Test Accuracy: 69.23076923076923%\n",
      "Epoch 83/2000, Train Loss: 3.330250865123311, Train Accuracy: 87.24590163934427%, Test Loss: 3.511477543757512, Test Accuracy: 69.23076923076923%\n",
      "Epoch 84/2000, Train Loss: 3.3300027651864976, Train Accuracy: 87.24590163934427%, Test Loss: 3.5114960487072286, Test Accuracy: 69.15384615384616%\n",
      "Epoch 85/2000, Train Loss: 3.329939424014482, Train Accuracy: 87.24590163934427%, Test Loss: 3.5116017048175516, Test Accuracy: 69.07692307692308%\n",
      "Epoch 86/2000, Train Loss: 3.3298605582753167, Train Accuracy: 87.24590163934427%, Test Loss: 3.511522678228525, Test Accuracy: 69.07692307692308%\n",
      "Epoch 87/2000, Train Loss: 3.3297122228341025, Train Accuracy: 87.24590163934427%, Test Loss: 3.5113970316373386, Test Accuracy: 69.3076923076923%\n",
      "Epoch 88/2000, Train Loss: 3.3295847509728103, Train Accuracy: 87.24590163934427%, Test Loss: 3.511346908716055, Test Accuracy: 69.3076923076923%\n",
      "Epoch 89/2000, Train Loss: 3.3294516547781523, Train Accuracy: 87.24590163934427%, Test Loss: 3.511467328438392, Test Accuracy: 69.07692307692308%\n",
      "Epoch 90/2000, Train Loss: 3.32934594545208, Train Accuracy: 87.24590163934427%, Test Loss: 3.511504182448754, Test Accuracy: 69.07692307692308%\n",
      "Epoch 91/2000, Train Loss: 3.3292692762906433, Train Accuracy: 87.24590163934427%, Test Loss: 3.5115016515438375, Test Accuracy: 69.23076923076923%\n",
      "Epoch 92/2000, Train Loss: 3.329178532616037, Train Accuracy: 87.24590163934427%, Test Loss: 3.511359013043917, Test Accuracy: 69.23076923076923%\n",
      "Epoch 93/2000, Train Loss: 3.3290852249645795, Train Accuracy: 87.24590163934427%, Test Loss: 3.5116250148186317, Test Accuracy: 69.15384615384616%\n",
      "Epoch 94/2000, Train Loss: 3.3290142465810306, Train Accuracy: 87.24590163934427%, Test Loss: 3.5114436608094435, Test Accuracy: 69.15384615384616%\n",
      "Epoch 95/2000, Train Loss: 3.3289436981326244, Train Accuracy: 87.24590163934427%, Test Loss: 3.511565923690796, Test Accuracy: 69.23076923076923%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/2000, Train Loss: 3.3289076382996607, Train Accuracy: 87.24590163934427%, Test Loss: 3.5119242393053494, Test Accuracy: 69.07692307692308%\n",
      "Epoch 97/2000, Train Loss: 3.3288282292788147, Train Accuracy: 87.24590163934427%, Test Loss: 3.5111925968757043, Test Accuracy: 69.23076923076923%\n",
      "Epoch 98/2000, Train Loss: 3.3287823395650893, Train Accuracy: 87.24590163934427%, Test Loss: 3.51162904042464, Test Accuracy: 69.15384615384616%\n",
      "Epoch 99/2000, Train Loss: 3.328663067739518, Train Accuracy: 87.24590163934427%, Test Loss: 3.5114541237170878, Test Accuracy: 69.07692307692308%\n",
      "Epoch 100/2000, Train Loss: 3.3285908269100504, Train Accuracy: 87.24590163934427%, Test Loss: 3.5117675891289344, Test Accuracy: 69.15384615384616%\n",
      "Epoch 101/2000, Train Loss: 3.3284844726812644, Train Accuracy: 87.27868852459017%, Test Loss: 3.511664684002216, Test Accuracy: 69.07692307692308%\n",
      "Epoch 102/2000, Train Loss: 3.32842860847223, Train Accuracy: 87.27868852459017%, Test Loss: 3.5118990402955275, Test Accuracy: 69.0%\n",
      "Epoch 103/2000, Train Loss: 3.328290259251829, Train Accuracy: 87.27868852459017%, Test Loss: 3.5112233070226817, Test Accuracy: 69.23076923076923%\n",
      "Epoch 104/2000, Train Loss: 3.3282705838563014, Train Accuracy: 87.27868852459017%, Test Loss: 3.5114687772897573, Test Accuracy: 69.07692307692308%\n",
      "Epoch 105/2000, Train Loss: 3.3281060125007005, Train Accuracy: 87.27868852459017%, Test Loss: 3.5113507325832662, Test Accuracy: 69.23076923076923%\n",
      "Epoch 106/2000, Train Loss: 3.3279765707547546, Train Accuracy: 87.31147540983606%, Test Loss: 3.5110787611741285, Test Accuracy: 69.15384615384616%\n",
      "Epoch 107/2000, Train Loss: 3.32788323965229, Train Accuracy: 87.31147540983606%, Test Loss: 3.5116268946574283, Test Accuracy: 69.07692307692308%\n",
      "Epoch 108/2000, Train Loss: 3.3277350566426263, Train Accuracy: 87.31147540983606%, Test Loss: 3.5112018768603983, Test Accuracy: 69.07692307692308%\n",
      "Epoch 109/2000, Train Loss: 3.3276858837878116, Train Accuracy: 87.31147540983606%, Test Loss: 3.5111424922943115, Test Accuracy: 69.23076923076923%\n",
      "Epoch 110/2000, Train Loss: 3.3276826162807276, Train Accuracy: 87.31147540983606%, Test Loss: 3.5111122864943285, Test Accuracy: 69.07692307692308%\n",
      "Epoch 111/2000, Train Loss: 3.32757330331646, Train Accuracy: 87.31147540983606%, Test Loss: 3.5113319617051344, Test Accuracy: 69.23076923076923%\n",
      "Epoch 112/2000, Train Loss: 3.327526436477411, Train Accuracy: 87.34426229508196%, Test Loss: 3.5116483248197117, Test Accuracy: 68.92307692307692%\n",
      "Epoch 113/2000, Train Loss: 3.327452456364866, Train Accuracy: 87.34426229508196%, Test Loss: 3.511086188829862, Test Accuracy: 69.23076923076923%\n",
      "Epoch 114/2000, Train Loss: 3.327424041560439, Train Accuracy: 87.34426229508196%, Test Loss: 3.511358068539546, Test Accuracy: 69.15384615384616%\n",
      "Epoch 115/2000, Train Loss: 3.3272620849922054, Train Accuracy: 87.34426229508196%, Test Loss: 3.5112696335865903, Test Accuracy: 69.15384615384616%\n",
      "Epoch 116/2000, Train Loss: 3.3272575159541895, Train Accuracy: 87.34426229508196%, Test Loss: 3.5116738906273475, Test Accuracy: 68.92307692307692%\n",
      "Epoch 117/2000, Train Loss: 3.327245114279575, Train Accuracy: 87.34426229508196%, Test Loss: 3.5113985996979933, Test Accuracy: 69.0%\n",
      "Epoch 118/2000, Train Loss: 3.3271410426155468, Train Accuracy: 87.34426229508196%, Test Loss: 3.511298628953787, Test Accuracy: 69.23076923076923%\n",
      "Epoch 119/2000, Train Loss: 3.3271455178495315, Train Accuracy: 87.34426229508196%, Test Loss: 3.511614524401151, Test Accuracy: 69.15384615384616%\n",
      "Epoch 120/2000, Train Loss: 3.3270663589727683, Train Accuracy: 87.34426229508196%, Test Loss: 3.5114928025465746, Test Accuracy: 68.84615384615384%\n",
      "Epoch 121/2000, Train Loss: 3.3270545865668626, Train Accuracy: 87.34426229508196%, Test Loss: 3.51131063241225, Test Accuracy: 69.15384615384616%\n",
      "Epoch 122/2000, Train Loss: 3.3270040574620983, Train Accuracy: 87.34426229508196%, Test Loss: 3.5112373186991763, Test Accuracy: 69.15384615384616%\n",
      "Epoch 123/2000, Train Loss: 3.327047981199671, Train Accuracy: 87.34426229508196%, Test Loss: 3.5114096861619215, Test Accuracy: 69.15384615384616%\n",
      "Epoch 124/2000, Train Loss: 3.3270222671696397, Train Accuracy: 87.34426229508196%, Test Loss: 3.5117025650464573, Test Accuracy: 68.6923076923077%\n",
      "Epoch 125/2000, Train Loss: 3.3269537824099182, Train Accuracy: 87.34426229508196%, Test Loss: 3.511248359313378, Test Accuracy: 69.15384615384616%\n",
      "Epoch 126/2000, Train Loss: 3.326906419191204, Train Accuracy: 87.34426229508196%, Test Loss: 3.5114382322017965, Test Accuracy: 68.76923076923077%\n",
      "Epoch 127/2000, Train Loss: 3.326737142000042, Train Accuracy: 87.37704918032787%, Test Loss: 3.511369466781616, Test Accuracy: 69.07692307692308%\n",
      "Epoch 128/2000, Train Loss: 3.3266365371766637, Train Accuracy: 87.37704918032787%, Test Loss: 3.511621649448688, Test Accuracy: 68.84615384615384%\n",
      "Epoch 129/2000, Train Loss: 3.3264313760350963, Train Accuracy: 87.40983606557377%, Test Loss: 3.5112408858079176, Test Accuracy: 69.07692307692308%\n",
      "Epoch 130/2000, Train Loss: 3.326334406117924, Train Accuracy: 87.40983606557377%, Test Loss: 3.511634716620812, Test Accuracy: 68.76923076923077%\n",
      "Epoch 131/2000, Train Loss: 3.326302016367678, Train Accuracy: 87.40983606557377%, Test Loss: 3.511489308797396, Test Accuracy: 68.92307692307692%\n",
      "Epoch 132/2000, Train Loss: 3.3262359470617575, Train Accuracy: 87.40983606557377%, Test Loss: 3.511513535793011, Test Accuracy: 68.84615384615384%\n",
      "Epoch 133/2000, Train Loss: 3.3262266291946663, Train Accuracy: 87.40983606557377%, Test Loss: 3.5114008555045495, Test Accuracy: 69.0%\n",
      "Epoch 134/2000, Train Loss: 3.326154701045302, Train Accuracy: 87.40983606557377%, Test Loss: 3.511011141997117, Test Accuracy: 69.23076923076923%\n",
      "Epoch 135/2000, Train Loss: 3.3261773312678105, Train Accuracy: 87.40983606557377%, Test Loss: 3.5113503015958347, Test Accuracy: 69.0%\n",
      "Epoch 136/2000, Train Loss: 3.3260329903149213, Train Accuracy: 87.44262295081967%, Test Loss: 3.511086078790518, Test Accuracy: 69.0%\n",
      "Epoch 137/2000, Train Loss: 3.3259832272764114, Train Accuracy: 87.44262295081967%, Test Loss: 3.511050691971412, Test Accuracy: 69.0%\n",
      "Epoch 138/2000, Train Loss: 3.3258031508961663, Train Accuracy: 87.44262295081967%, Test Loss: 3.511071810355553, Test Accuracy: 69.07692307692308%\n",
      "Epoch 139/2000, Train Loss: 3.3257227608414945, Train Accuracy: 87.47540983606558%, Test Loss: 3.5111712217330933, Test Accuracy: 68.92307692307692%\n",
      "Epoch 140/2000, Train Loss: 3.3257235347247516, Train Accuracy: 87.47540983606558%, Test Loss: 3.5112988215226393, Test Accuracy: 69.0%\n",
      "Epoch 141/2000, Train Loss: 3.3254654485671247, Train Accuracy: 87.47540983606558%, Test Loss: 3.5105377527383657, Test Accuracy: 69.07692307692308%\n",
      "Epoch 142/2000, Train Loss: 3.322271014823288, Train Accuracy: 87.8688524590164%, Test Loss: 3.5061313188993015, Test Accuracy: 69.6923076923077%\n",
      "Saved the best model with validation loss:  3.5061313188993015\n",
      "Epoch 143/2000, Train Loss: 3.319825297496358, Train Accuracy: 88.1639344262295%, Test Loss: 3.5053746791986318, Test Accuracy: 69.6923076923077%\n",
      "Epoch 144/2000, Train Loss: 3.318772921796705, Train Accuracy: 88.19672131147541%, Test Loss: 3.505410460325388, Test Accuracy: 69.6923076923077%\n",
      "Epoch 145/2000, Train Loss: 3.3184319597775818, Train Accuracy: 88.32786885245902%, Test Loss: 3.5056827526826124, Test Accuracy: 69.6923076923077%\n",
      "Epoch 146/2000, Train Loss: 3.317457812731383, Train Accuracy: 88.39344262295081%, Test Loss: 3.5057392028661876, Test Accuracy: 69.61538461538461%\n",
      "Epoch 147/2000, Train Loss: 3.3170783949680014, Train Accuracy: 88.39344262295081%, Test Loss: 3.505819219809312, Test Accuracy: 69.46153846153847%\n",
      "Epoch 148/2000, Train Loss: 3.3169447554916633, Train Accuracy: 88.39344262295081%, Test Loss: 3.5052778262358446, Test Accuracy: 69.46153846153847%\n",
      "Epoch 149/2000, Train Loss: 3.3166666499903945, Train Accuracy: 88.39344262295081%, Test Loss: 3.505878971173213, Test Accuracy: 69.46153846153847%\n",
      "Epoch 150/2000, Train Loss: 3.316596703451188, Train Accuracy: 88.42622950819673%, Test Loss: 3.5053433730052066, Test Accuracy: 69.46153846153847%\n",
      "Epoch 151/2000, Train Loss: 3.316451729321089, Train Accuracy: 88.42622950819673%, Test Loss: 3.505302520898672, Test Accuracy: 69.38461538461539%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/2000, Train Loss: 3.316321505874884, Train Accuracy: 88.42622950819673%, Test Loss: 3.5053388613920946, Test Accuracy: 69.38461538461539%\n",
      "Epoch 153/2000, Train Loss: 3.3162390364975227, Train Accuracy: 88.42622950819673%, Test Loss: 3.505292461468623, Test Accuracy: 69.38461538461539%\n",
      "Epoch 154/2000, Train Loss: 3.3162233204138083, Train Accuracy: 88.42622950819673%, Test Loss: 3.5051044959288378, Test Accuracy: 69.38461538461539%\n",
      "Saved the best model with validation loss:  3.5051044959288378\n",
      "Epoch 155/2000, Train Loss: 3.3161090592868994, Train Accuracy: 88.42622950819673%, Test Loss: 3.505373551295354, Test Accuracy: 69.38461538461539%\n",
      "Epoch 156/2000, Train Loss: 3.3159830570220947, Train Accuracy: 88.45901639344262%, Test Loss: 3.505096206298241, Test Accuracy: 69.46153846153847%\n",
      "Epoch 157/2000, Train Loss: 3.3159043945249964, Train Accuracy: 88.45901639344262%, Test Loss: 3.504988358570979, Test Accuracy: 69.53846153846153%\n",
      "Epoch 158/2000, Train Loss: 3.315795319979308, Train Accuracy: 88.45901639344262%, Test Loss: 3.5051971215468187, Test Accuracy: 69.46153846153847%\n",
      "Epoch 159/2000, Train Loss: 3.3157444977369464, Train Accuracy: 88.49180327868852%, Test Loss: 3.5048855543136597, Test Accuracy: 69.61538461538461%\n",
      "Epoch 160/2000, Train Loss: 3.3155460514006068, Train Accuracy: 88.49180327868852%, Test Loss: 3.505859274130601, Test Accuracy: 69.38461538461539%\n",
      "Epoch 161/2000, Train Loss: 3.3154684715583675, Train Accuracy: 88.49180327868852%, Test Loss: 3.5051557284135084, Test Accuracy: 69.61538461538461%\n",
      "Epoch 162/2000, Train Loss: 3.315376594418385, Train Accuracy: 88.49180327868852%, Test Loss: 3.5051061923687277, Test Accuracy: 69.61538461538461%\n",
      "Epoch 163/2000, Train Loss: 3.3153403157093484, Train Accuracy: 88.49180327868852%, Test Loss: 3.5049872490075917, Test Accuracy: 69.46153846153847%\n",
      "Epoch 164/2000, Train Loss: 3.315310845609571, Train Accuracy: 88.49180327868852%, Test Loss: 3.505039407656743, Test Accuracy: 69.46153846153847%\n",
      "Epoch 165/2000, Train Loss: 3.31527364840273, Train Accuracy: 88.49180327868852%, Test Loss: 3.5050393526370707, Test Accuracy: 69.46153846153847%\n",
      "Epoch 166/2000, Train Loss: 3.3152708733668095, Train Accuracy: 88.49180327868852%, Test Loss: 3.5049651586092434, Test Accuracy: 69.53846153846153%\n",
      "Epoch 167/2000, Train Loss: 3.315250209120453, Train Accuracy: 88.49180327868852%, Test Loss: 3.5051275491714478, Test Accuracy: 69.46153846153847%\n",
      "Epoch 168/2000, Train Loss: 3.3152656750600844, Train Accuracy: 88.49180327868852%, Test Loss: 3.505086201887864, Test Accuracy: 69.53846153846153%\n",
      "Epoch 169/2000, Train Loss: 3.315206484716447, Train Accuracy: 88.49180327868852%, Test Loss: 3.5048271050819984, Test Accuracy: 69.61538461538461%\n",
      "Epoch 170/2000, Train Loss: 3.3152012903182233, Train Accuracy: 88.49180327868852%, Test Loss: 3.5048713500683126, Test Accuracy: 69.61538461538461%\n",
      "Epoch 171/2000, Train Loss: 3.3151878216227546, Train Accuracy: 88.49180327868852%, Test Loss: 3.505007743835449, Test Accuracy: 69.53846153846153%\n",
      "Epoch 172/2000, Train Loss: 3.3151671143828847, Train Accuracy: 88.49180327868852%, Test Loss: 3.504902903850262, Test Accuracy: 69.61538461538461%\n",
      "Epoch 173/2000, Train Loss: 3.3151637335292627, Train Accuracy: 88.49180327868852%, Test Loss: 3.5048356973207913, Test Accuracy: 69.61538461538461%\n",
      "Epoch 174/2000, Train Loss: 3.3151478259289853, Train Accuracy: 88.49180327868852%, Test Loss: 3.5051526381419253, Test Accuracy: 69.53846153846153%\n",
      "Epoch 175/2000, Train Loss: 3.3151951305201797, Train Accuracy: 88.49180327868852%, Test Loss: 3.5050731897354126, Test Accuracy: 69.61538461538461%\n",
      "Epoch 176/2000, Train Loss: 3.315111605847468, Train Accuracy: 88.49180327868852%, Test Loss: 3.5048744220000048, Test Accuracy: 69.53846153846153%\n",
      "Epoch 177/2000, Train Loss: 3.315112618149304, Train Accuracy: 88.49180327868852%, Test Loss: 3.5049962263840895, Test Accuracy: 69.46153846153847%\n",
      "Epoch 178/2000, Train Loss: 3.3151098939239003, Train Accuracy: 88.49180327868852%, Test Loss: 3.504978207441477, Test Accuracy: 69.53846153846153%\n",
      "Epoch 179/2000, Train Loss: 3.3150929075772644, Train Accuracy: 88.49180327868852%, Test Loss: 3.5048350279147806, Test Accuracy: 69.6923076923077%\n",
      "Epoch 180/2000, Train Loss: 3.315086884576766, Train Accuracy: 88.49180327868852%, Test Loss: 3.5050513652654796, Test Accuracy: 69.61538461538461%\n",
      "Epoch 181/2000, Train Loss: 3.3151146271189704, Train Accuracy: 88.49180327868852%, Test Loss: 3.5050872839414158, Test Accuracy: 69.53846153846153%\n",
      "Epoch 182/2000, Train Loss: 3.31508554005232, Train Accuracy: 88.49180327868852%, Test Loss: 3.5047584038514357, Test Accuracy: 69.76923076923077%\n",
      "Epoch 183/2000, Train Loss: 3.3151404544955394, Train Accuracy: 88.49180327868852%, Test Loss: 3.5048063901754527, Test Accuracy: 69.76923076923077%\n",
      "Epoch 184/2000, Train Loss: 3.315087986774132, Train Accuracy: 88.49180327868852%, Test Loss: 3.505176067352295, Test Accuracy: 69.6923076923077%\n",
      "Epoch 185/2000, Train Loss: 3.315039005435881, Train Accuracy: 88.49180327868852%, Test Loss: 3.505074244279128, Test Accuracy: 69.61538461538461%\n",
      "Epoch 186/2000, Train Loss: 3.3150479011848324, Train Accuracy: 88.49180327868852%, Test Loss: 3.5050496688255897, Test Accuracy: 69.6923076923077%\n",
      "Epoch 187/2000, Train Loss: 3.3150176845613073, Train Accuracy: 88.49180327868852%, Test Loss: 3.505159304692195, Test Accuracy: 69.61538461538461%\n",
      "Epoch 188/2000, Train Loss: 3.315025005184236, Train Accuracy: 88.49180327868852%, Test Loss: 3.5053557065817027, Test Accuracy: 69.61538461538461%\n",
      "Epoch 189/2000, Train Loss: 3.315032181192617, Train Accuracy: 88.49180327868852%, Test Loss: 3.5053760363505435, Test Accuracy: 69.6923076923077%\n",
      "Epoch 190/2000, Train Loss: 3.314991880635746, Train Accuracy: 88.49180327868852%, Test Loss: 3.5051930867708645, Test Accuracy: 69.6923076923077%\n",
      "Epoch 191/2000, Train Loss: 3.3149852987195625, Train Accuracy: 88.49180327868852%, Test Loss: 3.5054259391931386, Test Accuracy: 69.61538461538461%\n",
      "Epoch 192/2000, Train Loss: 3.3149892658483786, Train Accuracy: 88.49180327868852%, Test Loss: 3.5054203546964207, Test Accuracy: 69.61538461538461%\n",
      "Epoch 193/2000, Train Loss: 3.314977075232834, Train Accuracy: 88.49180327868852%, Test Loss: 3.5052417516708374, Test Accuracy: 69.6923076923077%\n",
      "Epoch 194/2000, Train Loss: 3.3149941006644825, Train Accuracy: 88.49180327868852%, Test Loss: 3.505372615960928, Test Accuracy: 69.61538461538461%\n",
      "Epoch 195/2000, Train Loss: 3.31495500001751, Train Accuracy: 88.49180327868852%, Test Loss: 3.505387883919936, Test Accuracy: 69.53846153846153%\n",
      "Epoch 196/2000, Train Loss: 3.3149645953881937, Train Accuracy: 88.49180327868852%, Test Loss: 3.5055614709854126, Test Accuracy: 69.46153846153847%\n",
      "Epoch 197/2000, Train Loss: 3.314952486851176, Train Accuracy: 88.49180327868852%, Test Loss: 3.505304584136376, Test Accuracy: 69.6923076923077%\n",
      "Epoch 198/2000, Train Loss: 3.3149574271968154, Train Accuracy: 88.49180327868852%, Test Loss: 3.5055700632242055, Test Accuracy: 69.53846153846153%\n",
      "Epoch 199/2000, Train Loss: 3.3149782399662207, Train Accuracy: 88.49180327868852%, Test Loss: 3.5054107354237485, Test Accuracy: 69.53846153846153%\n",
      "Epoch 200/2000, Train Loss: 3.3149424185518357, Train Accuracy: 88.49180327868852%, Test Loss: 3.505432862501878, Test Accuracy: 69.53846153846153%\n",
      "Epoch 201/2000, Train Loss: 3.314930040328229, Train Accuracy: 88.49180327868852%, Test Loss: 3.5055869084138136, Test Accuracy: 69.53846153846153%\n",
      "Epoch 202/2000, Train Loss: 3.3149229034048613, Train Accuracy: 88.49180327868852%, Test Loss: 3.5054294421122623, Test Accuracy: 69.46153846153847%\n",
      "Epoch 203/2000, Train Loss: 3.3149472729104463, Train Accuracy: 88.49180327868852%, Test Loss: 3.5053596863379846, Test Accuracy: 69.53846153846153%\n",
      "Epoch 204/2000, Train Loss: 3.314975429753788, Train Accuracy: 88.49180327868852%, Test Loss: 3.505276478253878, Test Accuracy: 69.46153846153847%\n",
      "Epoch 205/2000, Train Loss: 3.3150327010232896, Train Accuracy: 88.49180327868852%, Test Loss: 3.505992733515226, Test Accuracy: 69.46153846153847%\n",
      "Epoch 206/2000, Train Loss: 3.315006893189227, Train Accuracy: 88.49180327868852%, Test Loss: 3.5054939801876364, Test Accuracy: 69.53846153846153%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/2000, Train Loss: 3.3149046663378106, Train Accuracy: 88.49180327868852%, Test Loss: 3.505713004332322, Test Accuracy: 69.53846153846153%\n",
      "Epoch 208/2000, Train Loss: 3.314906898092051, Train Accuracy: 88.49180327868852%, Test Loss: 3.5057035317787757, Test Accuracy: 69.38461538461539%\n",
      "Epoch 209/2000, Train Loss: 3.314904893030886, Train Accuracy: 88.49180327868852%, Test Loss: 3.505587495290316, Test Accuracy: 69.53846153846153%\n",
      "Epoch 210/2000, Train Loss: 3.314896079360462, Train Accuracy: 88.49180327868852%, Test Loss: 3.505601011789762, Test Accuracy: 69.53846153846153%\n",
      "Epoch 211/2000, Train Loss: 3.3148877151676865, Train Accuracy: 88.49180327868852%, Test Loss: 3.50566605421213, Test Accuracy: 69.38461538461539%\n",
      "Epoch 212/2000, Train Loss: 3.314893671723663, Train Accuracy: 88.49180327868852%, Test Loss: 3.505318760871887, Test Accuracy: 69.46153846153847%\n",
      "Epoch 213/2000, Train Loss: 3.3148959308374124, Train Accuracy: 88.49180327868852%, Test Loss: 3.505486194904034, Test Accuracy: 69.61538461538461%\n",
      "Epoch 214/2000, Train Loss: 3.314881148885508, Train Accuracy: 88.49180327868852%, Test Loss: 3.5054107079139123, Test Accuracy: 69.46153846153847%\n",
      "Epoch 215/2000, Train Loss: 3.3148794994979607, Train Accuracy: 88.49180327868852%, Test Loss: 3.505913184239314, Test Accuracy: 69.53846153846153%\n",
      "Epoch 216/2000, Train Loss: 3.314883650326338, Train Accuracy: 88.49180327868852%, Test Loss: 3.5056599011788, Test Accuracy: 69.46153846153847%\n",
      "Epoch 217/2000, Train Loss: 3.3148651787492094, Train Accuracy: 88.49180327868852%, Test Loss: 3.505704476283147, Test Accuracy: 69.3076923076923%\n",
      "Epoch 218/2000, Train Loss: 3.314861813529593, Train Accuracy: 88.49180327868852%, Test Loss: 3.5056622945345364, Test Accuracy: 69.53846153846153%\n",
      "Epoch 219/2000, Train Loss: 3.3148602110440613, Train Accuracy: 88.49180327868852%, Test Loss: 3.5058589440125685, Test Accuracy: 69.46153846153847%\n",
      "Epoch 220/2000, Train Loss: 3.3148528317936132, Train Accuracy: 88.49180327868852%, Test Loss: 3.505742201438317, Test Accuracy: 69.53846153846153%\n",
      "Epoch 221/2000, Train Loss: 3.314855950777648, Train Accuracy: 88.49180327868852%, Test Loss: 3.5057416695814867, Test Accuracy: 69.46153846153847%\n",
      "Epoch 222/2000, Train Loss: 3.3148610044698246, Train Accuracy: 88.49180327868852%, Test Loss: 3.505925957973187, Test Accuracy: 69.46153846153847%\n",
      "Epoch 223/2000, Train Loss: 3.3148462811454396, Train Accuracy: 88.49180327868852%, Test Loss: 3.5058158544393687, Test Accuracy: 69.38461538461539%\n",
      "Epoch 224/2000, Train Loss: 3.314850799372939, Train Accuracy: 88.49180327868852%, Test Loss: 3.5058860320311327, Test Accuracy: 69.38461538461539%\n",
      "Epoch 225/2000, Train Loss: 3.314843885234145, Train Accuracy: 88.49180327868852%, Test Loss: 3.5056114471875706, Test Accuracy: 69.53846153846153%\n",
      "Epoch 226/2000, Train Loss: 3.3148393591896435, Train Accuracy: 88.49180327868852%, Test Loss: 3.505456594320444, Test Accuracy: 69.61538461538461%\n",
      "Epoch 227/2000, Train Loss: 3.3148314053895045, Train Accuracy: 88.49180327868852%, Test Loss: 3.5055843133192797, Test Accuracy: 69.61538461538461%\n",
      "Epoch 228/2000, Train Loss: 3.3148393591896435, Train Accuracy: 88.49180327868852%, Test Loss: 3.505742604915912, Test Accuracy: 69.53846153846153%\n",
      "Epoch 229/2000, Train Loss: 3.314832222266275, Train Accuracy: 88.49180327868852%, Test Loss: 3.505552346889789, Test Accuracy: 69.46153846153847%\n",
      "Epoch 230/2000, Train Loss: 3.314835786819458, Train Accuracy: 88.49180327868852%, Test Loss: 3.5056949120301466, Test Accuracy: 69.53846153846153%\n",
      "Epoch 231/2000, Train Loss: 3.3148537307489114, Train Accuracy: 88.49180327868852%, Test Loss: 3.505952156507052, Test Accuracy: 69.53846153846153%\n",
      "Epoch 232/2000, Train Loss: 3.314833097770566, Train Accuracy: 88.49180327868852%, Test Loss: 3.5058868481562686, Test Accuracy: 69.46153846153847%\n",
      "Epoch 233/2000, Train Loss: 3.3148438656916386, Train Accuracy: 88.49180327868852%, Test Loss: 3.5068663633786716, Test Accuracy: 69.3076923076923%\n",
      "Epoch 234/2000, Train Loss: 3.315043261793793, Train Accuracy: 88.49180327868852%, Test Loss: 3.5053664996073794, Test Accuracy: 69.46153846153847%\n",
      "Epoch 235/2000, Train Loss: 3.3152072468741993, Train Accuracy: 88.42622950819673%, Test Loss: 3.506380237065829, Test Accuracy: 69.38461538461539%\n",
      "Epoch 236/2000, Train Loss: 3.3147946459348083, Train Accuracy: 88.49180327868852%, Test Loss: 3.5066240475727963, Test Accuracy: 69.46153846153847%\n",
      "Epoch 237/2000, Train Loss: 3.3146256071622253, Train Accuracy: 88.52459016393442%, Test Loss: 3.507158251909109, Test Accuracy: 69.38461538461539%\n",
      "Epoch 238/2000, Train Loss: 3.314740360760298, Train Accuracy: 88.49180327868852%, Test Loss: 3.504896503228408, Test Accuracy: 69.76923076923077%\n",
      "Epoch 239/2000, Train Loss: 3.3145689925209423, Train Accuracy: 88.52459016393442%, Test Loss: 3.5060682480151835, Test Accuracy: 69.76923076923077%\n",
      "Epoch 240/2000, Train Loss: 3.3144703497652146, Train Accuracy: 88.52459016393442%, Test Loss: 3.5059755398676944, Test Accuracy: 69.6923076923077%\n",
      "Epoch 241/2000, Train Loss: 3.3142908635686656, Train Accuracy: 88.59016393442623%, Test Loss: 3.5052704444298377, Test Accuracy: 69.6923076923077%\n",
      "Epoch 242/2000, Train Loss: 3.310737500425245, Train Accuracy: 89.11475409836065%, Test Loss: 3.501475324997535, Test Accuracy: 69.84615384615384%\n",
      "Saved the best model with validation loss:  3.501475324997535\n",
      "Epoch 243/2000, Train Loss: 3.307850923694548, Train Accuracy: 89.24590163934427%, Test Loss: 3.4995776414871216, Test Accuracy: 70.23076923076923%\n",
      "Saved the best model with validation loss:  3.4995776414871216\n",
      "Epoch 244/2000, Train Loss: 3.3058931163099947, Train Accuracy: 89.54098360655738%, Test Loss: 3.5014425974625807, Test Accuracy: 70.15384615384616%\n",
      "Epoch 245/2000, Train Loss: 3.305219419666978, Train Accuracy: 89.60655737704919%, Test Loss: 3.5006772646537194, Test Accuracy: 70.15384615384616%\n",
      "Epoch 246/2000, Train Loss: 3.3043888670499206, Train Accuracy: 89.63934426229508%, Test Loss: 3.5007835901700535, Test Accuracy: 69.92307692307692%\n",
      "Epoch 247/2000, Train Loss: 3.303909254855797, Train Accuracy: 89.70491803278688%, Test Loss: 3.5002722831872792, Test Accuracy: 70.23076923076923%\n",
      "Epoch 248/2000, Train Loss: 3.303539635705166, Train Accuracy: 89.70491803278688%, Test Loss: 3.5013140531686635, Test Accuracy: 69.92307692307692%\n",
      "Epoch 249/2000, Train Loss: 3.3034646980098037, Train Accuracy: 89.73770491803279%, Test Loss: 3.5006276735892663, Test Accuracy: 70.0%\n",
      "Epoch 250/2000, Train Loss: 3.303147288619495, Train Accuracy: 89.77049180327869%, Test Loss: 3.5007164294903097, Test Accuracy: 70.0%\n",
      "Epoch 251/2000, Train Loss: 3.3028536507340727, Train Accuracy: 89.80327868852459%, Test Loss: 3.5005247868024387, Test Accuracy: 70.07692307692308%\n",
      "Epoch 252/2000, Train Loss: 3.302725365904511, Train Accuracy: 89.80327868852459%, Test Loss: 3.5006836377657375, Test Accuracy: 70.07692307692308%\n",
      "Epoch 253/2000, Train Loss: 3.3026474811991706, Train Accuracy: 89.80327868852459%, Test Loss: 3.5003421948506284, Test Accuracy: 70.15384615384616%\n",
      "Epoch 254/2000, Train Loss: 3.3025164682357038, Train Accuracy: 89.80327868852459%, Test Loss: 3.5003421765107374, Test Accuracy: 70.07692307692308%\n",
      "Epoch 255/2000, Train Loss: 3.302423313015797, Train Accuracy: 89.80327868852459%, Test Loss: 3.5005628787554226, Test Accuracy: 70.07692307692308%\n",
      "Epoch 256/2000, Train Loss: 3.3023875932224462, Train Accuracy: 89.80327868852459%, Test Loss: 3.5006388425827026, Test Accuracy: 69.92307692307692%\n",
      "Epoch 257/2000, Train Loss: 3.3023071093637437, Train Accuracy: 89.80327868852459%, Test Loss: 3.500773319831261, Test Accuracy: 69.92307692307692%\n",
      "Epoch 258/2000, Train Loss: 3.3023019970440473, Train Accuracy: 89.80327868852459%, Test Loss: 3.5004838429964504, Test Accuracy: 70.15384615384616%\n",
      "Epoch 259/2000, Train Loss: 3.302243604034674, Train Accuracy: 89.80327868852459%, Test Loss: 3.5004381583287167, Test Accuracy: 70.0%\n",
      "Epoch 260/2000, Train Loss: 3.302239496199811, Train Accuracy: 89.80327868852459%, Test Loss: 3.5005964499253492, Test Accuracy: 70.07692307692308%\n",
      "Epoch 261/2000, Train Loss: 3.302188888925021, Train Accuracy: 89.80327868852459%, Test Loss: 3.5004379199101376, Test Accuracy: 70.07692307692308%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262/2000, Train Loss: 3.3021809937524016, Train Accuracy: 89.80327868852459%, Test Loss: 3.5002766114014845, Test Accuracy: 70.15384615384616%\n",
      "Epoch 263/2000, Train Loss: 3.3021556275789856, Train Accuracy: 89.80327868852459%, Test Loss: 3.5003681091161876, Test Accuracy: 70.0%\n",
      "Epoch 264/2000, Train Loss: 3.3021649688970847, Train Accuracy: 89.80327868852459%, Test Loss: 3.500233430128831, Test Accuracy: 70.15384615384616%\n",
      "Epoch 265/2000, Train Loss: 3.302114052850692, Train Accuracy: 89.80327868852459%, Test Loss: 3.5004158203418436, Test Accuracy: 69.92307692307692%\n",
      "Epoch 266/2000, Train Loss: 3.3020968710789913, Train Accuracy: 89.80327868852459%, Test Loss: 3.5002383360495934, Test Accuracy: 70.15384615384616%\n",
      "Epoch 267/2000, Train Loss: 3.30210042390667, Train Accuracy: 89.80327868852459%, Test Loss: 3.5003668069839478, Test Accuracy: 70.07692307692308%\n",
      "Epoch 268/2000, Train Loss: 3.302081319152332, Train Accuracy: 89.80327868852459%, Test Loss: 3.5003795256981483, Test Accuracy: 70.07692307692308%\n",
      "Epoch 269/2000, Train Loss: 3.3020574655689177, Train Accuracy: 89.80327868852459%, Test Loss: 3.5001807671326857, Test Accuracy: 70.0%\n",
      "Epoch 270/2000, Train Loss: 3.302051423025913, Train Accuracy: 89.80327868852459%, Test Loss: 3.500246442281283, Test Accuracy: 70.0%\n",
      "Epoch 271/2000, Train Loss: 3.3020445010701165, Train Accuracy: 89.80327868852459%, Test Loss: 3.500336316915659, Test Accuracy: 70.0%\n",
      "Epoch 272/2000, Train Loss: 3.3020575867324578, Train Accuracy: 89.80327868852459%, Test Loss: 3.5004047063680797, Test Accuracy: 70.0%\n",
      "Epoch 273/2000, Train Loss: 3.302016121442201, Train Accuracy: 89.80327868852459%, Test Loss: 3.5003550969637356, Test Accuracy: 70.0%\n",
      "Epoch 274/2000, Train Loss: 3.301999346154635, Train Accuracy: 89.80327868852459%, Test Loss: 3.5003858713003306, Test Accuracy: 70.07692307692308%\n",
      "Epoch 275/2000, Train Loss: 3.3016708796141576, Train Accuracy: 89.8360655737705%, Test Loss: 3.5006308463903575, Test Accuracy: 70.0%\n",
      "Epoch 276/2000, Train Loss: 3.3013725984291953, Train Accuracy: 89.8688524590164%, Test Loss: 3.5005873900193434, Test Accuracy: 69.84615384615384%\n",
      "Epoch 277/2000, Train Loss: 3.3013268689640234, Train Accuracy: 89.8688524590164%, Test Loss: 3.5003229471353383, Test Accuracy: 70.15384615384616%\n",
      "Epoch 278/2000, Train Loss: 3.3013176644434696, Train Accuracy: 89.8688524590164%, Test Loss: 3.5003231305342455, Test Accuracy: 70.0%\n",
      "Epoch 279/2000, Train Loss: 3.3013263217738418, Train Accuracy: 89.8688524590164%, Test Loss: 3.500272906743563, Test Accuracy: 70.07692307692308%\n",
      "Epoch 280/2000, Train Loss: 3.301305935031078, Train Accuracy: 89.8688524590164%, Test Loss: 3.5005484452614417, Test Accuracy: 69.92307692307692%\n",
      "Epoch 281/2000, Train Loss: 3.301297981230939, Train Accuracy: 89.8688524590164%, Test Loss: 3.500235860164349, Test Accuracy: 70.07692307692308%\n",
      "Epoch 282/2000, Train Loss: 3.301288166984183, Train Accuracy: 89.8688524590164%, Test Loss: 3.5003249370134792, Test Accuracy: 69.92307692307692%\n",
      "Epoch 283/2000, Train Loss: 3.3012765079248148, Train Accuracy: 89.8688524590164%, Test Loss: 3.5003129152151256, Test Accuracy: 70.07692307692308%\n",
      "Epoch 284/2000, Train Loss: 3.3012758864731087, Train Accuracy: 89.8688524590164%, Test Loss: 3.5003441388790426, Test Accuracy: 70.07692307692308%\n",
      "Epoch 285/2000, Train Loss: 3.3012687769092497, Train Accuracy: 89.8688524590164%, Test Loss: 3.5003222135397105, Test Accuracy: 70.07692307692308%\n",
      "Epoch 286/2000, Train Loss: 3.301260651135054, Train Accuracy: 89.8688524590164%, Test Loss: 3.5003609382189236, Test Accuracy: 70.07692307692308%\n",
      "Epoch 287/2000, Train Loss: 3.301255949207994, Train Accuracy: 89.8688524590164%, Test Loss: 3.500220197897691, Test Accuracy: 70.0%\n",
      "Epoch 288/2000, Train Loss: 3.2994279822365185, Train Accuracy: 90.09836065573771%, Test Loss: 3.496060142150292, Test Accuracy: 70.53846153846153%\n",
      "Saved the best model with validation loss:  3.496060142150292\n",
      "Epoch 289/2000, Train Loss: 3.2956781426414112, Train Accuracy: 90.45901639344262%, Test Loss: 3.495356321334839, Test Accuracy: 70.53846153846153%\n",
      "Epoch 290/2000, Train Loss: 3.294505744683938, Train Accuracy: 90.62295081967213%, Test Loss: 3.495454577299265, Test Accuracy: 70.46153846153847%\n",
      "Epoch 291/2000, Train Loss: 3.294154405593872, Train Accuracy: 90.62295081967213%, Test Loss: 3.4954312397883487, Test Accuracy: 70.46153846153847%\n",
      "Epoch 292/2000, Train Loss: 3.294033496106257, Train Accuracy: 90.62295081967213%, Test Loss: 3.495329123276931, Test Accuracy: 70.46153846153847%\n",
      "Epoch 293/2000, Train Loss: 3.29394383508651, Train Accuracy: 90.62295081967213%, Test Loss: 3.495488790365366, Test Accuracy: 70.38461538461539%\n",
      "Epoch 294/2000, Train Loss: 3.2938468104503196, Train Accuracy: 90.65573770491804%, Test Loss: 3.495485470845149, Test Accuracy: 70.46153846153847%\n",
      "Epoch 295/2000, Train Loss: 3.2936589600609953, Train Accuracy: 90.65573770491804%, Test Loss: 3.495356504733746, Test Accuracy: 70.46153846153847%\n",
      "Epoch 296/2000, Train Loss: 3.293603510153098, Train Accuracy: 90.65573770491804%, Test Loss: 3.4957633385291467, Test Accuracy: 70.23076923076923%\n",
      "Epoch 297/2000, Train Loss: 3.293558742179245, Train Accuracy: 90.65573770491804%, Test Loss: 3.495651465195876, Test Accuracy: 70.53846153846153%\n",
      "Epoch 298/2000, Train Loss: 3.293536229211776, Train Accuracy: 90.65573770491804%, Test Loss: 3.4954745127604556, Test Accuracy: 70.53846153846153%\n",
      "Epoch 299/2000, Train Loss: 3.293519461741213, Train Accuracy: 90.65573770491804%, Test Loss: 3.4954735315763035, Test Accuracy: 70.53846153846153%\n",
      "Epoch 300/2000, Train Loss: 3.293501509994757, Train Accuracy: 90.65573770491804%, Test Loss: 3.495424619087806, Test Accuracy: 70.46153846153847%\n",
      "Epoch 301/2000, Train Loss: 3.293490632635648, Train Accuracy: 90.65573770491804%, Test Loss: 3.495315826856173, Test Accuracy: 70.53846153846153%\n",
      "Epoch 302/2000, Train Loss: 3.293473802629064, Train Accuracy: 90.65573770491804%, Test Loss: 3.4953148456720204, Test Accuracy: 70.53846153846153%\n",
      "Epoch 303/2000, Train Loss: 3.2934606231626917, Train Accuracy: 90.65573770491804%, Test Loss: 3.4952920308479896, Test Accuracy: 70.38461538461539%\n",
      "Epoch 304/2000, Train Loss: 3.2935215606064094, Train Accuracy: 90.65573770491804%, Test Loss: 3.495513365818904, Test Accuracy: 70.38461538461539%\n",
      "Epoch 305/2000, Train Loss: 3.2933997560719974, Train Accuracy: 90.65573770491804%, Test Loss: 3.4950336676377516, Test Accuracy: 70.53846153846153%\n",
      "Saved the best model with validation loss:  3.4950336676377516\n",
      "Epoch 306/2000, Train Loss: 3.2931433935634424, Train Accuracy: 90.68852459016394%, Test Loss: 3.4950166573891273, Test Accuracy: 70.61538461538461%\n",
      "Epoch 307/2000, Train Loss: 3.2930753817323777, Train Accuracy: 90.68852459016394%, Test Loss: 3.49508169064155, Test Accuracy: 70.61538461538461%\n",
      "Epoch 308/2000, Train Loss: 3.2929430945974882, Train Accuracy: 90.72131147540983%, Test Loss: 3.4948535424012404, Test Accuracy: 70.53846153846153%\n",
      "Epoch 309/2000, Train Loss: 3.2927981556439008, Train Accuracy: 90.72131147540983%, Test Loss: 3.4954369985140286, Test Accuracy: 70.46153846153847%\n",
      "Epoch 310/2000, Train Loss: 3.2924996790338734, Train Accuracy: 90.78688524590164%, Test Loss: 3.4959482504771304, Test Accuracy: 70.53846153846153%\n",
      "Epoch 311/2000, Train Loss: 3.2922344403188735, Train Accuracy: 90.78688524590164%, Test Loss: 3.4955353553478536, Test Accuracy: 70.46153846153847%\n",
      "Epoch 312/2000, Train Loss: 3.2921727211748966, Train Accuracy: 90.78688524590164%, Test Loss: 3.4952725813939023, Test Accuracy: 70.46153846153847%\n",
      "Epoch 313/2000, Train Loss: 3.2921390533447266, Train Accuracy: 90.78688524590164%, Test Loss: 3.495512237915626, Test Accuracy: 70.3076923076923%\n",
      "Epoch 314/2000, Train Loss: 3.292125135171609, Train Accuracy: 90.78688524590164%, Test Loss: 3.495275249847999, Test Accuracy: 70.53846153846153%\n",
      "Epoch 315/2000, Train Loss: 3.292116497383743, Train Accuracy: 90.78688524590164%, Test Loss: 3.495413275865408, Test Accuracy: 70.46153846153847%\n",
      "Epoch 316/2000, Train Loss: 3.29211061118079, Train Accuracy: 90.78688524590164%, Test Loss: 3.49523756137261, Test Accuracy: 70.53846153846153%\n",
      "Epoch 317/2000, Train Loss: 3.2921048852263906, Train Accuracy: 90.78688524590164%, Test Loss: 3.495258624737079, Test Accuracy: 70.46153846153847%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 318/2000, Train Loss: 3.292099507128606, Train Accuracy: 90.78688524590164%, Test Loss: 3.4952220825048594, Test Accuracy: 70.61538461538461%\n",
      "Epoch 319/2000, Train Loss: 3.292095317215216, Train Accuracy: 90.78688524590164%, Test Loss: 3.4952412018409142, Test Accuracy: 70.53846153846153%\n",
      "Epoch 320/2000, Train Loss: 3.292086069701148, Train Accuracy: 90.78688524590164%, Test Loss: 3.495280751815209, Test Accuracy: 70.46153846153847%\n",
      "Epoch 321/2000, Train Loss: 3.292080011524138, Train Accuracy: 90.78688524590164%, Test Loss: 3.4952366076982937, Test Accuracy: 70.61538461538461%\n",
      "Epoch 322/2000, Train Loss: 3.292079104751837, Train Accuracy: 90.78688524590164%, Test Loss: 3.4953667108829203, Test Accuracy: 70.53846153846153%\n",
      "Epoch 323/2000, Train Loss: 3.2920732107318815, Train Accuracy: 90.78688524590164%, Test Loss: 3.495236690227802, Test Accuracy: 70.61538461538461%\n",
      "Epoch 324/2000, Train Loss: 3.2920706467550307, Train Accuracy: 90.78688524590164%, Test Loss: 3.4953141029064474, Test Accuracy: 70.53846153846153%\n",
      "Epoch 325/2000, Train Loss: 3.2920736563010293, Train Accuracy: 90.78688524590164%, Test Loss: 3.49527867940756, Test Accuracy: 70.46153846153847%\n",
      "Epoch 326/2000, Train Loss: 3.2920660151809944, Train Accuracy: 90.78688524590164%, Test Loss: 3.4952977987436147, Test Accuracy: 70.3076923076923%\n",
      "Epoch 327/2000, Train Loss: 3.292059058048686, Train Accuracy: 90.78688524590164%, Test Loss: 3.4953365417627187, Test Accuracy: 70.46153846153847%\n",
      "Epoch 328/2000, Train Loss: 3.2920563220977783, Train Accuracy: 90.78688524590164%, Test Loss: 3.4953094445742092, Test Accuracy: 70.38461538461539%\n",
      "Epoch 329/2000, Train Loss: 3.2920512097780823, Train Accuracy: 90.78688524590164%, Test Loss: 3.495230298775893, Test Accuracy: 70.38461538461539%\n",
      "Epoch 330/2000, Train Loss: 3.292050443711828, Train Accuracy: 90.78688524590164%, Test Loss: 3.4952764969605665, Test Accuracy: 70.53846153846153%\n",
      "Epoch 331/2000, Train Loss: 3.292045952843838, Train Accuracy: 90.78688524590164%, Test Loss: 3.495230142886822, Test Accuracy: 70.38461538461539%\n",
      "Epoch 332/2000, Train Loss: 3.2920442213777634, Train Accuracy: 90.78688524590164%, Test Loss: 3.495375844148489, Test Accuracy: 70.23076923076923%\n",
      "Epoch 333/2000, Train Loss: 3.292041258733781, Train Accuracy: 90.78688524590164%, Test Loss: 3.495189987696134, Test Accuracy: 70.46153846153847%\n",
      "Epoch 334/2000, Train Loss: 3.2920395702612204, Train Accuracy: 90.78688524590164%, Test Loss: 3.4954406848320594, Test Accuracy: 70.3076923076923%\n",
      "Epoch 335/2000, Train Loss: 3.292037150898918, Train Accuracy: 90.78688524590164%, Test Loss: 3.495212820860056, Test Accuracy: 70.46153846153847%\n",
      "Epoch 336/2000, Train Loss: 3.292034391497002, Train Accuracy: 90.78688524590164%, Test Loss: 3.495194801917443, Test Accuracy: 70.38461538461539%\n",
      "Epoch 337/2000, Train Loss: 3.29203043609369, Train Accuracy: 90.78688524590164%, Test Loss: 3.4953152674895067, Test Accuracy: 70.23076923076923%\n",
      "Epoch 338/2000, Train Loss: 3.292029224458288, Train Accuracy: 90.78688524590164%, Test Loss: 3.495225952221797, Test Accuracy: 70.46153846153847%\n",
      "Epoch 339/2000, Train Loss: 3.292027469541206, Train Accuracy: 90.78688524590164%, Test Loss: 3.4951954713234534, Test Accuracy: 70.53846153846153%\n",
      "Epoch 340/2000, Train Loss: 3.2920290251247217, Train Accuracy: 90.78688524590164%, Test Loss: 3.495470936481769, Test Accuracy: 70.23076923076923%\n",
      "Epoch 341/2000, Train Loss: 3.2920221539794423, Train Accuracy: 90.78688524590164%, Test Loss: 3.4952785968780518, Test Accuracy: 70.53846153846153%\n",
      "Epoch 342/2000, Train Loss: 3.292019914408199, Train Accuracy: 90.78688524590164%, Test Loss: 3.495298880797166, Test Accuracy: 70.38461538461539%\n",
      "Epoch 343/2000, Train Loss: 3.292018155582616, Train Accuracy: 90.78688524590164%, Test Loss: 3.495278395139254, Test Accuracy: 70.53846153846153%\n",
      "Epoch 344/2000, Train Loss: 3.292015736220313, Train Accuracy: 90.78688524590164%, Test Loss: 3.4954812985200148, Test Accuracy: 70.23076923076923%\n",
      "Epoch 345/2000, Train Loss: 3.292012339732686, Train Accuracy: 90.78688524590164%, Test Loss: 3.4953797230353723, Test Accuracy: 70.46153846153847%\n",
      "Epoch 346/2000, Train Loss: 3.292009072225602, Train Accuracy: 90.78688524590164%, Test Loss: 3.495346481983478, Test Accuracy: 70.3076923076923%\n",
      "Epoch 347/2000, Train Loss: 3.2920080794662727, Train Accuracy: 90.78688524590164%, Test Loss: 3.49539499099438, Test Accuracy: 70.3076923076923%\n",
      "Epoch 348/2000, Train Loss: 3.292006598144281, Train Accuracy: 90.78688524590164%, Test Loss: 3.495270408116854, Test Accuracy: 70.46153846153847%\n",
      "Epoch 349/2000, Train Loss: 3.292004526638594, Train Accuracy: 90.78688524590164%, Test Loss: 3.495285685245807, Test Accuracy: 70.46153846153847%\n",
      "Epoch 350/2000, Train Loss: 3.2920024394989014, Train Accuracy: 90.78688524590164%, Test Loss: 3.4954644716702976, Test Accuracy: 70.38461538461539%\n",
      "Epoch 351/2000, Train Loss: 3.2920006884903206, Train Accuracy: 90.78688524590164%, Test Loss: 3.4955719525997457, Test Accuracy: 70.3076923076923%\n",
      "Epoch 352/2000, Train Loss: 3.2920042569520045, Train Accuracy: 90.78688524590164%, Test Loss: 3.495369260127728, Test Accuracy: 70.38461538461539%\n",
      "Epoch 353/2000, Train Loss: 3.2919972802771897, Train Accuracy: 90.78688524590164%, Test Loss: 3.495531751559331, Test Accuracy: 70.38461538461539%\n",
      "Epoch 354/2000, Train Loss: 3.2919885838618046, Train Accuracy: 90.78688524590164%, Test Loss: 3.495556042744563, Test Accuracy: 70.38461538461539%\n",
      "Epoch 355/2000, Train Loss: 3.2920338169473116, Train Accuracy: 90.78688524590164%, Test Loss: 3.494939235540537, Test Accuracy: 70.46153846153847%\n",
      "Epoch 356/2000, Train Loss: 3.2917726313481563, Train Accuracy: 90.81967213114754%, Test Loss: 3.4957130688887377, Test Accuracy: 70.38461538461539%\n",
      "Epoch 357/2000, Train Loss: 3.2917614452174453, Train Accuracy: 90.81967213114754%, Test Loss: 3.495387939306406, Test Accuracy: 70.38461538461539%\n",
      "Epoch 358/2000, Train Loss: 3.291687293130843, Train Accuracy: 90.81967213114754%, Test Loss: 3.495151162147522, Test Accuracy: 70.3076923076923%\n",
      "Epoch 359/2000, Train Loss: 3.291682810079856, Train Accuracy: 90.81967213114754%, Test Loss: 3.495356018726642, Test Accuracy: 70.3076923076923%\n",
      "Epoch 360/2000, Train Loss: 3.291678796049024, Train Accuracy: 90.81967213114754%, Test Loss: 3.4951601211841288, Test Accuracy: 70.38461538461539%\n",
      "Epoch 361/2000, Train Loss: 3.2916779283617363, Train Accuracy: 90.81967213114754%, Test Loss: 3.495175554202153, Test Accuracy: 70.3076923076923%\n",
      "Epoch 362/2000, Train Loss: 3.2916726206169753, Train Accuracy: 90.81967213114754%, Test Loss: 3.495257487663856, Test Accuracy: 70.38461538461539%\n",
      "Epoch 363/2000, Train Loss: 3.2916702481566884, Train Accuracy: 90.81967213114754%, Test Loss: 3.495211665446942, Test Accuracy: 70.3076923076923%\n",
      "Epoch 364/2000, Train Loss: 3.291667719356349, Train Accuracy: 90.81967213114754%, Test Loss: 3.495196360808152, Test Accuracy: 70.38461538461539%\n",
      "Epoch 365/2000, Train Loss: 3.291665909720249, Train Accuracy: 90.81967213114754%, Test Loss: 3.4951026072868934, Test Accuracy: 70.53846153846153%\n",
      "Epoch 366/2000, Train Loss: 3.2916636779660084, Train Accuracy: 90.81967213114754%, Test Loss: 3.4951928028693566, Test Accuracy: 70.38461538461539%\n",
      "Epoch 367/2000, Train Loss: 3.2916617940683834, Train Accuracy: 90.81967213114754%, Test Loss: 3.495172830728384, Test Accuracy: 70.38461538461539%\n",
      "Epoch 368/2000, Train Loss: 3.291658854875408, Train Accuracy: 90.81967213114754%, Test Loss: 3.4950848451027503, Test Accuracy: 70.53846153846153%\n",
      "Epoch 369/2000, Train Loss: 3.2914848992081938, Train Accuracy: 90.85245901639344%, Test Loss: 3.4948715613438535, Test Accuracy: 70.46153846153847%\n",
      "Epoch 370/2000, Train Loss: 3.2881072661915764, Train Accuracy: 91.21311475409836%, Test Loss: 3.494244153683002, Test Accuracy: 70.53846153846153%\n",
      "Epoch 371/2000, Train Loss: 3.2867054079399733, Train Accuracy: 91.44262295081967%, Test Loss: 3.4927449959975023, Test Accuracy: 70.84615384615384%\n",
      "Saved the best model with validation loss:  3.4927449959975023\n",
      "Epoch 372/2000, Train Loss: 3.2864413495923652, Train Accuracy: 91.37704918032787%, Test Loss: 3.4930760126847487, Test Accuracy: 70.6923076923077%\n",
      "Epoch 373/2000, Train Loss: 3.2857192578862926, Train Accuracy: 91.44262295081967%, Test Loss: 3.4931138111994815, Test Accuracy: 70.53846153846153%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374/2000, Train Loss: 3.2856392313222416, Train Accuracy: 91.44262295081967%, Test Loss: 3.493286261191735, Test Accuracy: 70.6923076923077%\n",
      "Epoch 375/2000, Train Loss: 3.2855583175283964, Train Accuracy: 91.44262295081967%, Test Loss: 3.4937473168739905, Test Accuracy: 70.53846153846153%\n",
      "Epoch 376/2000, Train Loss: 3.2853054023179853, Train Accuracy: 91.47540983606558%, Test Loss: 3.493983653875498, Test Accuracy: 70.46153846153847%\n",
      "Epoch 377/2000, Train Loss: 3.2852412130011888, Train Accuracy: 91.47540983606558%, Test Loss: 3.4937866009198704, Test Accuracy: 70.61538461538461%\n",
      "Epoch 378/2000, Train Loss: 3.285190070261721, Train Accuracy: 91.47540983606558%, Test Loss: 3.4937451435969424, Test Accuracy: 70.6923076923077%\n",
      "Epoch 379/2000, Train Loss: 3.2850867529384424, Train Accuracy: 91.50819672131148%, Test Loss: 3.4936954241532545, Test Accuracy: 70.61538461538461%\n",
      "Epoch 380/2000, Train Loss: 3.2848082604955455, Train Accuracy: 91.54098360655738%, Test Loss: 3.4941089428388157, Test Accuracy: 70.61538461538461%\n",
      "Epoch 381/2000, Train Loss: 3.284652858483987, Train Accuracy: 91.54098360655738%, Test Loss: 3.493641046377329, Test Accuracy: 70.53846153846153%\n",
      "Epoch 382/2000, Train Loss: 3.284606683449667, Train Accuracy: 91.54098360655738%, Test Loss: 3.4937560833417454, Test Accuracy: 70.53846153846153%\n",
      "Epoch 383/2000, Train Loss: 3.284590900921431, Train Accuracy: 91.54098360655738%, Test Loss: 3.4936406153898973, Test Accuracy: 70.61538461538461%\n",
      "Epoch 384/2000, Train Loss: 3.2845780966711824, Train Accuracy: 91.54098360655738%, Test Loss: 3.493539415873014, Test Accuracy: 70.53846153846153%\n",
      "Epoch 385/2000, Train Loss: 3.2845697324784076, Train Accuracy: 91.54098360655738%, Test Loss: 3.4935817443407498, Test Accuracy: 70.53846153846153%\n",
      "Epoch 386/2000, Train Loss: 3.2845601683757346, Train Accuracy: 91.54098360655738%, Test Loss: 3.4934468269348145, Test Accuracy: 70.53846153846153%\n",
      "Epoch 387/2000, Train Loss: 3.2845554508146693, Train Accuracy: 91.54098360655738%, Test Loss: 3.4935534183795633, Test Accuracy: 70.53846153846153%\n",
      "Epoch 388/2000, Train Loss: 3.2845497600367812, Train Accuracy: 91.54098360655738%, Test Loss: 3.493396759033203, Test Accuracy: 70.53846153846153%\n",
      "Epoch 389/2000, Train Loss: 3.28454454218755, Train Accuracy: 91.54098360655738%, Test Loss: 3.493470476223872, Test Accuracy: 70.53846153846153%\n",
      "Epoch 390/2000, Train Loss: 3.2845400435025573, Train Accuracy: 91.54098360655738%, Test Loss: 3.493435859680176, Test Accuracy: 70.53846153846153%\n",
      "Epoch 391/2000, Train Loss: 3.284537416989686, Train Accuracy: 91.54098360655738%, Test Loss: 3.4932793378829956, Test Accuracy: 70.76923076923077%\n",
      "Epoch 392/2000, Train Loss: 3.2845329573897066, Train Accuracy: 91.54098360655738%, Test Loss: 3.493372816305894, Test Accuracy: 70.61538461538461%\n",
      "Epoch 393/2000, Train Loss: 3.2845294983660587, Train Accuracy: 91.54098360655738%, Test Loss: 3.4933824355785665, Test Accuracy: 70.53846153846153%\n",
      "Epoch 394/2000, Train Loss: 3.2845271141802677, Train Accuracy: 91.54098360655738%, Test Loss: 3.493348763539241, Test Accuracy: 70.6923076923077%\n",
      "Epoch 395/2000, Train Loss: 3.2845238662156904, Train Accuracy: 91.54098360655738%, Test Loss: 3.493339327665476, Test Accuracy: 70.53846153846153%\n",
      "Epoch 396/2000, Train Loss: 3.284521149807289, Train Accuracy: 91.54098360655738%, Test Loss: 3.493368515601525, Test Accuracy: 70.53846153846153%\n",
      "Epoch 397/2000, Train Loss: 3.2845185584709293, Train Accuracy: 91.54098360655738%, Test Loss: 3.493313881067129, Test Accuracy: 70.53846153846153%\n",
      "Epoch 398/2000, Train Loss: 3.2845162719976706, Train Accuracy: 91.54098360655738%, Test Loss: 3.493332578585698, Test Accuracy: 70.53846153846153%\n",
      "Epoch 399/2000, Train Loss: 3.2845144310935597, Train Accuracy: 91.54098360655738%, Test Loss: 3.493208114917462, Test Accuracy: 70.53846153846153%\n",
      "Epoch 400/2000, Train Loss: 3.2845127895230153, Train Accuracy: 91.54098360655738%, Test Loss: 3.4932296184393077, Test Accuracy: 70.53846153846153%\n",
      "Epoch 401/2000, Train Loss: 3.2845112808415147, Train Accuracy: 91.54098360655738%, Test Loss: 3.4932930836310754, Test Accuracy: 70.53846153846153%\n",
      "Epoch 402/2000, Train Loss: 3.2845085018970925, Train Accuracy: 91.54098360655738%, Test Loss: 3.493224721688491, Test Accuracy: 70.61538461538461%\n",
      "Epoch 403/2000, Train Loss: 3.2845072785361866, Train Accuracy: 91.54098360655738%, Test Loss: 3.4932503608556895, Test Accuracy: 70.53846153846153%\n",
      "Epoch 404/2000, Train Loss: 3.2845053750960553, Train Accuracy: 91.54098360655738%, Test Loss: 3.4932796496611376, Test Accuracy: 70.53846153846153%\n",
      "Epoch 405/2000, Train Loss: 3.284504132192643, Train Accuracy: 91.54098360655738%, Test Loss: 3.4932102240048923, Test Accuracy: 70.53846153846153%\n",
      "Epoch 406/2000, Train Loss: 3.2845029909102643, Train Accuracy: 91.54098360655738%, Test Loss: 3.4931646493765025, Test Accuracy: 70.61538461538461%\n",
      "Epoch 407/2000, Train Loss: 3.2845006770774967, Train Accuracy: 91.54098360655738%, Test Loss: 3.4930216440787683, Test Accuracy: 70.6923076923077%\n",
      "Epoch 408/2000, Train Loss: 3.2844992934680377, Train Accuracy: 91.54098360655738%, Test Loss: 3.493024917749258, Test Accuracy: 70.6923076923077%\n",
      "Epoch 409/2000, Train Loss: 3.284497585452971, Train Accuracy: 91.54098360655738%, Test Loss: 3.4930874567765455, Test Accuracy: 70.53846153846153%\n",
      "Epoch 410/2000, Train Loss: 3.2844964324450885, Train Accuracy: 91.54098360655738%, Test Loss: 3.4931791745699368, Test Accuracy: 70.53846153846153%\n",
      "Epoch 411/2000, Train Loss: 3.2844956585618315, Train Accuracy: 91.54098360655738%, Test Loss: 3.492971997994643, Test Accuracy: 70.53846153846153%\n",
      "Epoch 412/2000, Train Loss: 3.284494665802502, Train Accuracy: 91.54098360655738%, Test Loss: 3.493016875707186, Test Accuracy: 70.61538461538461%\n",
      "Epoch 413/2000, Train Loss: 3.284495404509247, Train Accuracy: 91.54098360655738%, Test Loss: 3.493022010876582, Test Accuracy: 70.61538461538461%\n",
      "Epoch 414/2000, Train Loss: 3.2844921878126803, Train Accuracy: 91.54098360655738%, Test Loss: 3.4930089437044582, Test Accuracy: 70.61538461538461%\n",
      "Epoch 415/2000, Train Loss: 3.28449049934012, Train Accuracy: 91.54098360655738%, Test Loss: 3.493021946686965, Test Accuracy: 70.61538461538461%\n",
      "Epoch 416/2000, Train Loss: 3.2844891626326764, Train Accuracy: 91.54098360655738%, Test Loss: 3.4929117147739115, Test Accuracy: 70.61538461538461%\n",
      "Epoch 417/2000, Train Loss: 3.2844883613899105, Train Accuracy: 91.54098360655738%, Test Loss: 3.492907093121455, Test Accuracy: 70.53846153846153%\n",
      "Epoch 418/2000, Train Loss: 3.284487865010246, Train Accuracy: 91.54098360655738%, Test Loss: 3.492914520777189, Test Accuracy: 70.61538461538461%\n",
      "Epoch 419/2000, Train Loss: 3.284486801897893, Train Accuracy: 91.54098360655738%, Test Loss: 3.4929057084597073, Test Accuracy: 70.61538461538461%\n",
      "Epoch 420/2000, Train Loss: 3.284485222863369, Train Accuracy: 91.54098360655738%, Test Loss: 3.492928614983192, Test Accuracy: 70.61538461538461%\n",
      "Epoch 421/2000, Train Loss: 3.2844848124707333, Train Accuracy: 91.54098360655738%, Test Loss: 3.492849817642799, Test Accuracy: 70.53846153846153%\n",
      "Epoch 422/2000, Train Loss: 3.2844839486919466, Train Accuracy: 91.54098360655738%, Test Loss: 3.4927315895374003, Test Accuracy: 70.6923076923077%\n",
      "Epoch 423/2000, Train Loss: 3.284481490244631, Train Accuracy: 91.54098360655738%, Test Loss: 3.4930308782137356, Test Accuracy: 70.53846153846153%\n",
      "Epoch 424/2000, Train Loss: 3.284480767171891, Train Accuracy: 91.54098360655738%, Test Loss: 3.4929899985973654, Test Accuracy: 70.53846153846153%\n",
      "Epoch 425/2000, Train Loss: 3.2844784259796143, Train Accuracy: 91.54098360655738%, Test Loss: 3.4929103576219998, Test Accuracy: 70.61538461538461%\n",
      "Epoch 426/2000, Train Loss: 3.284469096387019, Train Accuracy: 91.54098360655738%, Test Loss: 3.492982561771686, Test Accuracy: 70.76923076923077%\n",
      "Epoch 427/2000, Train Loss: 3.284347307486612, Train Accuracy: 91.57377049180327%, Test Loss: 3.4935833307412953, Test Accuracy: 70.76923076923077%\n",
      "Epoch 428/2000, Train Loss: 3.2842117997466542, Train Accuracy: 91.57377049180327%, Test Loss: 3.4921212196350098, Test Accuracy: 70.84615384615384%\n",
      "Epoch 429/2000, Train Loss: 3.2841608837002614, Train Accuracy: 91.57377049180327%, Test Loss: 3.492620743238009, Test Accuracy: 70.76923076923077%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430/2000, Train Loss: 3.2841538405809247, Train Accuracy: 91.57377049180327%, Test Loss: 3.492624356196477, Test Accuracy: 70.6923076923077%\n",
      "Epoch 431/2000, Train Loss: 3.2841501665897055, Train Accuracy: 91.57377049180327%, Test Loss: 3.4926592111587524, Test Accuracy: 70.76923076923077%\n",
      "Epoch 432/2000, Train Loss: 3.28414740718779, Train Accuracy: 91.57377049180327%, Test Loss: 3.49264474098499, Test Accuracy: 70.6923076923077%\n",
      "Epoch 433/2000, Train Loss: 3.2841459610423103, Train Accuracy: 91.57377049180327%, Test Loss: 3.4927332492975087, Test Accuracy: 70.61538461538461%\n",
      "Epoch 434/2000, Train Loss: 3.2841436042160286, Train Accuracy: 91.57377049180327%, Test Loss: 3.4925879698533278, Test Accuracy: 70.6923076923077%\n",
      "Epoch 435/2000, Train Loss: 3.2841424043061305, Train Accuracy: 91.57377049180327%, Test Loss: 3.49276825097891, Test Accuracy: 70.61538461538461%\n",
      "Epoch 436/2000, Train Loss: 3.2841405751275237, Train Accuracy: 91.57377049180327%, Test Loss: 3.4925380761806784, Test Accuracy: 70.6923076923077%\n",
      "Epoch 437/2000, Train Loss: 3.284139410394137, Train Accuracy: 91.57377049180327%, Test Loss: 3.4927493058718166, Test Accuracy: 70.61538461538461%\n",
      "Epoch 438/2000, Train Loss: 3.2841388514784517, Train Accuracy: 91.57377049180327%, Test Loss: 3.4927750642483053, Test Accuracy: 70.61538461538461%\n",
      "Epoch 439/2000, Train Loss: 3.284137686745065, Train Accuracy: 91.57377049180327%, Test Loss: 3.4926419533216038, Test Accuracy: 70.61538461538461%\n",
      "Epoch 440/2000, Train Loss: 3.284137092652868, Train Accuracy: 91.57377049180327%, Test Loss: 3.4925754987276516, Test Accuracy: 70.76923076923077%\n",
      "Epoch 441/2000, Train Loss: 3.2841354784418324, Train Accuracy: 91.57377049180327%, Test Loss: 3.4927610709117007, Test Accuracy: 70.6923076923077%\n",
      "Epoch 442/2000, Train Loss: 3.2841348335391185, Train Accuracy: 91.57377049180327%, Test Loss: 3.4926608342390795, Test Accuracy: 70.6923076923077%\n",
      "Epoch 443/2000, Train Loss: 3.2841334890146725, Train Accuracy: 91.57377049180327%, Test Loss: 3.4926933325254, Test Accuracy: 70.6923076923077%\n",
      "Epoch 444/2000, Train Loss: 3.284132824569452, Train Accuracy: 91.57377049180327%, Test Loss: 3.4925776628347545, Test Accuracy: 70.76923076923077%\n",
      "Epoch 445/2000, Train Loss: 3.284131995967177, Train Accuracy: 91.57377049180327%, Test Loss: 3.492741117110619, Test Accuracy: 70.61538461538461%\n",
      "Epoch 446/2000, Train Loss: 3.2841306827107415, Train Accuracy: 91.57377049180327%, Test Loss: 3.492705198434683, Test Accuracy: 70.6923076923077%\n",
      "Epoch 447/2000, Train Loss: 3.284129990906012, Train Accuracy: 91.57377049180327%, Test Loss: 3.4925909409156213, Test Accuracy: 70.76923076923077%\n",
      "Epoch 448/2000, Train Loss: 3.284128712826088, Train Accuracy: 91.57377049180327%, Test Loss: 3.492688747552725, Test Accuracy: 70.84615384615384%\n",
      "Epoch 449/2000, Train Loss: 3.2841284001459843, Train Accuracy: 91.57377049180327%, Test Loss: 3.4926279141352725, Test Accuracy: 70.84615384615384%\n",
      "Epoch 450/2000, Train Loss: 3.2841282203549245, Train Accuracy: 91.57377049180327%, Test Loss: 3.492526127741887, Test Accuracy: 70.76923076923077%\n",
      "Epoch 451/2000, Train Loss: 3.2841276575307377, Train Accuracy: 91.57377049180327%, Test Loss: 3.4928723940482507, Test Accuracy: 70.61538461538461%\n",
      "Epoch 452/2000, Train Loss: 3.284127028262029, Train Accuracy: 91.57377049180327%, Test Loss: 3.4926800544445333, Test Accuracy: 70.76923076923077%\n",
      "Epoch 453/2000, Train Loss: 3.2841267820264473, Train Accuracy: 91.57377049180327%, Test Loss: 3.492812679364131, Test Accuracy: 70.61538461538461%\n",
      "Epoch 454/2000, Train Loss: 3.284126387267816, Train Accuracy: 91.57377049180327%, Test Loss: 3.4926420541910024, Test Accuracy: 70.76923076923077%\n",
      "Epoch 455/2000, Train Loss: 3.2841262192022604, Train Accuracy: 91.57377049180327%, Test Loss: 3.4928275438455434, Test Accuracy: 70.6923076923077%\n",
      "Epoch 456/2000, Train Loss: 3.284126770300943, Train Accuracy: 91.57377049180327%, Test Loss: 3.4926625948685865, Test Accuracy: 70.6923076923077%\n",
      "Epoch 457/2000, Train Loss: 3.2841251287303987, Train Accuracy: 91.57377049180327%, Test Loss: 3.4928666536624613, Test Accuracy: 70.61538461538461%\n",
      "Epoch 458/2000, Train Loss: 3.284125406233991, Train Accuracy: 91.57377049180327%, Test Loss: 3.4927581823789158, Test Accuracy: 70.6923076923077%\n",
      "Epoch 459/2000, Train Loss: 3.2841238076569605, Train Accuracy: 91.57377049180327%, Test Loss: 3.492928459094121, Test Accuracy: 70.6923076923077%\n",
      "Epoch 460/2000, Train Loss: 3.2841244017491573, Train Accuracy: 91.57377049180327%, Test Loss: 3.492707243332496, Test Accuracy: 70.61538461538461%\n",
      "Epoch 461/2000, Train Loss: 3.284124444742672, Train Accuracy: 91.57377049180327%, Test Loss: 3.492895218042227, Test Accuracy: 70.61538461538461%\n",
      "Epoch 462/2000, Train Loss: 3.2841237920229553, Train Accuracy: 91.57377049180327%, Test Loss: 3.492950283564054, Test Accuracy: 70.6923076923077%\n",
      "Epoch 463/2000, Train Loss: 3.2841221739034183, Train Accuracy: 91.57377049180327%, Test Loss: 3.4927915793198805, Test Accuracy: 70.6923076923077%\n",
      "Epoch 464/2000, Train Loss: 3.284122314609465, Train Accuracy: 91.57377049180327%, Test Loss: 3.4927297280384946, Test Accuracy: 70.6923076923077%\n",
      "Epoch 465/2000, Train Loss: 3.284124042167038, Train Accuracy: 91.57377049180327%, Test Loss: 3.492749837728647, Test Accuracy: 70.6923076923077%\n",
      "Epoch 466/2000, Train Loss: 3.284124542455204, Train Accuracy: 91.57377049180327%, Test Loss: 3.4929613791979275, Test Accuracy: 70.61538461538461%\n",
      "Epoch 467/2000, Train Loss: 3.2841230533162102, Train Accuracy: 91.57377049180327%, Test Loss: 3.4928739070892334, Test Accuracy: 70.61538461538461%\n",
      "Epoch 468/2000, Train Loss: 3.284121157693081, Train Accuracy: 91.57377049180327%, Test Loss: 3.4930582229907694, Test Accuracy: 70.61538461538461%\n",
      "Epoch 469/2000, Train Loss: 3.2840631828933464, Train Accuracy: 91.57377049180327%, Test Loss: 3.493261318940383, Test Accuracy: 70.6923076923077%\n",
      "Epoch 470/2000, Train Loss: 3.285426984067823, Train Accuracy: 91.44262295081967%, Test Loss: 3.493215900201064, Test Accuracy: 70.6923076923077%\n",
      "Epoch 471/2000, Train Loss: 3.2849198169395573, Train Accuracy: 91.50819672131148%, Test Loss: 3.4932186145048876, Test Accuracy: 70.61538461538461%\n",
      "Epoch 472/2000, Train Loss: 3.284166742543705, Train Accuracy: 91.57377049180327%, Test Loss: 3.492964689548199, Test Accuracy: 70.61538461538461%\n",
      "Epoch 473/2000, Train Loss: 3.284130233233092, Train Accuracy: 91.57377049180327%, Test Loss: 3.492848359621488, Test Accuracy: 70.6923076923077%\n",
      "Epoch 474/2000, Train Loss: 3.284118902487833, Train Accuracy: 91.57377049180327%, Test Loss: 3.492752506182744, Test Accuracy: 70.76923076923077%\n",
      "Epoch 475/2000, Train Loss: 3.284101697265125, Train Accuracy: 91.57377049180327%, Test Loss: 3.4927297738882213, Test Accuracy: 70.84615384615384%\n",
      "Epoch 476/2000, Train Loss: 3.284023671853738, Train Accuracy: 91.57377049180327%, Test Loss: 3.4931164796535787, Test Accuracy: 70.53846153846153%\n",
      "Epoch 477/2000, Train Loss: 3.2841854329969062, Train Accuracy: 91.57377049180327%, Test Loss: 3.49322955424969, Test Accuracy: 70.6923076923077%\n",
      "Epoch 478/2000, Train Loss: 3.2845092914143548, Train Accuracy: 91.54098360655738%, Test Loss: 3.493174800506005, Test Accuracy: 70.6923076923077%\n",
      "Epoch 479/2000, Train Loss: 3.2854013169398075, Train Accuracy: 91.47540983606558%, Test Loss: 3.492633397762592, Test Accuracy: 70.76923076923077%\n",
      "Epoch 480/2000, Train Loss: 3.2841497405630644, Train Accuracy: 91.57377049180327%, Test Loss: 3.4927660135122447, Test Accuracy: 70.6923076923077%\n",
      "Epoch 481/2000, Train Loss: 3.2840340880096934, Train Accuracy: 91.60655737704919%, Test Loss: 3.4923394918441772, Test Accuracy: 70.6923076923077%\n",
      "Epoch 482/2000, Train Loss: 3.2839519860314543, Train Accuracy: 91.60655737704919%, Test Loss: 3.4926665196051965, Test Accuracy: 70.6923076923077%\n",
      "Epoch 483/2000, Train Loss: 3.28428150395878, Train Accuracy: 91.57377049180327%, Test Loss: 3.49273589024177, Test Accuracy: 70.6923076923077%\n",
      "Epoch 484/2000, Train Loss: 3.2840793914482242, Train Accuracy: 91.57377049180327%, Test Loss: 3.4927348723778358, Test Accuracy: 70.61538461538461%\n",
      "Epoch 485/2000, Train Loss: 3.284115083882066, Train Accuracy: 91.60655737704919%, Test Loss: 3.492765105687655, Test Accuracy: 70.6923076923077%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486/2000, Train Loss: 3.28486099008654, Train Accuracy: 91.50819672131148%, Test Loss: 3.4928653056804952, Test Accuracy: 70.61538461538461%\n",
      "Epoch 487/2000, Train Loss: 3.284157729539715, Train Accuracy: 91.57377049180327%, Test Loss: 3.492639257357671, Test Accuracy: 70.6923076923077%\n",
      "Epoch 488/2000, Train Loss: 3.283992579725922, Train Accuracy: 91.60655737704919%, Test Loss: 3.492649115048922, Test Accuracy: 70.76923076923077%\n",
      "Epoch 489/2000, Train Loss: 3.284044554976166, Train Accuracy: 91.60655737704919%, Test Loss: 3.4924906400533824, Test Accuracy: 70.6923076923077%\n",
      "Epoch 490/2000, Train Loss: 3.2842479221156387, Train Accuracy: 91.57377049180327%, Test Loss: 3.4926609901281505, Test Accuracy: 70.61538461538461%\n",
      "Epoch 491/2000, Train Loss: 3.284016628734401, Train Accuracy: 91.60655737704919%, Test Loss: 3.492631196975708, Test Accuracy: 70.61538461538461%\n",
      "Epoch 492/2000, Train Loss: 3.2838718187613565, Train Accuracy: 91.60655737704919%, Test Loss: 3.492596745491028, Test Accuracy: 70.6923076923077%\n",
      "Epoch 493/2000, Train Loss: 3.2838305957981797, Train Accuracy: 91.60655737704919%, Test Loss: 3.4924957843927236, Test Accuracy: 70.6923076923077%\n",
      "Epoch 494/2000, Train Loss: 3.28391999494834, Train Accuracy: 91.60655737704919%, Test Loss: 3.492617680476262, Test Accuracy: 70.6923076923077%\n",
      "Epoch 495/2000, Train Loss: 3.283825436576468, Train Accuracy: 91.60655737704919%, Test Loss: 3.492654717885531, Test Accuracy: 70.6923076923077%\n",
      "Epoch 496/2000, Train Loss: 3.2838217664937503, Train Accuracy: 91.60655737704919%, Test Loss: 3.4926362129358144, Test Accuracy: 70.76923076923077%\n",
      "Epoch 497/2000, Train Loss: 3.2838098846498083, Train Accuracy: 91.63934426229508%, Test Loss: 3.4928928155165453, Test Accuracy: 70.6923076923077%\n",
      "Epoch 498/2000, Train Loss: 3.2840675056957807, Train Accuracy: 91.57377049180327%, Test Loss: 3.4927146434783936, Test Accuracy: 70.61538461538461%\n",
      "Epoch 499/2000, Train Loss: 3.284639241265469, Train Accuracy: 91.50819672131148%, Test Loss: 3.492936134338379, Test Accuracy: 70.76923076923077%\n",
      "Epoch 500/2000, Train Loss: 3.2838797217509788, Train Accuracy: 91.60655737704919%, Test Loss: 3.4925983777413, Test Accuracy: 70.6923076923077%\n",
      "Epoch 501/2000, Train Loss: 3.283912443723835, Train Accuracy: 91.60655737704919%, Test Loss: 3.4927851328482995, Test Accuracy: 70.6923076923077%\n",
      "Epoch 502/2000, Train Loss: 3.2838027203669315, Train Accuracy: 91.60655737704919%, Test Loss: 3.492760025537931, Test Accuracy: 70.6923076923077%\n",
      "Epoch 503/2000, Train Loss: 3.283830623157689, Train Accuracy: 91.60655737704919%, Test Loss: 3.4926588718707743, Test Accuracy: 70.6923076923077%\n",
      "Epoch 504/2000, Train Loss: 3.285425850602447, Train Accuracy: 91.44262295081967%, Test Loss: 3.4928134312996497, Test Accuracy: 70.76923076923077%\n",
      "Epoch 505/2000, Train Loss: 3.2842169042493476, Train Accuracy: 91.57377049180327%, Test Loss: 3.492798539308401, Test Accuracy: 70.6923076923077%\n",
      "Epoch 506/2000, Train Loss: 3.283960440119759, Train Accuracy: 91.57377049180327%, Test Loss: 3.49275527550624, Test Accuracy: 70.6923076923077%\n",
      "Epoch 507/2000, Train Loss: 3.2838359543534574, Train Accuracy: 91.60655737704919%, Test Loss: 3.492932860667889, Test Accuracy: 70.61538461538461%\n",
      "Epoch 508/2000, Train Loss: 3.283831307145416, Train Accuracy: 91.60655737704919%, Test Loss: 3.492777265035189, Test Accuracy: 70.6923076923077%\n",
      "Epoch 509/2000, Train Loss: 3.2838367829557327, Train Accuracy: 91.60655737704919%, Test Loss: 3.4926112981942983, Test Accuracy: 70.84615384615384%\n",
      "Epoch 510/2000, Train Loss: 3.2838004221681687, Train Accuracy: 91.60655737704919%, Test Loss: 3.4927378251002383, Test Accuracy: 70.6923076923077%\n",
      "Epoch 511/2000, Train Loss: 3.2837931640812608, Train Accuracy: 91.60655737704919%, Test Loss: 3.4926954232729397, Test Accuracy: 70.6923076923077%\n",
      "Epoch 512/2000, Train Loss: 3.283795630345579, Train Accuracy: 91.60655737704919%, Test Loss: 3.492590693327097, Test Accuracy: 70.76923076923077%\n",
      "Epoch 513/2000, Train Loss: 3.2838114285078204, Train Accuracy: 91.60655737704919%, Test Loss: 3.4928341186963596, Test Accuracy: 70.6923076923077%\n",
      "Epoch 514/2000, Train Loss: 3.283564927148037, Train Accuracy: 91.63934426229508%, Test Loss: 3.492606447293208, Test Accuracy: 70.76923076923077%\n",
      "Epoch 515/2000, Train Loss: 3.285339683782859, Train Accuracy: 91.44262295081967%, Test Loss: 3.492932814818162, Test Accuracy: 70.76923076923077%\n",
      "Epoch 516/2000, Train Loss: 3.2841143256328147, Train Accuracy: 91.60655737704919%, Test Loss: 3.4927281049581675, Test Accuracy: 70.76923076923077%\n",
      "Epoch 517/2000, Train Loss: 3.2839151405897296, Train Accuracy: 91.60655737704919%, Test Loss: 3.4926868677139282, Test Accuracy: 70.92307692307692%\n",
      "Epoch 518/2000, Train Loss: 3.2838451627825127, Train Accuracy: 91.60655737704919%, Test Loss: 3.492575553747324, Test Accuracy: 70.76923076923077%\n",
      "Epoch 519/2000, Train Loss: 3.2837602036898255, Train Accuracy: 91.60655737704919%, Test Loss: 3.492792166196383, Test Accuracy: 70.6923076923077%\n",
      "Epoch 520/2000, Train Loss: 3.283860710800671, Train Accuracy: 91.63934426229508%, Test Loss: 3.492500506914579, Test Accuracy: 70.76923076923077%\n",
      "Epoch 521/2000, Train Loss: 3.282180860394337, Train Accuracy: 91.8360655737705%, Test Loss: 3.485210528740516, Test Accuracy: 71.6923076923077%\n",
      "Saved the best model with validation loss:  3.485210528740516\n",
      "Epoch 522/2000, Train Loss: 3.279112952654479, Train Accuracy: 92.19672131147541%, Test Loss: 3.4845613607993493, Test Accuracy: 71.6923076923077%\n",
      "Epoch 523/2000, Train Loss: 3.2773927274297496, Train Accuracy: 92.36065573770492%, Test Loss: 3.48294944029588, Test Accuracy: 71.76923076923077%\n",
      "Saved the best model with validation loss:  3.48294944029588\n",
      "Epoch 524/2000, Train Loss: 3.276763517348493, Train Accuracy: 92.39344262295081%, Test Loss: 3.482357767912058, Test Accuracy: 71.76923076923077%\n",
      "Epoch 525/2000, Train Loss: 3.2764693283643878, Train Accuracy: 92.39344262295081%, Test Loss: 3.4820392773701596, Test Accuracy: 71.92307692307692%\n",
      "Epoch 526/2000, Train Loss: 3.2761432186501924, Train Accuracy: 92.42622950819673%, Test Loss: 3.4815068520032444, Test Accuracy: 71.92307692307692%\n",
      "Saved the best model with validation loss:  3.4815068520032444\n",
      "Epoch 527/2000, Train Loss: 3.2759591008796067, Train Accuracy: 92.42622950819673%, Test Loss: 3.481575791652386, Test Accuracy: 71.92307692307692%\n",
      "Epoch 528/2000, Train Loss: 3.2758339741190925, Train Accuracy: 92.42622950819673%, Test Loss: 3.4816153599665713, Test Accuracy: 71.76923076923077%\n",
      "Epoch 529/2000, Train Loss: 3.2757930990125312, Train Accuracy: 92.45901639344262%, Test Loss: 3.4816145163315992, Test Accuracy: 71.84615384615384%\n",
      "Epoch 530/2000, Train Loss: 3.275509126850816, Train Accuracy: 92.45901639344262%, Test Loss: 3.4813248744377723, Test Accuracy: 71.84615384615384%\n",
      "Epoch 531/2000, Train Loss: 3.275758125742928, Train Accuracy: 92.45901639344262%, Test Loss: 3.481389586742108, Test Accuracy: 71.84615384615384%\n",
      "Epoch 532/2000, Train Loss: 3.275481110713521, Train Accuracy: 92.49180327868852%, Test Loss: 3.4813543191322913, Test Accuracy: 71.84615384615384%\n",
      "Epoch 533/2000, Train Loss: 3.2752281759606032, Train Accuracy: 92.49180327868852%, Test Loss: 3.4812808403602014, Test Accuracy: 71.84615384615384%\n",
      "Epoch 534/2000, Train Loss: 3.27516203630166, Train Accuracy: 92.49180327868852%, Test Loss: 3.481330394744873, Test Accuracy: 71.84615384615384%\n",
      "Epoch 535/2000, Train Loss: 3.275480270385742, Train Accuracy: 92.45901639344262%, Test Loss: 3.481581137730525, Test Accuracy: 71.84615384615384%\n",
      "Epoch 536/2000, Train Loss: 3.2752156296714405, Train Accuracy: 92.49180327868852%, Test Loss: 3.4812685434634867, Test Accuracy: 71.92307692307692%\n",
      "Epoch 537/2000, Train Loss: 3.275152433113974, Train Accuracy: 92.49180327868852%, Test Loss: 3.481528722322904, Test Accuracy: 71.84615384615384%\n",
      "Epoch 538/2000, Train Loss: 3.275112343616173, Train Accuracy: 92.49180327868852%, Test Loss: 3.481288506434514, Test Accuracy: 71.92307692307692%\n",
      "Epoch 539/2000, Train Loss: 3.275180691578349, Train Accuracy: 92.52459016393442%, Test Loss: 3.481486815672654, Test Accuracy: 71.84615384615384%\n",
      "Epoch 540/2000, Train Loss: 3.275101532701586, Train Accuracy: 92.49180327868852%, Test Loss: 3.48150764978849, Test Accuracy: 71.84615384615384%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/2000, Train Loss: 3.275077600948146, Train Accuracy: 92.49180327868852%, Test Loss: 3.481445083251366, Test Accuracy: 71.84615384615384%\n",
      "Epoch 542/2000, Train Loss: 3.2750541186723554, Train Accuracy: 92.49180327868852%, Test Loss: 3.481505274772644, Test Accuracy: 71.84615384615384%\n",
      "Epoch 543/2000, Train Loss: 3.275007908461524, Train Accuracy: 92.49180327868852%, Test Loss: 3.4815530501879177, Test Accuracy: 71.84615384615384%\n",
      "Epoch 544/2000, Train Loss: 3.275255848149784, Train Accuracy: 92.49180327868852%, Test Loss: 3.4815462277485776, Test Accuracy: 71.84615384615384%\n",
      "Epoch 545/2000, Train Loss: 3.275056233171557, Train Accuracy: 92.49180327868852%, Test Loss: 3.481555993740375, Test Accuracy: 71.84615384615384%\n",
      "Epoch 546/2000, Train Loss: 3.2753201234536093, Train Accuracy: 92.45901639344262%, Test Loss: 3.4815587172141442, Test Accuracy: 71.84615384615384%\n",
      "Epoch 547/2000, Train Loss: 3.2754458325808167, Train Accuracy: 92.45901639344262%, Test Loss: 3.481309881577125, Test Accuracy: 71.92307692307692%\n",
      "Epoch 548/2000, Train Loss: 3.2752763404220833, Train Accuracy: 92.45901639344262%, Test Loss: 3.481474372056814, Test Accuracy: 71.84615384615384%\n",
      "Epoch 549/2000, Train Loss: 3.27506612949684, Train Accuracy: 92.49180327868852%, Test Loss: 3.4815011482972364, Test Accuracy: 71.84615384615384%\n",
      "Epoch 550/2000, Train Loss: 3.275038136810553, Train Accuracy: 92.49180327868852%, Test Loss: 3.4814384900606594, Test Accuracy: 71.84615384615384%\n",
      "Epoch 551/2000, Train Loss: 3.2758951929749034, Train Accuracy: 92.36065573770492%, Test Loss: 3.4817914962768555, Test Accuracy: 71.84615384615384%\n",
      "Epoch 552/2000, Train Loss: 3.2754114299524026, Train Accuracy: 92.45901639344262%, Test Loss: 3.4814852567819448, Test Accuracy: 72.0%\n",
      "Epoch 553/2000, Train Loss: 3.2753562849076068, Train Accuracy: 92.45901639344262%, Test Loss: 3.4815660531704244, Test Accuracy: 71.84615384615384%\n",
      "Epoch 554/2000, Train Loss: 3.275214038911413, Train Accuracy: 92.45901639344262%, Test Loss: 3.481533940021808, Test Accuracy: 71.92307692307692%\n",
      "Epoch 555/2000, Train Loss: 3.2750306246710603, Train Accuracy: 92.49180327868852%, Test Loss: 3.4814450098918033, Test Accuracy: 71.92307692307692%\n",
      "Epoch 556/2000, Train Loss: 3.2750861371149784, Train Accuracy: 92.49180327868852%, Test Loss: 3.4814963065660915, Test Accuracy: 71.84615384615384%\n",
      "Epoch 557/2000, Train Loss: 3.275036256821429, Train Accuracy: 92.49180327868852%, Test Loss: 3.4815267599545994, Test Accuracy: 71.92307692307692%\n",
      "Epoch 558/2000, Train Loss: 3.2750093116134895, Train Accuracy: 92.49180327868852%, Test Loss: 3.481541560246394, Test Accuracy: 71.84615384615384%\n",
      "Epoch 559/2000, Train Loss: 3.2750424557044857, Train Accuracy: 92.49180327868852%, Test Loss: 3.4816248416900635, Test Accuracy: 71.84615384615384%\n",
      "Epoch 560/2000, Train Loss: 3.2750219947001975, Train Accuracy: 92.49180327868852%, Test Loss: 3.4815725088119507, Test Accuracy: 71.84615384615384%\n",
      "Epoch 561/2000, Train Loss: 3.2750659028037648, Train Accuracy: 92.49180327868852%, Test Loss: 3.4816312881616445, Test Accuracy: 71.84615384615384%\n",
      "Epoch 562/2000, Train Loss: 3.275029588918217, Train Accuracy: 92.49180327868852%, Test Loss: 3.481599000784067, Test Accuracy: 71.84615384615384%\n",
      "Epoch 563/2000, Train Loss: 3.2749908986638805, Train Accuracy: 92.49180327868852%, Test Loss: 3.4815363792272715, Test Accuracy: 71.92307692307692%\n",
      "Epoch 564/2000, Train Loss: 3.275716437668097, Train Accuracy: 92.42622950819673%, Test Loss: 3.481462937134963, Test Accuracy: 71.84615384615384%\n",
      "Epoch 565/2000, Train Loss: 3.2751922841931953, Train Accuracy: 92.49180327868852%, Test Loss: 3.4813925944841824, Test Accuracy: 72.0%\n",
      "Epoch 566/2000, Train Loss: 3.275001443800379, Train Accuracy: 92.49180327868852%, Test Loss: 3.4814903277617235, Test Accuracy: 71.84615384615384%\n",
      "Epoch 567/2000, Train Loss: 3.27496588034708, Train Accuracy: 92.49180327868852%, Test Loss: 3.481579450460581, Test Accuracy: 72.0%\n",
      "Epoch 568/2000, Train Loss: 3.274989370439873, Train Accuracy: 92.52459016393442%, Test Loss: 3.4816203575867872, Test Accuracy: 71.84615384615384%\n",
      "Epoch 569/2000, Train Loss: 3.2753166566129592, Train Accuracy: 92.42622950819673%, Test Loss: 3.4814386459497304, Test Accuracy: 71.92307692307692%\n",
      "Epoch 570/2000, Train Loss: 3.275052426291294, Train Accuracy: 92.49180327868852%, Test Loss: 3.481577075444735, Test Accuracy: 71.84615384615384%\n",
      "Epoch 571/2000, Train Loss: 3.274987244215168, Train Accuracy: 92.49180327868852%, Test Loss: 3.481632195986234, Test Accuracy: 71.84615384615384%\n",
      "Epoch 572/2000, Train Loss: 3.2750965728134407, Train Accuracy: 92.49180327868852%, Test Loss: 3.4813386476956882, Test Accuracy: 72.0%\n",
      "Epoch 573/2000, Train Loss: 3.2750363662594655, Train Accuracy: 92.49180327868852%, Test Loss: 3.481504192719093, Test Accuracy: 71.76923076923077%\n",
      "Epoch 574/2000, Train Loss: 3.274997214801976, Train Accuracy: 92.49180327868852%, Test Loss: 3.4815691251021166, Test Accuracy: 71.76923076923077%\n",
      "Epoch 575/2000, Train Loss: 3.2749351868863967, Train Accuracy: 92.52459016393442%, Test Loss: 3.4814934180333066, Test Accuracy: 71.84615384615384%\n",
      "Epoch 576/2000, Train Loss: 3.2749739357682524, Train Accuracy: 92.49180327868852%, Test Loss: 3.481501423395597, Test Accuracy: 71.84615384615384%\n",
      "Epoch 577/2000, Train Loss: 3.2750073729968463, Train Accuracy: 92.52459016393442%, Test Loss: 3.4814092562748837, Test Accuracy: 71.92307692307692%\n",
      "Epoch 578/2000, Train Loss: 3.275008701887287, Train Accuracy: 92.49180327868852%, Test Loss: 3.4814888697404127, Test Accuracy: 71.76923076923077%\n",
      "Epoch 579/2000, Train Loss: 3.274955741694716, Train Accuracy: 92.49180327868852%, Test Loss: 3.4814057441858144, Test Accuracy: 71.76923076923077%\n",
      "Epoch 580/2000, Train Loss: 3.2749166371392424, Train Accuracy: 92.52459016393442%, Test Loss: 3.4814106959563036, Test Accuracy: 71.84615384615384%\n",
      "Epoch 581/2000, Train Loss: 3.274906467218868, Train Accuracy: 92.52459016393442%, Test Loss: 3.4810006343401394, Test Accuracy: 71.84615384615384%\n",
      "Epoch 582/2000, Train Loss: 3.274974455598925, Train Accuracy: 92.49180327868852%, Test Loss: 3.4806305903654833, Test Accuracy: 71.92307692307692%\n",
      "Epoch 583/2000, Train Loss: 3.2747850105410716, Train Accuracy: 92.52459016393442%, Test Loss: 3.4809894928565392, Test Accuracy: 72.0%\n",
      "Epoch 584/2000, Train Loss: 3.2746677515936677, Train Accuracy: 92.52459016393442%, Test Loss: 3.4806558902447042, Test Accuracy: 71.92307692307692%\n",
      "Epoch 585/2000, Train Loss: 3.2747671486901457, Train Accuracy: 92.52459016393442%, Test Loss: 3.480648783537058, Test Accuracy: 71.84615384615384%\n",
      "Epoch 586/2000, Train Loss: 3.2746851405159374, Train Accuracy: 92.52459016393442%, Test Loss: 3.4807281494140625, Test Accuracy: 71.92307692307692%\n",
      "Epoch 587/2000, Train Loss: 3.2748059913760326, Train Accuracy: 92.52459016393442%, Test Loss: 3.480634010755099, Test Accuracy: 71.92307692307692%\n",
      "Epoch 588/2000, Train Loss: 3.2747102330942623, Train Accuracy: 92.52459016393442%, Test Loss: 3.4806901675004225, Test Accuracy: 71.92307692307692%\n",
      "Epoch 589/2000, Train Loss: 3.274663092660122, Train Accuracy: 92.52459016393442%, Test Loss: 3.4808038014632006, Test Accuracy: 71.92307692307692%\n",
      "Epoch 590/2000, Train Loss: 3.274638402657431, Train Accuracy: 92.52459016393442%, Test Loss: 3.480693166072552, Test Accuracy: 71.92307692307692%\n",
      "Epoch 591/2000, Train Loss: 3.2745882604942946, Train Accuracy: 92.52459016393442%, Test Loss: 3.480781619365399, Test Accuracy: 71.92307692307692%\n",
      "Epoch 592/2000, Train Loss: 3.274674098999774, Train Accuracy: 92.55737704918033%, Test Loss: 3.4807699551949134, Test Accuracy: 72.0%\n",
      "Epoch 593/2000, Train Loss: 3.274650354854396, Train Accuracy: 92.52459016393442%, Test Loss: 3.4807381629943848, Test Accuracy: 72.0%\n",
      "Epoch 594/2000, Train Loss: 3.2745964136279997, Train Accuracy: 92.52459016393442%, Test Loss: 3.480787002123319, Test Accuracy: 72.0%\n",
      "Epoch 595/2000, Train Loss: 3.2744935262398642, Train Accuracy: 92.55737704918033%, Test Loss: 3.4808007387014537, Test Accuracy: 71.92307692307692%\n",
      "Epoch 596/2000, Train Loss: 3.2747423961514333, Train Accuracy: 92.55737704918033%, Test Loss: 3.480798968902001, Test Accuracy: 71.92307692307692%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 597/2000, Train Loss: 3.274704292172291, Train Accuracy: 92.52459016393442%, Test Loss: 3.480534920325646, Test Accuracy: 72.0%\n",
      "Epoch 598/2000, Train Loss: 3.2746494402650925, Train Accuracy: 92.52459016393442%, Test Loss: 3.480615597504836, Test Accuracy: 71.92307692307692%\n",
      "Epoch 599/2000, Train Loss: 3.274642373694748, Train Accuracy: 92.55737704918033%, Test Loss: 3.4806657020862284, Test Accuracy: 72.0%\n",
      "Epoch 600/2000, Train Loss: 3.274644558546973, Train Accuracy: 92.52459016393442%, Test Loss: 3.480709726993854, Test Accuracy: 71.92307692307692%\n",
      "Epoch 601/2000, Train Loss: 3.2745836367372605, Train Accuracy: 92.52459016393442%, Test Loss: 3.4807627567878137, Test Accuracy: 71.92307692307692%\n",
      "Epoch 602/2000, Train Loss: 3.2746556547821544, Train Accuracy: 92.55737704918033%, Test Loss: 3.480773238035349, Test Accuracy: 71.92307692307692%\n",
      "Epoch 603/2000, Train Loss: 3.274637843741745, Train Accuracy: 92.52459016393442%, Test Loss: 3.4807136425605187, Test Accuracy: 72.0%\n",
      "Epoch 604/2000, Train Loss: 3.274554623932135, Train Accuracy: 92.55737704918033%, Test Loss: 3.480688342681298, Test Accuracy: 71.92307692307692%\n",
      "Epoch 605/2000, Train Loss: 3.2746968660198275, Train Accuracy: 92.52459016393442%, Test Loss: 3.480709726993854, Test Accuracy: 72.07692307692308%\n",
      "Epoch 606/2000, Train Loss: 3.274639610384331, Train Accuracy: 92.52459016393442%, Test Loss: 3.4805956070239725, Test Accuracy: 72.07692307692308%\n",
      "Epoch 607/2000, Train Loss: 3.2745658804158695, Train Accuracy: 92.55737704918033%, Test Loss: 3.4807959978397074, Test Accuracy: 71.92307692307692%\n",
      "Epoch 608/2000, Train Loss: 3.274548057649956, Train Accuracy: 92.55737704918033%, Test Loss: 3.4805999168982873, Test Accuracy: 71.92307692307692%\n",
      "Epoch 609/2000, Train Loss: 3.274589026560549, Train Accuracy: 92.55737704918033%, Test Loss: 3.4806024661430945, Test Accuracy: 72.07692307692308%\n",
      "Epoch 610/2000, Train Loss: 3.2746526999551744, Train Accuracy: 92.52459016393442%, Test Loss: 3.4805494455190806, Test Accuracy: 72.0%\n",
      "Epoch 611/2000, Train Loss: 3.2745317122975335, Train Accuracy: 92.55737704918033%, Test Loss: 3.4806369818173923, Test Accuracy: 72.0%\n",
      "Epoch 612/2000, Train Loss: 3.2746047973632812, Train Accuracy: 92.52459016393442%, Test Loss: 3.48075008392334, Test Accuracy: 72.0%\n",
      "Epoch 613/2000, Train Loss: 3.2746520237844496, Train Accuracy: 92.52459016393442%, Test Loss: 3.4804833027032704, Test Accuracy: 72.0%\n",
      "Saved the best model with validation loss:  3.4804833027032704\n",
      "Epoch 614/2000, Train Loss: 3.2744689925772246, Train Accuracy: 92.55737704918033%, Test Loss: 3.4805508301808286, Test Accuracy: 72.0%\n",
      "Epoch 615/2000, Train Loss: 3.2745341981043583, Train Accuracy: 92.52459016393442%, Test Loss: 3.48045867223006, Test Accuracy: 72.0%\n",
      "Epoch 616/2000, Train Loss: 3.2744849978900348, Train Accuracy: 92.55737704918033%, Test Loss: 3.480405459037194, Test Accuracy: 72.0%\n",
      "Epoch 617/2000, Train Loss: 3.273483608589798, Train Accuracy: 92.65573770491804%, Test Loss: 3.473164805999169, Test Accuracy: 72.6923076923077%\n",
      "Saved the best model with validation loss:  3.473164805999169\n",
      "Epoch 618/2000, Train Loss: 3.268565103655956, Train Accuracy: 93.24590163934427%, Test Loss: 3.475334268349868, Test Accuracy: 72.46153846153847%\n",
      "Epoch 619/2000, Train Loss: 3.266386215804053, Train Accuracy: 93.50819672131148%, Test Loss: 3.4745010687754703, Test Accuracy: 72.53846153846153%\n",
      "Epoch 620/2000, Train Loss: 3.2653919438846777, Train Accuracy: 93.57377049180327%, Test Loss: 3.4751987824073205, Test Accuracy: 72.53846153846153%\n",
      "Epoch 621/2000, Train Loss: 3.2652038159917613, Train Accuracy: 93.57377049180327%, Test Loss: 3.4748857663227963, Test Accuracy: 72.61538461538461%\n",
      "Epoch 622/2000, Train Loss: 3.264744492827869, Train Accuracy: 93.57377049180327%, Test Loss: 3.474844162280743, Test Accuracy: 72.53846153846153%\n",
      "Epoch 623/2000, Train Loss: 3.2644839208634173, Train Accuracy: 93.60655737704919%, Test Loss: 3.474912111575787, Test Accuracy: 72.53846153846153%\n",
      "Epoch 624/2000, Train Loss: 3.264371817229224, Train Accuracy: 93.63934426229508%, Test Loss: 3.474577390230619, Test Accuracy: 72.53846153846153%\n",
      "Epoch 625/2000, Train Loss: 3.2642261356603903, Train Accuracy: 93.63934426229508%, Test Loss: 3.474656719427842, Test Accuracy: 72.61538461538461%\n",
      "Epoch 626/2000, Train Loss: 3.264202817541654, Train Accuracy: 93.63934426229508%, Test Loss: 3.4747889592097354, Test Accuracy: 72.53846153846153%\n",
      "Epoch 627/2000, Train Loss: 3.2641203051707786, Train Accuracy: 93.60655737704919%, Test Loss: 3.4748966235380907, Test Accuracy: 72.53846153846153%\n",
      "Epoch 628/2000, Train Loss: 3.2639500156777803, Train Accuracy: 93.67213114754098%, Test Loss: 3.474505800467271, Test Accuracy: 72.6923076923077%\n",
      "Epoch 629/2000, Train Loss: 3.2641035377002154, Train Accuracy: 93.63934426229508%, Test Loss: 3.4744774928459754, Test Accuracy: 72.6923076923077%\n",
      "Epoch 630/2000, Train Loss: 3.2638511657714844, Train Accuracy: 93.63934426229508%, Test Loss: 3.4746314103786764, Test Accuracy: 72.61538461538461%\n",
      "Epoch 631/2000, Train Loss: 3.26381275302074, Train Accuracy: 93.67213114754098%, Test Loss: 3.4743710664602427, Test Accuracy: 72.6923076923077%\n",
      "Epoch 632/2000, Train Loss: 3.263910145056052, Train Accuracy: 93.63934426229508%, Test Loss: 3.4744552740683923, Test Accuracy: 72.61538461538461%\n",
      "Epoch 633/2000, Train Loss: 3.263923140822864, Train Accuracy: 93.63934426229508%, Test Loss: 3.4745638920710635, Test Accuracy: 72.61538461538461%\n",
      "Epoch 634/2000, Train Loss: 3.2635856104678793, Train Accuracy: 93.67213114754098%, Test Loss: 3.474434311573322, Test Accuracy: 72.61538461538461%\n",
      "Epoch 635/2000, Train Loss: 3.263536636946631, Train Accuracy: 93.67213114754098%, Test Loss: 3.4744617572197547, Test Accuracy: 72.6923076923077%\n",
      "Epoch 636/2000, Train Loss: 3.2634844154608054, Train Accuracy: 93.67213114754098%, Test Loss: 3.4744106714542093, Test Accuracy: 72.61538461538461%\n",
      "Epoch 637/2000, Train Loss: 3.263305836036557, Train Accuracy: 93.70491803278688%, Test Loss: 3.474354918186481, Test Accuracy: 72.61538461538461%\n",
      "Epoch 638/2000, Train Loss: 3.263400378774424, Train Accuracy: 93.70491803278688%, Test Loss: 3.474221110343933, Test Accuracy: 72.61538461538461%\n",
      "Epoch 639/2000, Train Loss: 3.263277264892078, Train Accuracy: 93.70491803278688%, Test Loss: 3.4741733074188232, Test Accuracy: 72.61538461538461%\n",
      "Epoch 640/2000, Train Loss: 3.263291296411733, Train Accuracy: 93.70491803278688%, Test Loss: 3.4737900220430813, Test Accuracy: 72.61538461538461%\n",
      "Epoch 641/2000, Train Loss: 3.2632583477458015, Train Accuracy: 93.70491803278688%, Test Loss: 3.473376906835116, Test Accuracy: 72.84615384615384%\n",
      "Epoch 642/2000, Train Loss: 3.2631313605386705, Train Accuracy: 93.70491803278688%, Test Loss: 3.4733009246679454, Test Accuracy: 72.84615384615384%\n",
      "Epoch 643/2000, Train Loss: 3.2630022627408386, Train Accuracy: 93.73770491803279%, Test Loss: 3.4733566412558923, Test Accuracy: 72.76923076923077%\n",
      "Epoch 644/2000, Train Loss: 3.263077735900879, Train Accuracy: 93.70491803278688%, Test Loss: 3.473333010306725, Test Accuracy: 72.6923076923077%\n",
      "Epoch 645/2000, Train Loss: 3.2629564394716355, Train Accuracy: 93.73770491803279%, Test Loss: 3.473333688882681, Test Accuracy: 72.6923076923077%\n",
      "Epoch 646/2000, Train Loss: 3.2629802422445326, Train Accuracy: 93.73770491803279%, Test Loss: 3.473398346167344, Test Accuracy: 72.6923076923077%\n",
      "Epoch 647/2000, Train Loss: 3.263014637055944, Train Accuracy: 93.70491803278688%, Test Loss: 3.473362079033485, Test Accuracy: 72.61538461538461%\n",
      "Epoch 648/2000, Train Loss: 3.2628917303241667, Train Accuracy: 93.73770491803279%, Test Loss: 3.473350240634038, Test Accuracy: 72.6923076923077%\n",
      "Epoch 649/2000, Train Loss: 3.262921040175391, Train Accuracy: 93.73770491803279%, Test Loss: 3.473279888813312, Test Accuracy: 72.61538461538461%\n",
      "Epoch 650/2000, Train Loss: 3.2628883455620437, Train Accuracy: 93.73770491803279%, Test Loss: 3.473342675429124, Test Accuracy: 72.61538461538461%\n",
      "Epoch 651/2000, Train Loss: 3.2630631532825407, Train Accuracy: 93.73770491803279%, Test Loss: 3.473340786420382, Test Accuracy: 72.61538461538461%\n",
      "Epoch 652/2000, Train Loss: 3.262871527280964, Train Accuracy: 93.73770491803279%, Test Loss: 3.473265345279987, Test Accuracy: 72.6923076923077%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 653/2000, Train Loss: 3.2629101510907783, Train Accuracy: 93.73770491803279%, Test Loss: 3.473274882023151, Test Accuracy: 72.61538461538461%\n",
      "Epoch 654/2000, Train Loss: 3.262881845724387, Train Accuracy: 93.73770491803279%, Test Loss: 3.473219972390395, Test Accuracy: 72.61538461538461%\n",
      "Epoch 655/2000, Train Loss: 3.2628964947872476, Train Accuracy: 93.73770491803279%, Test Loss: 3.473212847342858, Test Accuracy: 72.61538461538461%\n",
      "Epoch 656/2000, Train Loss: 3.2629008840342038, Train Accuracy: 93.73770491803279%, Test Loss: 3.473198789816636, Test Accuracy: 72.61538461538461%\n",
      "Epoch 657/2000, Train Loss: 3.2629594763771435, Train Accuracy: 93.73770491803279%, Test Loss: 3.4731061366888194, Test Accuracy: 72.61538461538461%\n",
      "Epoch 658/2000, Train Loss: 3.2628431906465623, Train Accuracy: 93.73770491803279%, Test Loss: 3.473215048129742, Test Accuracy: 72.61538461538461%\n",
      "Epoch 659/2000, Train Loss: 3.262970893109431, Train Accuracy: 93.73770491803279%, Test Loss: 3.473208950116084, Test Accuracy: 72.61538461538461%\n",
      "Epoch 660/2000, Train Loss: 3.2629546845545536, Train Accuracy: 93.73770491803279%, Test Loss: 3.4731779281909647, Test Accuracy: 72.6923076923077%\n",
      "Epoch 661/2000, Train Loss: 3.2628613182755766, Train Accuracy: 93.73770491803279%, Test Loss: 3.4731394052505493, Test Accuracy: 72.6923076923077%\n",
      "Epoch 662/2000, Train Loss: 3.262820884829662, Train Accuracy: 93.73770491803279%, Test Loss: 3.4730880902363706, Test Accuracy: 72.6923076923077%\n",
      "Epoch 663/2000, Train Loss: 3.2628942395819993, Train Accuracy: 93.73770491803279%, Test Loss: 3.4731467136969933, Test Accuracy: 72.6923076923077%\n",
      "Epoch 664/2000, Train Loss: 3.2629448116802777, Train Accuracy: 93.73770491803279%, Test Loss: 3.4730525475281935, Test Accuracy: 72.6923076923077%\n",
      "Epoch 665/2000, Train Loss: 3.262913844624504, Train Accuracy: 93.73770491803279%, Test Loss: 3.473126640686622, Test Accuracy: 72.6923076923077%\n",
      "Epoch 666/2000, Train Loss: 3.2628662859807247, Train Accuracy: 93.73770491803279%, Test Loss: 3.4729780784020057, Test Accuracy: 72.6923076923077%\n",
      "Epoch 667/2000, Train Loss: 3.2628434994181648, Train Accuracy: 93.73770491803279%, Test Loss: 3.473072207891024, Test Accuracy: 72.76923076923077%\n",
      "Epoch 668/2000, Train Loss: 3.2629035691745947, Train Accuracy: 93.77049180327869%, Test Loss: 3.4736096217082095, Test Accuracy: 72.76923076923077%\n",
      "Epoch 669/2000, Train Loss: 3.2623917079362714, Train Accuracy: 93.77049180327869%, Test Loss: 3.4730019936194787, Test Accuracy: 72.76923076923077%\n",
      "Epoch 670/2000, Train Loss: 3.262237337769055, Train Accuracy: 93.80327868852459%, Test Loss: 3.473221182823181, Test Accuracy: 72.84615384615384%\n",
      "Epoch 671/2000, Train Loss: 3.262257806590346, Train Accuracy: 93.80327868852459%, Test Loss: 3.473018774619469, Test Accuracy: 72.76923076923077%\n",
      "Epoch 672/2000, Train Loss: 3.262232936796595, Train Accuracy: 93.80327868852459%, Test Loss: 3.473253690279447, Test Accuracy: 72.76923076923077%\n",
      "Epoch 673/2000, Train Loss: 3.262173828531484, Train Accuracy: 93.80327868852459%, Test Loss: 3.473214186154879, Test Accuracy: 72.76923076923077%\n",
      "Epoch 674/2000, Train Loss: 3.2622039435339754, Train Accuracy: 93.80327868852459%, Test Loss: 3.473183191739596, Test Accuracy: 72.76923076923077%\n",
      "Epoch 675/2000, Train Loss: 3.262164358232842, Train Accuracy: 93.80327868852459%, Test Loss: 3.4731213404582095, Test Accuracy: 72.76923076923077%\n",
      "Epoch 676/2000, Train Loss: 3.262210095515017, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730643125680776, Test Accuracy: 72.76923076923077%\n",
      "Epoch 677/2000, Train Loss: 3.262219530637147, Train Accuracy: 93.80327868852459%, Test Loss: 3.472980178319491, Test Accuracy: 72.76923076923077%\n",
      "Epoch 678/2000, Train Loss: 3.2621836154187314, Train Accuracy: 93.80327868852459%, Test Loss: 3.4732288213876576, Test Accuracy: 72.76923076923077%\n",
      "Epoch 679/2000, Train Loss: 3.2622163139405798, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730505759899435, Test Accuracy: 72.76923076923077%\n",
      "Epoch 680/2000, Train Loss: 3.2621382103591667, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730624510691714, Test Accuracy: 72.76923076923077%\n",
      "Epoch 681/2000, Train Loss: 3.2621286306224886, Train Accuracy: 93.80327868852459%, Test Loss: 3.473006798670842, Test Accuracy: 72.76923076923077%\n",
      "Epoch 682/2000, Train Loss: 3.2622166930652057, Train Accuracy: 93.80327868852459%, Test Loss: 3.473010613368108, Test Accuracy: 72.76923076923077%\n",
      "Epoch 683/2000, Train Loss: 3.262225299585061, Train Accuracy: 93.80327868852459%, Test Loss: 3.472902453862704, Test Accuracy: 72.76923076923077%\n",
      "Epoch 684/2000, Train Loss: 3.2622413205318765, Train Accuracy: 93.77049180327869%, Test Loss: 3.4729458643839908, Test Accuracy: 72.84615384615384%\n",
      "Epoch 685/2000, Train Loss: 3.262150346255693, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729038385244517, Test Accuracy: 72.84615384615384%\n",
      "Epoch 686/2000, Train Loss: 3.2622144964874766, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729774365058312, Test Accuracy: 72.76923076923077%\n",
      "Epoch 687/2000, Train Loss: 3.262275899042849, Train Accuracy: 93.77049180327869%, Test Loss: 3.4728154861010037, Test Accuracy: 72.76923076923077%\n",
      "Epoch 688/2000, Train Loss: 3.262149842059026, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728846916785607, Test Accuracy: 72.76923076923077%\n",
      "Epoch 689/2000, Train Loss: 3.262110530352983, Train Accuracy: 93.80327868852459%, Test Loss: 3.472883160297687, Test Accuracy: 72.76923076923077%\n",
      "Epoch 690/2000, Train Loss: 3.2621251833243448, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728352381632877, Test Accuracy: 72.76923076923077%\n",
      "Epoch 691/2000, Train Loss: 3.2621697246051227, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728444264485288, Test Accuracy: 72.84615384615384%\n",
      "Epoch 692/2000, Train Loss: 3.262108959135462, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728517624048085, Test Accuracy: 72.84615384615384%\n",
      "Epoch 693/2000, Train Loss: 3.262095709316066, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728078108567457, Test Accuracy: 72.84615384615384%\n",
      "Epoch 694/2000, Train Loss: 3.2621215406011363, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729518431883593, Test Accuracy: 72.84615384615384%\n",
      "Epoch 695/2000, Train Loss: 3.2620859028863127, Train Accuracy: 93.80327868852459%, Test Loss: 3.4727607782070455, Test Accuracy: 72.84615384615384%\n",
      "Epoch 696/2000, Train Loss: 3.2622136717937034, Train Accuracy: 93.80327868852459%, Test Loss: 3.4727644553551307, Test Accuracy: 72.76923076923077%\n",
      "Epoch 697/2000, Train Loss: 3.26231152894067, Train Accuracy: 93.77049180327869%, Test Loss: 3.472844279729403, Test Accuracy: 72.76923076923077%\n",
      "Epoch 698/2000, Train Loss: 3.2621192345853713, Train Accuracy: 93.80327868852459%, Test Loss: 3.4727419523092418, Test Accuracy: 72.84615384615384%\n",
      "Epoch 699/2000, Train Loss: 3.2621466331794613, Train Accuracy: 93.80327868852459%, Test Loss: 3.472828984260559, Test Accuracy: 72.76923076923077%\n",
      "Epoch 700/2000, Train Loss: 3.262107407460447, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728494607485256, Test Accuracy: 72.76923076923077%\n",
      "Epoch 701/2000, Train Loss: 3.262104675418041, Train Accuracy: 93.80327868852459%, Test Loss: 3.4727586691196146, Test Accuracy: 72.76923076923077%\n",
      "Epoch 702/2000, Train Loss: 3.2623932400687794, Train Accuracy: 93.77049180327869%, Test Loss: 3.4727271428475013, Test Accuracy: 72.84615384615384%\n",
      "Epoch 703/2000, Train Loss: 3.26207691333333, Train Accuracy: 93.80327868852459%, Test Loss: 3.472796632693364, Test Accuracy: 72.84615384615384%\n",
      "Epoch 704/2000, Train Loss: 3.2620871418812234, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729036734654355, Test Accuracy: 72.76923076923077%\n",
      "Epoch 705/2000, Train Loss: 3.2620963190422683, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729138796146097, Test Accuracy: 72.76923076923077%\n",
      "Epoch 706/2000, Train Loss: 3.262221269920224, Train Accuracy: 93.80327868852459%, Test Loss: 3.472797210399921, Test Accuracy: 72.76923076923077%\n",
      "Epoch 707/2000, Train Loss: 3.2622695086432283, Train Accuracy: 93.77049180327869%, Test Loss: 3.4727829419649563, Test Accuracy: 72.84615384615384%\n",
      "Epoch 708/2000, Train Loss: 3.2620744548860143, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729024997124305, Test Accuracy: 72.84615384615384%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709/2000, Train Loss: 3.2620974016971274, Train Accuracy: 93.80327868852459%, Test Loss: 3.472967798893268, Test Accuracy: 72.76923076923077%\n",
      "Epoch 710/2000, Train Loss: 3.262067032642052, Train Accuracy: 93.80327868852459%, Test Loss: 3.472808003425598, Test Accuracy: 72.76923076923077%\n",
      "Epoch 711/2000, Train Loss: 3.262060974465042, Train Accuracy: 93.80327868852459%, Test Loss: 3.472959830210759, Test Accuracy: 72.84615384615384%\n",
      "Epoch 712/2000, Train Loss: 3.262065191737941, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730449456434984, Test Accuracy: 72.76923076923077%\n",
      "Epoch 713/2000, Train Loss: 3.2620758502209775, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729069746457615, Test Accuracy: 72.76923076923077%\n",
      "Epoch 714/2000, Train Loss: 3.262061677995275, Train Accuracy: 93.80327868852459%, Test Loss: 3.472864371079665, Test Accuracy: 72.84615384615384%\n",
      "Epoch 715/2000, Train Loss: 3.262102732892896, Train Accuracy: 93.80327868852459%, Test Loss: 3.47295606136322, Test Accuracy: 72.76923076923077%\n",
      "Epoch 716/2000, Train Loss: 3.26205162532994, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729091846025906, Test Accuracy: 72.76923076923077%\n",
      "Epoch 717/2000, Train Loss: 3.2620653676204996, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729835161795983, Test Accuracy: 72.76923076923077%\n",
      "Epoch 718/2000, Train Loss: 3.262084054165199, Train Accuracy: 93.80327868852459%, Test Loss: 3.472862335351797, Test Accuracy: 72.84615384615384%\n",
      "Epoch 719/2000, Train Loss: 3.2620778396481374, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728386310430674, Test Accuracy: 72.76923076923077%\n",
      "Epoch 720/2000, Train Loss: 3.262316273861244, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728627663392286, Test Accuracy: 72.76923076923077%\n",
      "Epoch 721/2000, Train Loss: 3.2623531974729945, Train Accuracy: 93.77049180327869%, Test Loss: 3.4727347997518687, Test Accuracy: 72.76923076923077%\n",
      "Epoch 722/2000, Train Loss: 3.2623424803624386, Train Accuracy: 93.77049180327869%, Test Loss: 3.473012841664828, Test Accuracy: 72.76923076923077%\n",
      "Epoch 723/2000, Train Loss: 3.2623211360368574, Train Accuracy: 93.77049180327869%, Test Loss: 3.472824353438157, Test Accuracy: 72.84615384615384%\n",
      "Epoch 724/2000, Train Loss: 3.262170443769361, Train Accuracy: 93.80327868852459%, Test Loss: 3.472843912931589, Test Accuracy: 72.84615384615384%\n",
      "Epoch 725/2000, Train Loss: 3.262171178567605, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729712192828837, Test Accuracy: 72.76923076923077%\n",
      "Epoch 726/2000, Train Loss: 3.2622206289260114, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728245643469005, Test Accuracy: 72.76923076923077%\n",
      "Epoch 727/2000, Train Loss: 3.262342124688821, Train Accuracy: 93.77049180327869%, Test Loss: 3.4730683931937585, Test Accuracy: 72.84615384615384%\n",
      "Epoch 728/2000, Train Loss: 3.2623219333711218, Train Accuracy: 93.77049180327869%, Test Loss: 3.4730204068697414, Test Accuracy: 72.76923076923077%\n",
      "Epoch 729/2000, Train Loss: 3.2622574430997253, Train Accuracy: 93.77049180327869%, Test Loss: 3.4729094872107873, Test Accuracy: 72.76923076923077%\n",
      "Epoch 730/2000, Train Loss: 3.26205768350695, Train Accuracy: 93.80327868852459%, Test Loss: 3.473012768305265, Test Accuracy: 72.76923076923077%\n",
      "Epoch 731/2000, Train Loss: 3.262057007336226, Train Accuracy: 93.80327868852459%, Test Loss: 3.472846141228309, Test Accuracy: 72.84615384615384%\n",
      "Epoch 732/2000, Train Loss: 3.2620391220342917, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729796739724965, Test Accuracy: 72.92307692307692%\n",
      "Epoch 733/2000, Train Loss: 3.262040861317369, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729486887271586, Test Accuracy: 72.84615384615384%\n",
      "Epoch 734/2000, Train Loss: 3.2620455945124394, Train Accuracy: 93.80327868852459%, Test Loss: 3.473000489748441, Test Accuracy: 72.76923076923077%\n",
      "Epoch 735/2000, Train Loss: 3.2620420416847606, Train Accuracy: 93.80327868852459%, Test Loss: 3.472986478071946, Test Accuracy: 72.76923076923077%\n",
      "Epoch 736/2000, Train Loss: 3.262040153878634, Train Accuracy: 93.80327868852459%, Test Loss: 3.472944351343008, Test Accuracy: 72.76923076923077%\n",
      "Epoch 737/2000, Train Loss: 3.2620410841019427, Train Accuracy: 93.80327868852459%, Test Loss: 3.473023194533128, Test Accuracy: 72.76923076923077%\n",
      "Epoch 738/2000, Train Loss: 3.2620553462231747, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729174283834605, Test Accuracy: 72.84615384615384%\n",
      "Epoch 739/2000, Train Loss: 3.262064484299206, Train Accuracy: 93.80327868852459%, Test Loss: 3.472786545753479, Test Accuracy: 72.84615384615384%\n",
      "Epoch 740/2000, Train Loss: 3.262070616737741, Train Accuracy: 93.80327868852459%, Test Loss: 3.472629794707665, Test Accuracy: 72.84615384615384%\n",
      "Epoch 741/2000, Train Loss: 3.262057706957958, Train Accuracy: 93.80327868852459%, Test Loss: 3.473023240382855, Test Accuracy: 72.76923076923077%\n",
      "Epoch 742/2000, Train Loss: 3.26203468588532, Train Accuracy: 93.80327868852459%, Test Loss: 3.472792442028339, Test Accuracy: 72.76923076923077%\n",
      "Epoch 743/2000, Train Loss: 3.2621205048482924, Train Accuracy: 93.80327868852459%, Test Loss: 3.472940591665415, Test Accuracy: 72.76923076923077%\n",
      "Epoch 744/2000, Train Loss: 3.2620705854697305, Train Accuracy: 93.80327868852459%, Test Loss: 3.473141385958745, Test Accuracy: 72.84615384615384%\n",
      "Epoch 745/2000, Train Loss: 3.26204252243042, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728915783075185, Test Accuracy: 72.76923076923077%\n",
      "Epoch 746/2000, Train Loss: 3.262035729455166, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728869933348436, Test Accuracy: 72.76923076923077%\n",
      "Epoch 747/2000, Train Loss: 3.262031367567719, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728570534632754, Test Accuracy: 72.76923076923077%\n",
      "Epoch 748/2000, Train Loss: 3.2620317036988307, Train Accuracy: 93.80327868852459%, Test Loss: 3.472997647065383, Test Accuracy: 72.76923076923077%\n",
      "Epoch 749/2000, Train Loss: 3.262040009264086, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729207020539503, Test Accuracy: 72.76923076923077%\n",
      "Epoch 750/2000, Train Loss: 3.2620372459536693, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728795289993286, Test Accuracy: 72.76923076923077%\n",
      "Epoch 751/2000, Train Loss: 3.262034201231159, Train Accuracy: 93.80327868852459%, Test Loss: 3.472895099566533, Test Accuracy: 72.76923076923077%\n",
      "Epoch 752/2000, Train Loss: 3.2620374022937213, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728968877058763, Test Accuracy: 72.76923076923077%\n",
      "Epoch 753/2000, Train Loss: 3.262031621620303, Train Accuracy: 93.80327868852459%, Test Loss: 3.472974447103647, Test Accuracy: 72.76923076923077%\n",
      "Epoch 754/2000, Train Loss: 3.2620335602369464, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728252245829654, Test Accuracy: 72.84615384615384%\n",
      "Epoch 755/2000, Train Loss: 3.2620369215480616, Train Accuracy: 93.80327868852459%, Test Loss: 3.4727501227305484, Test Accuracy: 72.84615384615384%\n",
      "Epoch 756/2000, Train Loss: 3.2623980592508786, Train Accuracy: 93.77049180327869%, Test Loss: 3.4727056209857645, Test Accuracy: 72.6923076923077%\n",
      "Epoch 757/2000, Train Loss: 3.2624244650856395, Train Accuracy: 93.73770491803279%, Test Loss: 3.4730073580375085, Test Accuracy: 72.76923076923077%\n",
      "Epoch 758/2000, Train Loss: 3.263731667252838, Train Accuracy: 93.63934426229508%, Test Loss: 3.4724935476596537, Test Accuracy: 72.6923076923077%\n",
      "Epoch 759/2000, Train Loss: 3.263662717381462, Train Accuracy: 93.63934426229508%, Test Loss: 3.4726420732644887, Test Accuracy: 72.84615384615384%\n",
      "Epoch 760/2000, Train Loss: 3.263062563098845, Train Accuracy: 93.67213114754098%, Test Loss: 3.471754413384658, Test Accuracy: 72.92307692307692%\n",
      "Saved the best model with validation loss:  3.471754413384658\n",
      "Epoch 761/2000, Train Loss: 3.2623482102253396, Train Accuracy: 93.77049180327869%, Test Loss: 3.4725782596147976, Test Accuracy: 72.6923076923077%\n",
      "Epoch 762/2000, Train Loss: 3.2623356131256602, Train Accuracy: 93.77049180327869%, Test Loss: 3.472616333227891, Test Accuracy: 72.92307692307692%\n",
      "Epoch 763/2000, Train Loss: 3.262328534829812, Train Accuracy: 93.77049180327869%, Test Loss: 3.4725713913257303, Test Accuracy: 72.84615384615384%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 764/2000, Train Loss: 3.2623176301111942, Train Accuracy: 93.77049180327869%, Test Loss: 3.472676451389606, Test Accuracy: 72.76923076923077%\n",
      "Epoch 765/2000, Train Loss: 3.2622722055091233, Train Accuracy: 93.77049180327869%, Test Loss: 3.4727283532802877, Test Accuracy: 72.76923076923077%\n",
      "Epoch 766/2000, Train Loss: 3.2620288622183877, Train Accuracy: 93.80327868852459%, Test Loss: 3.472716588240403, Test Accuracy: 72.76923076923077%\n",
      "Epoch 767/2000, Train Loss: 3.26202404694479, Train Accuracy: 93.80327868852459%, Test Loss: 3.47264656653771, Test Accuracy: 72.76923076923077%\n",
      "Epoch 768/2000, Train Loss: 3.262027087758799, Train Accuracy: 93.80327868852459%, Test Loss: 3.4727004307966967, Test Accuracy: 72.76923076923077%\n",
      "Epoch 769/2000, Train Loss: 3.2620201892540104, Train Accuracy: 93.80327868852459%, Test Loss: 3.472669032903818, Test Accuracy: 72.92307692307692%\n",
      "Epoch 770/2000, Train Loss: 3.262024410435411, Train Accuracy: 93.80327868852459%, Test Loss: 3.4727000089792104, Test Accuracy: 72.76923076923077%\n",
      "Epoch 771/2000, Train Loss: 3.262020474574605, Train Accuracy: 93.80327868852459%, Test Loss: 3.4727348547715406, Test Accuracy: 72.84615384615384%\n",
      "Epoch 772/2000, Train Loss: 3.262021623673986, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728155319507303, Test Accuracy: 72.6923076923077%\n",
      "Epoch 773/2000, Train Loss: 3.2620167145963577, Train Accuracy: 93.80327868852459%, Test Loss: 3.4727463263731737, Test Accuracy: 72.84615384615384%\n",
      "Epoch 774/2000, Train Loss: 3.2620194075537507, Train Accuracy: 93.80327868852459%, Test Loss: 3.472729132725642, Test Accuracy: 72.76923076923077%\n",
      "Epoch 775/2000, Train Loss: 3.262019657697834, Train Accuracy: 93.80327868852459%, Test Loss: 3.4726336552546573, Test Accuracy: 72.76923076923077%\n",
      "Epoch 776/2000, Train Loss: 3.2620247582920263, Train Accuracy: 93.80327868852459%, Test Loss: 3.472758412361145, Test Accuracy: 72.76923076923077%\n",
      "Epoch 777/2000, Train Loss: 3.2620180239442917, Train Accuracy: 93.80327868852459%, Test Loss: 3.472709215604342, Test Accuracy: 72.76923076923077%\n",
      "Epoch 778/2000, Train Loss: 3.2620191261416576, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728240416600156, Test Accuracy: 72.76923076923077%\n",
      "Epoch 779/2000, Train Loss: 3.2620195326257924, Train Accuracy: 93.80327868852459%, Test Loss: 3.472743401160607, Test Accuracy: 72.76923076923077%\n",
      "Epoch 780/2000, Train Loss: 3.2620189854356108, Train Accuracy: 93.80327868852459%, Test Loss: 3.4727527728447547, Test Accuracy: 72.6923076923077%\n",
      "Epoch 781/2000, Train Loss: 3.262015628032997, Train Accuracy: 93.80327868852459%, Test Loss: 3.472820153603187, Test Accuracy: 72.6923076923077%\n",
      "Epoch 782/2000, Train Loss: 3.262014084174985, Train Accuracy: 93.80327868852459%, Test Loss: 3.472698349219102, Test Accuracy: 72.76923076923077%\n",
      "Epoch 783/2000, Train Loss: 3.2620162612102073, Train Accuracy: 93.80327868852459%, Test Loss: 3.472826829323402, Test Accuracy: 72.76923076923077%\n",
      "Epoch 784/2000, Train Loss: 3.2620122120028636, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728450500048123, Test Accuracy: 72.6923076923077%\n",
      "Epoch 785/2000, Train Loss: 3.2620240938468057, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728754116938663, Test Accuracy: 72.6923076923077%\n",
      "Epoch 786/2000, Train Loss: 3.262025629887815, Train Accuracy: 93.80327868852459%, Test Loss: 3.4727376149250913, Test Accuracy: 72.76923076923077%\n",
      "Epoch 787/2000, Train Loss: 3.2620173594990716, Train Accuracy: 93.80327868852459%, Test Loss: 3.472957427685077, Test Accuracy: 72.6923076923077%\n",
      "Epoch 788/2000, Train Loss: 3.2620161361381657, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728997945785522, Test Accuracy: 72.6923076923077%\n",
      "Epoch 789/2000, Train Loss: 3.262026966595259, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728031433545627, Test Accuracy: 72.84615384615384%\n",
      "Epoch 790/2000, Train Loss: 3.262032751177178, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728023455693173, Test Accuracy: 72.76923076923077%\n",
      "Epoch 791/2000, Train Loss: 3.2620111918840253, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728750632359433, Test Accuracy: 72.6923076923077%\n",
      "Epoch 792/2000, Train Loss: 3.2620256455218204, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730353080309353, Test Accuracy: 72.76923076923077%\n",
      "Epoch 793/2000, Train Loss: 3.2620098942615945, Train Accuracy: 93.80327868852459%, Test Loss: 3.472893531505878, Test Accuracy: 72.6923076923077%\n",
      "Epoch 794/2000, Train Loss: 3.262015405248423, Train Accuracy: 93.80327868852459%, Test Loss: 3.472958729817317, Test Accuracy: 72.76923076923077%\n",
      "Epoch 795/2000, Train Loss: 3.2620161869486823, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728914132485023, Test Accuracy: 72.6923076923077%\n",
      "Epoch 796/2000, Train Loss: 3.262032383778056, Train Accuracy: 93.80327868852459%, Test Loss: 3.473060286962069, Test Accuracy: 72.6923076923077%\n",
      "Epoch 797/2000, Train Loss: 3.262019958652434, Train Accuracy: 93.80327868852459%, Test Loss: 3.472983378630418, Test Accuracy: 72.76923076923077%\n",
      "Epoch 798/2000, Train Loss: 3.2620147525287067, Train Accuracy: 93.80327868852459%, Test Loss: 3.472976042674138, Test Accuracy: 72.6923076923077%\n",
      "Epoch 799/2000, Train Loss: 3.2620090734763223, Train Accuracy: 93.80327868852459%, Test Loss: 3.4731407715724063, Test Accuracy: 72.6923076923077%\n",
      "Epoch 800/2000, Train Loss: 3.26201096519095, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729253695561337, Test Accuracy: 72.6923076923077%\n",
      "Epoch 801/2000, Train Loss: 3.2620076273308425, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730883286549497, Test Accuracy: 72.6923076923077%\n",
      "Epoch 802/2000, Train Loss: 3.2620110941714926, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730315025036154, Test Accuracy: 72.6923076923077%\n",
      "Epoch 803/2000, Train Loss: 3.262006185093864, Train Accuracy: 93.80327868852459%, Test Loss: 3.473006716141334, Test Accuracy: 72.76923076923077%\n",
      "Epoch 804/2000, Train Loss: 3.262008608364668, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730764719156118, Test Accuracy: 72.6923076923077%\n",
      "Epoch 805/2000, Train Loss: 3.2620053369490827, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730627903571496, Test Accuracy: 72.76923076923077%\n",
      "Epoch 806/2000, Train Loss: 3.2620049656414594, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730734733434825, Test Accuracy: 72.76923076923077%\n",
      "Epoch 807/2000, Train Loss: 3.2620060717473263, Train Accuracy: 93.80327868852459%, Test Loss: 3.472969311934251, Test Accuracy: 72.6923076923077%\n",
      "Epoch 808/2000, Train Loss: 3.26200752961831, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730478800260105, Test Accuracy: 72.76923076923077%\n",
      "Epoch 809/2000, Train Loss: 3.262013232121702, Train Accuracy: 93.80327868852459%, Test Loss: 3.473135929841262, Test Accuracy: 72.6923076923077%\n",
      "Epoch 810/2000, Train Loss: 3.2620050711709947, Train Accuracy: 93.80327868852459%, Test Loss: 3.473097700339097, Test Accuracy: 72.6923076923077%\n",
      "Epoch 811/2000, Train Loss: 3.2619954445323005, Train Accuracy: 93.80327868852459%, Test Loss: 3.47332633458651, Test Accuracy: 72.6923076923077%\n",
      "Epoch 812/2000, Train Loss: 3.2620845114598507, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729642684643087, Test Accuracy: 72.6923076923077%\n",
      "Epoch 813/2000, Train Loss: 3.2623231606405287, Train Accuracy: 93.77049180327869%, Test Loss: 3.4728780801479635, Test Accuracy: 72.76923076923077%\n",
      "Epoch 814/2000, Train Loss: 3.2622933075076244, Train Accuracy: 93.77049180327869%, Test Loss: 3.4731260446401744, Test Accuracy: 72.6923076923077%\n",
      "Epoch 815/2000, Train Loss: 3.262004090137169, Train Accuracy: 93.80327868852459%, Test Loss: 3.473033510721647, Test Accuracy: 72.6923076923077%\n",
      "Epoch 816/2000, Train Loss: 3.2620017294023858, Train Accuracy: 93.80327868852459%, Test Loss: 3.472967798893268, Test Accuracy: 72.76923076923077%\n",
      "Epoch 817/2000, Train Loss: 3.262000216812384, Train Accuracy: 93.80327868852459%, Test Loss: 3.473127383452195, Test Accuracy: 72.6923076923077%\n",
      "Epoch 818/2000, Train Loss: 3.2619987315818912, Train Accuracy: 93.80327868852459%, Test Loss: 3.472958252980159, Test Accuracy: 72.76923076923077%\n",
      "Epoch 819/2000, Train Loss: 3.2619970626518375, Train Accuracy: 93.80327868852459%, Test Loss: 3.473143687615028, Test Accuracy: 72.6923076923077%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 820/2000, Train Loss: 3.2620003457929267, Train Accuracy: 93.80327868852459%, Test Loss: 3.473021782361544, Test Accuracy: 72.61538461538461%\n",
      "Epoch 821/2000, Train Loss: 3.261999380393106, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730538313205424, Test Accuracy: 72.6923076923077%\n",
      "Epoch 822/2000, Train Loss: 3.261998293829746, Train Accuracy: 93.80327868852459%, Test Loss: 3.473014134627122, Test Accuracy: 72.6923076923077%\n",
      "Epoch 823/2000, Train Loss: 3.261997082194344, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730170323298526, Test Accuracy: 72.76923076923077%\n",
      "Epoch 824/2000, Train Loss: 3.2619976176590217, Train Accuracy: 93.80327868852459%, Test Loss: 3.472892220203693, Test Accuracy: 72.76923076923077%\n",
      "Epoch 825/2000, Train Loss: 3.261995776754911, Train Accuracy: 93.80327868852459%, Test Loss: 3.4735267162323, Test Accuracy: 72.61538461538461%\n",
      "Epoch 826/2000, Train Loss: 3.2619956673168744, Train Accuracy: 93.80327868852459%, Test Loss: 3.4731358748215895, Test Accuracy: 72.76923076923077%\n",
      "Epoch 827/2000, Train Loss: 3.2620001699103685, Train Accuracy: 93.80327868852459%, Test Loss: 3.4733906342433047, Test Accuracy: 72.61538461538461%\n",
      "Epoch 828/2000, Train Loss: 3.262015612398992, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729147232495823, Test Accuracy: 72.61538461538461%\n",
      "Epoch 829/2000, Train Loss: 3.261996093343516, Train Accuracy: 93.80327868852459%, Test Loss: 3.473147383103004, Test Accuracy: 72.61538461538461%\n",
      "Epoch 830/2000, Train Loss: 3.261998977817473, Train Accuracy: 93.80327868852459%, Test Loss: 3.473189271413363, Test Accuracy: 72.6923076923077%\n",
      "Epoch 831/2000, Train Loss: 3.2620012760162354, Train Accuracy: 93.80327868852459%, Test Loss: 3.473086558855497, Test Accuracy: 72.6923076923077%\n",
      "Epoch 832/2000, Train Loss: 3.261999376484605, Train Accuracy: 93.80327868852459%, Test Loss: 3.473126851595365, Test Accuracy: 72.6923076923077%\n",
      "Epoch 833/2000, Train Loss: 3.2619948348060985, Train Accuracy: 93.80327868852459%, Test Loss: 3.473013786169199, Test Accuracy: 72.6923076923077%\n",
      "Epoch 834/2000, Train Loss: 3.2619939163082936, Train Accuracy: 93.80327868852459%, Test Loss: 3.473192939391503, Test Accuracy: 72.76923076923077%\n",
      "Epoch 835/2000, Train Loss: 3.2619935606346755, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730051389107337, Test Accuracy: 72.76923076923077%\n",
      "Epoch 836/2000, Train Loss: 3.261993650530205, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730640924893894, Test Accuracy: 72.6923076923077%\n",
      "Epoch 837/2000, Train Loss: 3.2619957611209056, Train Accuracy: 93.80327868852459%, Test Loss: 3.473024377456078, Test Accuracy: 72.6923076923077%\n",
      "Epoch 838/2000, Train Loss: 3.262007517892806, Train Accuracy: 93.80327868852459%, Test Loss: 3.4731461635002723, Test Accuracy: 72.76923076923077%\n",
      "Epoch 839/2000, Train Loss: 3.262021264091867, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730921433522153, Test Accuracy: 72.6923076923077%\n",
      "Epoch 840/2000, Train Loss: 3.262561059388958, Train Accuracy: 93.73770491803279%, Test Loss: 3.4727360560343814, Test Accuracy: 72.76923076923077%\n",
      "Epoch 841/2000, Train Loss: 3.262319209145718, Train Accuracy: 93.77049180327869%, Test Loss: 3.4726695739305935, Test Accuracy: 72.84615384615384%\n",
      "Epoch 842/2000, Train Loss: 3.2623067683860905, Train Accuracy: 93.77049180327869%, Test Loss: 3.4730523274495053, Test Accuracy: 72.6923076923077%\n",
      "Epoch 843/2000, Train Loss: 3.262256172836804, Train Accuracy: 93.77049180327869%, Test Loss: 3.47290248137254, Test Accuracy: 72.76923076923077%\n",
      "Epoch 844/2000, Train Loss: 3.2620006545645293, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730020486391506, Test Accuracy: 72.76923076923077%\n",
      "Epoch 845/2000, Train Loss: 3.2619919972341567, Train Accuracy: 93.80327868852459%, Test Loss: 3.472838355944707, Test Accuracy: 72.92307692307692%\n",
      "Epoch 846/2000, Train Loss: 3.261990781690254, Train Accuracy: 93.80327868852459%, Test Loss: 3.472994245015658, Test Accuracy: 72.76923076923077%\n",
      "Epoch 847/2000, Train Loss: 3.261989945270976, Train Accuracy: 93.80327868852459%, Test Loss: 3.472725877395043, Test Accuracy: 72.84615384615384%\n",
      "Epoch 848/2000, Train Loss: 3.2619911764488845, Train Accuracy: 93.80327868852459%, Test Loss: 3.472927441963783, Test Accuracy: 72.84615384615384%\n",
      "Epoch 849/2000, Train Loss: 3.2619892886427584, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728130927452674, Test Accuracy: 72.84615384615384%\n",
      "Epoch 850/2000, Train Loss: 3.261988538210509, Train Accuracy: 93.80327868852459%, Test Loss: 3.473028834049518, Test Accuracy: 72.84615384615384%\n",
      "Epoch 851/2000, Train Loss: 3.261989401989296, Train Accuracy: 93.80327868852459%, Test Loss: 3.472842757518475, Test Accuracy: 72.76923076923077%\n",
      "Epoch 852/2000, Train Loss: 3.2619879480268135, Train Accuracy: 93.80327868852459%, Test Loss: 3.472837310570937, Test Accuracy: 72.76923076923077%\n",
      "Epoch 853/2000, Train Loss: 3.2619871858690606, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728975846217227, Test Accuracy: 72.84615384615384%\n",
      "Epoch 854/2000, Train Loss: 3.26198682237844, Train Accuracy: 93.80327868852459%, Test Loss: 3.472796770242544, Test Accuracy: 72.76923076923077%\n",
      "Epoch 855/2000, Train Loss: 3.261988374053455, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728227395277758, Test Accuracy: 72.76923076923077%\n",
      "Epoch 856/2000, Train Loss: 3.2619873930196293, Train Accuracy: 93.80327868852459%, Test Loss: 3.472838511833778, Test Accuracy: 72.76923076923077%\n",
      "Epoch 857/2000, Train Loss: 3.2619867754764242, Train Accuracy: 93.80327868852459%, Test Loss: 3.472796834432162, Test Accuracy: 72.76923076923077%\n",
      "Epoch 858/2000, Train Loss: 3.2619871663265543, Train Accuracy: 93.80327868852459%, Test Loss: 3.472805307461665, Test Accuracy: 72.76923076923077%\n",
      "Epoch 859/2000, Train Loss: 3.2619867012148998, Train Accuracy: 93.80327868852459%, Test Loss: 3.472996610861558, Test Accuracy: 72.76923076923077%\n",
      "Epoch 860/2000, Train Loss: 3.2619873070326006, Train Accuracy: 93.80327868852459%, Test Loss: 3.472782767735995, Test Accuracy: 72.76923076923077%\n",
      "Epoch 861/2000, Train Loss: 3.261985927331643, Train Accuracy: 93.80327868852459%, Test Loss: 3.472850808730492, Test Accuracy: 72.76923076923077%\n",
      "Epoch 862/2000, Train Loss: 3.2619858687041234, Train Accuracy: 93.80327868852459%, Test Loss: 3.4726655299846945, Test Accuracy: 72.76923076923077%\n",
      "Epoch 863/2000, Train Loss: 3.2619868810059596, Train Accuracy: 93.80327868852459%, Test Loss: 3.472808654491718, Test Accuracy: 72.84615384615384%\n",
      "Epoch 864/2000, Train Loss: 3.2619862361032457, Train Accuracy: 93.80327868852459%, Test Loss: 3.47289502620697, Test Accuracy: 72.76923076923077%\n",
      "Epoch 865/2000, Train Loss: 3.2619873187581048, Train Accuracy: 93.80327868852459%, Test Loss: 3.472858924132127, Test Accuracy: 72.76923076923077%\n",
      "Epoch 866/2000, Train Loss: 3.2619870021694997, Train Accuracy: 93.80327868852459%, Test Loss: 3.472736652080829, Test Accuracy: 72.76923076923077%\n",
      "Epoch 867/2000, Train Loss: 3.2619861579332197, Train Accuracy: 93.80327868852459%, Test Loss: 3.473005725787236, Test Accuracy: 72.76923076923077%\n",
      "Epoch 868/2000, Train Loss: 3.2619844342841477, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728037394010105, Test Accuracy: 72.76923076923077%\n",
      "Epoch 869/2000, Train Loss: 3.261985673279059, Train Accuracy: 93.80327868852459%, Test Loss: 3.472909239622263, Test Accuracy: 72.76923076923077%\n",
      "Epoch 870/2000, Train Loss: 3.2619870451630137, Train Accuracy: 93.80327868852459%, Test Loss: 3.47294570849492, Test Accuracy: 72.76923076923077%\n",
      "Epoch 871/2000, Train Loss: 3.26198513390588, Train Accuracy: 93.80327868852459%, Test Loss: 3.47299796801347, Test Accuracy: 72.76923076923077%\n",
      "Epoch 872/2000, Train Loss: 3.261985915606139, Train Accuracy: 93.80327868852459%, Test Loss: 3.472702769132761, Test Accuracy: 72.84615384615384%\n",
      "Epoch 873/2000, Train Loss: 3.2619851808078955, Train Accuracy: 93.80327868852459%, Test Loss: 3.4731060449893656, Test Accuracy: 72.76923076923077%\n",
      "Epoch 874/2000, Train Loss: 3.2619842857610983, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728407768102794, Test Accuracy: 72.84615384615384%\n",
      "Epoch 875/2000, Train Loss: 3.261985528664511, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730096872036276, Test Accuracy: 72.76923076923077%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 876/2000, Train Loss: 3.261985915606139, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729873217069187, Test Accuracy: 72.76923076923077%\n",
      "Epoch 877/2000, Train Loss: 3.261985028376345, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730145931243896, Test Accuracy: 72.6923076923077%\n",
      "Epoch 878/2000, Train Loss: 3.261983691668901, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730804975216207, Test Accuracy: 72.6923076923077%\n",
      "Epoch 879/2000, Train Loss: 3.2619835353288495, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730460001872134, Test Accuracy: 72.6923076923077%\n",
      "Epoch 880/2000, Train Loss: 3.2619824135889774, Train Accuracy: 93.80327868852459%, Test Loss: 3.4731304462139425, Test Accuracy: 72.6923076923077%\n",
      "Epoch 881/2000, Train Loss: 3.2619820970003723, Train Accuracy: 93.80327868852459%, Test Loss: 3.47299810556265, Test Accuracy: 72.6923076923077%\n",
      "Epoch 882/2000, Train Loss: 3.261982065732362, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728253071124735, Test Accuracy: 72.76923076923077%\n",
      "Epoch 883/2000, Train Loss: 3.261986005501669, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729060118014994, Test Accuracy: 72.6923076923077%\n",
      "Epoch 884/2000, Train Loss: 3.2619841606890567, Train Accuracy: 93.80327868852459%, Test Loss: 3.47312122124892, Test Accuracy: 72.61538461538461%\n",
      "Epoch 885/2000, Train Loss: 3.261982765354094, Train Accuracy: 93.80327868852459%, Test Loss: 3.472790938157302, Test Accuracy: 72.6923076923077%\n",
      "Epoch 886/2000, Train Loss: 3.261984414741641, Train Accuracy: 93.80327868852459%, Test Loss: 3.473205190438491, Test Accuracy: 72.6923076923077%\n",
      "Epoch 887/2000, Train Loss: 3.26198160843771, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730212688446045, Test Accuracy: 72.61538461538461%\n",
      "Epoch 888/2000, Train Loss: 3.2619813231171153, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730665408647976, Test Accuracy: 72.76923076923077%\n",
      "Epoch 889/2000, Train Loss: 3.261982042281354, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730995251582217, Test Accuracy: 72.6923076923077%\n",
      "Epoch 890/2000, Train Loss: 3.261984543722184, Train Accuracy: 93.80327868852459%, Test Loss: 3.4731321885035586, Test Accuracy: 72.76923076923077%\n",
      "Epoch 891/2000, Train Loss: 3.261981905483809, Train Accuracy: 93.80327868852459%, Test Loss: 3.4732860510165873, Test Accuracy: 72.6923076923077%\n",
      "Epoch 892/2000, Train Loss: 3.2619805179658483, Train Accuracy: 93.80327868852459%, Test Loss: 3.472927423623892, Test Accuracy: 72.76923076923077%\n",
      "Epoch 893/2000, Train Loss: 3.261981045613523, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729163371599636, Test Accuracy: 72.84615384615384%\n",
      "Epoch 894/2000, Train Loss: 3.2619792047094127, Train Accuracy: 93.80327868852459%, Test Loss: 3.4728822799829335, Test Accuracy: 72.76923076923077%\n",
      "Epoch 895/2000, Train Loss: 3.261983976989496, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729386751468363, Test Accuracy: 72.61538461538461%\n",
      "Epoch 896/2000, Train Loss: 3.2619822572489254, Train Accuracy: 93.80327868852459%, Test Loss: 3.4732514161330004, Test Accuracy: 72.6923076923077%\n",
      "Epoch 897/2000, Train Loss: 3.26198471178774, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730203335101786, Test Accuracy: 72.6923076923077%\n",
      "Epoch 898/2000, Train Loss: 3.2619870490715153, Train Accuracy: 93.80327868852459%, Test Loss: 3.4732611179351807, Test Accuracy: 72.6923076923077%\n",
      "Epoch 899/2000, Train Loss: 3.2619792555199294, Train Accuracy: 93.80327868852459%, Test Loss: 3.473255359209501, Test Accuracy: 72.61538461538461%\n",
      "Epoch 900/2000, Train Loss: 3.2619908872197887, Train Accuracy: 93.80327868852459%, Test Loss: 3.472844490638146, Test Accuracy: 72.76923076923077%\n",
      "Epoch 901/2000, Train Loss: 3.262333842574573, Train Accuracy: 93.77049180327869%, Test Loss: 3.472913072659419, Test Accuracy: 72.76923076923077%\n",
      "Epoch 902/2000, Train Loss: 3.2620224952697754, Train Accuracy: 93.80327868852459%, Test Loss: 3.473254533914419, Test Accuracy: 72.6923076923077%\n",
      "Epoch 903/2000, Train Loss: 3.262003910346109, Train Accuracy: 93.8360655737705%, Test Loss: 3.473312267890343, Test Accuracy: 72.76923076923077%\n",
      "Epoch 904/2000, Train Loss: 3.2623176496537005, Train Accuracy: 93.77049180327869%, Test Loss: 3.4730592599281898, Test Accuracy: 72.6923076923077%\n",
      "Epoch 905/2000, Train Loss: 3.262310266494751, Train Accuracy: 93.77049180327869%, Test Loss: 3.4729457176648655, Test Accuracy: 72.76923076923077%\n",
      "Epoch 906/2000, Train Loss: 3.2623064283464775, Train Accuracy: 93.77049180327869%, Test Loss: 3.4729208946228027, Test Accuracy: 72.6923076923077%\n",
      "Epoch 907/2000, Train Loss: 3.262303657219058, Train Accuracy: 93.77049180327869%, Test Loss: 3.4729842130954447, Test Accuracy: 72.76923076923077%\n",
      "Epoch 908/2000, Train Loss: 3.2623018397659553, Train Accuracy: 93.77049180327869%, Test Loss: 3.472944342173063, Test Accuracy: 72.6923076923077%\n",
      "Epoch 909/2000, Train Loss: 3.2622974544275003, Train Accuracy: 93.77049180327869%, Test Loss: 3.4730710891576915, Test Accuracy: 72.6923076923077%\n",
      "Epoch 910/2000, Train Loss: 3.2622793619749975, Train Accuracy: 93.77049180327869%, Test Loss: 3.473068145605234, Test Accuracy: 72.6923076923077%\n",
      "Epoch 911/2000, Train Loss: 3.261979607285046, Train Accuracy: 93.80327868852459%, Test Loss: 3.4732910853165846, Test Accuracy: 72.61538461538461%\n",
      "Epoch 912/2000, Train Loss: 3.261977590498377, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730733908139744, Test Accuracy: 72.6923076923077%\n",
      "Epoch 913/2000, Train Loss: 3.261976374954474, Train Accuracy: 93.80327868852459%, Test Loss: 3.4731328120598426, Test Accuracy: 72.6923076923077%\n",
      "Epoch 914/2000, Train Loss: 3.2619758082217856, Train Accuracy: 93.80327868852459%, Test Loss: 3.473059058189392, Test Accuracy: 72.6923076923077%\n",
      "Epoch 915/2000, Train Loss: 3.261975667515739, Train Accuracy: 93.80327868852459%, Test Loss: 3.4732603201499352, Test Accuracy: 72.61538461538461%\n",
      "Epoch 916/2000, Train Loss: 3.261975616705222, Train Accuracy: 93.80327868852459%, Test Loss: 3.4731806700046244, Test Accuracy: 72.6923076923077%\n",
      "Epoch 917/2000, Train Loss: 3.261976417947988, Train Accuracy: 93.80327868852459%, Test Loss: 3.4732601459209738, Test Accuracy: 72.6923076923077%\n",
      "Epoch 918/2000, Train Loss: 3.2619759762873417, Train Accuracy: 93.80327868852459%, Test Loss: 3.4730579944757314, Test Accuracy: 72.6923076923077%\n",
      "Epoch 919/2000, Train Loss: 3.261975577620209, Train Accuracy: 93.80327868852459%, Test Loss: 3.473260998725891, Test Accuracy: 72.61538461538461%\n",
      "Epoch 920/2000, Train Loss: 3.2619748037369525, Train Accuracy: 93.80327868852459%, Test Loss: 3.47302691753094, Test Accuracy: 72.6923076923077%\n",
      "Epoch 921/2000, Train Loss: 3.2619756831497444, Train Accuracy: 93.80327868852459%, Test Loss: 3.473121560536898, Test Accuracy: 72.61538461538461%\n",
      "Epoch 922/2000, Train Loss: 3.2619750382470305, Train Accuracy: 93.80327868852459%, Test Loss: 3.473133371426509, Test Accuracy: 72.76923076923077%\n",
      "Epoch 923/2000, Train Loss: 3.2619750851490457, Train Accuracy: 93.80327868852459%, Test Loss: 3.4729549426298876, Test Accuracy: 72.6923076923077%\n",
      "Epoch 924/2000, Train Loss: 3.2619535571239036, Train Accuracy: 93.80327868852459%, Test Loss: 3.473532584997324, Test Accuracy: 72.6923076923077%\n",
      "Epoch 925/2000, Train Loss: 3.2590242565655316, Train Accuracy: 94.19672131147541%, Test Loss: 3.470488291520339, Test Accuracy: 73.07692307692308%\n",
      "Saved the best model with validation loss:  3.470488291520339\n",
      "Epoch 926/2000, Train Loss: 3.2558682824744554, Train Accuracy: 94.52459016393442%, Test Loss: 3.46853755070613, Test Accuracy: 73.3076923076923%\n",
      "Saved the best model with validation loss:  3.46853755070613\n",
      "Epoch 927/2000, Train Loss: 3.2549426516548534, Train Accuracy: 94.55737704918033%, Test Loss: 3.468448198758639, Test Accuracy: 73.23076923076923%\n",
      "Epoch 928/2000, Train Loss: 3.254668939309042, Train Accuracy: 94.55737704918033%, Test Loss: 3.4688837161430945, Test Accuracy: 73.15384615384616%\n",
      "Epoch 929/2000, Train Loss: 3.2545147098478724, Train Accuracy: 94.59016393442623%, Test Loss: 3.4689017809354343, Test Accuracy: 73.15384615384616%\n",
      "Epoch 930/2000, Train Loss: 3.2543593547383294, Train Accuracy: 94.59016393442623%, Test Loss: 3.468269265615023, Test Accuracy: 73.23076923076923%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 931/2000, Train Loss: 3.253975426564451, Train Accuracy: 94.65573770491804%, Test Loss: 3.4681093325981727, Test Accuracy: 73.23076923076923%\n",
      "Epoch 932/2000, Train Loss: 3.25358799246491, Train Accuracy: 94.68852459016394%, Test Loss: 3.4676439120219302, Test Accuracy: 73.3076923076923%\n",
      "Epoch 933/2000, Train Loss: 3.253442150647523, Train Accuracy: 94.68852459016394%, Test Loss: 3.46759924521813, Test Accuracy: 73.3076923076923%\n",
      "Epoch 934/2000, Train Loss: 3.2533987311066173, Train Accuracy: 94.68852459016394%, Test Loss: 3.467535220659696, Test Accuracy: 73.3076923076923%\n",
      "Saved the best model with validation loss:  3.467535220659696\n",
      "Epoch 935/2000, Train Loss: 3.253364629432803, Train Accuracy: 94.68852459016394%, Test Loss: 3.4675490947870107, Test Accuracy: 73.3076923076923%\n",
      "Epoch 936/2000, Train Loss: 3.253277102454764, Train Accuracy: 94.72131147540983%, Test Loss: 3.467424649458665, Test Accuracy: 73.38461538461539%\n",
      "Epoch 937/2000, Train Loss: 3.2531064651051507, Train Accuracy: 94.72131147540983%, Test Loss: 3.467279947721041, Test Accuracy: 73.38461538461539%\n",
      "Epoch 938/2000, Train Loss: 3.2530536925206417, Train Accuracy: 94.72131147540983%, Test Loss: 3.4671798027478733, Test Accuracy: 73.38461538461539%\n",
      "Epoch 939/2000, Train Loss: 3.2530368976905697, Train Accuracy: 94.72131147540983%, Test Loss: 3.467095815218412, Test Accuracy: 73.38461538461539%\n",
      "Epoch 940/2000, Train Loss: 3.2530199074354327, Train Accuracy: 94.72131147540983%, Test Loss: 3.4670295531933126, Test Accuracy: 73.38461538461539%\n",
      "Epoch 941/2000, Train Loss: 3.253008885461776, Train Accuracy: 94.72131147540983%, Test Loss: 3.4669784490878763, Test Accuracy: 73.38461538461539%\n",
      "Epoch 942/2000, Train Loss: 3.2530007948640915, Train Accuracy: 94.72131147540983%, Test Loss: 3.466935781332163, Test Accuracy: 73.38461538461539%\n",
      "Epoch 943/2000, Train Loss: 3.252993673574729, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668332980229306, Test Accuracy: 73.46153846153847%\n",
      "Epoch 944/2000, Train Loss: 3.2529898471519596, Train Accuracy: 94.72131147540983%, Test Loss: 3.46680996968196, Test Accuracy: 73.46153846153847%\n",
      "Epoch 945/2000, Train Loss: 3.252980423755333, Train Accuracy: 94.72131147540983%, Test Loss: 3.466808740909283, Test Accuracy: 73.38461538461539%\n",
      "Epoch 946/2000, Train Loss: 3.2529764527180154, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668522339600782, Test Accuracy: 73.38461538461539%\n",
      "Epoch 947/2000, Train Loss: 3.2529718993140047, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668356638688307, Test Accuracy: 73.38461538461539%\n",
      "Epoch 948/2000, Train Loss: 3.252967646864594, Train Accuracy: 94.72131147540983%, Test Loss: 3.466795591207651, Test Accuracy: 73.46153846153847%\n",
      "Epoch 949/2000, Train Loss: 3.252975577213725, Train Accuracy: 94.72131147540983%, Test Loss: 3.466836855961726, Test Accuracy: 73.3076923076923%\n",
      "Epoch 950/2000, Train Loss: 3.253746404022467, Train Accuracy: 94.65573770491804%, Test Loss: 3.4669773945441613, Test Accuracy: 73.3076923076923%\n",
      "Epoch 951/2000, Train Loss: 3.2535745667629556, Train Accuracy: 94.65573770491804%, Test Loss: 3.4668474930983324, Test Accuracy: 73.3076923076923%\n",
      "Epoch 952/2000, Train Loss: 3.252954490849229, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668592581382165, Test Accuracy: 73.38461538461539%\n",
      "Epoch 953/2000, Train Loss: 3.2529495192355795, Train Accuracy: 94.72131147540983%, Test Loss: 3.466817012199989, Test Accuracy: 73.3076923076923%\n",
      "Epoch 954/2000, Train Loss: 3.2529481942536402, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668510143573465, Test Accuracy: 73.3076923076923%\n",
      "Epoch 955/2000, Train Loss: 3.2529450635441015, Train Accuracy: 94.72131147540983%, Test Loss: 3.466845310651339, Test Accuracy: 73.38461538461539%\n",
      "Epoch 956/2000, Train Loss: 3.2529430858424453, Train Accuracy: 94.72131147540983%, Test Loss: 3.466821166185232, Test Accuracy: 73.3076923076923%\n",
      "Epoch 957/2000, Train Loss: 3.252941569343942, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668069986196666, Test Accuracy: 73.38461538461539%\n",
      "Epoch 958/2000, Train Loss: 3.25293824711784, Train Accuracy: 94.72131147540983%, Test Loss: 3.4667745370131273, Test Accuracy: 73.38461538461539%\n",
      "Epoch 959/2000, Train Loss: 3.252936417939233, Train Accuracy: 94.72131147540983%, Test Loss: 3.466859918374282, Test Accuracy: 73.3076923076923%\n",
      "Epoch 960/2000, Train Loss: 3.252933255961684, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668375987272997, Test Accuracy: 73.38461538461539%\n",
      "Epoch 961/2000, Train Loss: 3.2529318176332067, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668416793529806, Test Accuracy: 73.3076923076923%\n",
      "Epoch 962/2000, Train Loss: 3.2529300978926363, Train Accuracy: 94.72131147540983%, Test Loss: 3.466827465937688, Test Accuracy: 73.3076923076923%\n",
      "Epoch 963/2000, Train Loss: 3.252927959942427, Train Accuracy: 94.72131147540983%, Test Loss: 3.466833839049706, Test Accuracy: 73.38461538461539%\n",
      "Epoch 964/2000, Train Loss: 3.252927537824287, Train Accuracy: 94.72131147540983%, Test Loss: 3.4669670783556423, Test Accuracy: 73.23076923076923%\n",
      "Epoch 965/2000, Train Loss: 3.252926803026043, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668214046038113, Test Accuracy: 73.38461538461539%\n",
      "Epoch 966/2000, Train Loss: 3.2529240983431458, Train Accuracy: 94.72131147540983%, Test Loss: 3.4667958479661207, Test Accuracy: 73.3076923076923%\n",
      "Epoch 967/2000, Train Loss: 3.2529235433359616, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668584878628073, Test Accuracy: 73.3076923076923%\n",
      "Epoch 968/2000, Train Loss: 3.2529219447589313, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668512160961447, Test Accuracy: 73.23076923076923%\n",
      "Epoch 969/2000, Train Loss: 3.252920006142288, Train Accuracy: 94.72131147540983%, Test Loss: 3.466901476566608, Test Accuracy: 73.23076923076923%\n",
      "Epoch 970/2000, Train Loss: 3.25291965046867, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668486760212827, Test Accuracy: 73.23076923076923%\n",
      "Epoch 971/2000, Train Loss: 3.2529184544672733, Train Accuracy: 94.72131147540983%, Test Loss: 3.466865163583022, Test Accuracy: 73.23076923076923%\n",
      "Epoch 972/2000, Train Loss: 3.2529179541791073, Train Accuracy: 94.72131147540983%, Test Loss: 3.46686524611253, Test Accuracy: 73.23076923076923%\n",
      "Epoch 973/2000, Train Loss: 3.252916949694274, Train Accuracy: 94.72131147540983%, Test Loss: 3.467052799004775, Test Accuracy: 73.23076923076923%\n",
      "Epoch 974/2000, Train Loss: 3.252915448829776, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668702529026914, Test Accuracy: 73.23076923076923%\n",
      "Epoch 975/2000, Train Loss: 3.252914252828379, Train Accuracy: 94.72131147540983%, Test Loss: 3.466882503949679, Test Accuracy: 73.38461538461539%\n",
      "Epoch 976/2000, Train Loss: 3.2529145068809635, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668255860988912, Test Accuracy: 73.23076923076923%\n",
      "Epoch 977/2000, Train Loss: 3.2529144834299557, Train Accuracy: 94.72131147540983%, Test Loss: 3.4669631169392514, Test Accuracy: 73.3076923076923%\n",
      "Epoch 978/2000, Train Loss: 3.2529124666432865, Train Accuracy: 94.72131147540983%, Test Loss: 3.4668669517223654, Test Accuracy: 73.3076923076923%\n",
      "Epoch 979/2000, Train Loss: 3.2529120718846554, Train Accuracy: 94.72131147540983%, Test Loss: 3.4669254284638624, Test Accuracy: 73.23076923076923%\n",
      "Epoch 980/2000, Train Loss: 3.2529110439488145, Train Accuracy: 94.72131147540983%, Test Loss: 3.466983061570388, Test Accuracy: 73.3076923076923%\n",
      "Epoch 981/2000, Train Loss: 3.2529107156347057, Train Accuracy: 94.72131147540983%, Test Loss: 3.466853040915269, Test Accuracy: 73.3076923076923%\n",
      "Epoch 982/2000, Train Loss: 3.252909128783179, Train Accuracy: 94.72131147540983%, Test Loss: 3.4670113508517924, Test Accuracy: 73.3076923076923%\n",
      "Epoch 983/2000, Train Loss: 3.2529016791797076, Train Accuracy: 94.72131147540983%, Test Loss: 3.466976816837604, Test Accuracy: 73.23076923076923%\n",
      "Epoch 984/2000, Train Loss: 3.252417185267464, Train Accuracy: 94.81967213114754%, Test Loss: 3.4665675530066857, Test Accuracy: 73.53846153846153%\n",
      "Epoch 985/2000, Train Loss: 3.250085181877261, Train Accuracy: 95.08196721311475%, Test Loss: 3.465172978547903, Test Accuracy: 73.6923076923077%\n",
      "Saved the best model with validation loss:  3.465172978547903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986/2000, Train Loss: 3.249007107781582, Train Accuracy: 95.21311475409836%, Test Loss: 3.465272775063148, Test Accuracy: 73.46153846153847%\n",
      "Epoch 987/2000, Train Loss: 3.2481178612005515, Train Accuracy: 95.24590163934427%, Test Loss: 3.464074740043053, Test Accuracy: 73.6923076923077%\n",
      "Saved the best model with validation loss:  3.464074740043053\n",
      "Epoch 988/2000, Train Loss: 3.247685760748191, Train Accuracy: 95.31147540983606%, Test Loss: 3.4642558372937717, Test Accuracy: 73.6923076923077%\n",
      "Epoch 989/2000, Train Loss: 3.247269712510656, Train Accuracy: 95.34426229508196%, Test Loss: 3.4644934764275184, Test Accuracy: 73.61538461538461%\n",
      "Epoch 990/2000, Train Loss: 3.246882680986748, Train Accuracy: 95.37704918032787%, Test Loss: 3.464504370322594, Test Accuracy: 73.53846153846153%\n",
      "Epoch 991/2000, Train Loss: 3.246690613324525, Train Accuracy: 95.37704918032787%, Test Loss: 3.46482331936176, Test Accuracy: 73.61538461538461%\n",
      "Epoch 992/2000, Train Loss: 3.2466235590762778, Train Accuracy: 95.37704918032787%, Test Loss: 3.4645194182029138, Test Accuracy: 73.61538461538461%\n",
      "Epoch 993/2000, Train Loss: 3.2465876634003688, Train Accuracy: 95.37704918032787%, Test Loss: 3.46470675101647, Test Accuracy: 73.53846153846153%\n",
      "Epoch 994/2000, Train Loss: 3.246565224694424, Train Accuracy: 95.37704918032787%, Test Loss: 3.4647141236525316, Test Accuracy: 73.53846153846153%\n",
      "Epoch 995/2000, Train Loss: 3.2465502277749483, Train Accuracy: 95.37704918032787%, Test Loss: 3.464557931973384, Test Accuracy: 73.53846153846153%\n",
      "Epoch 996/2000, Train Loss: 3.2465372984526586, Train Accuracy: 95.37704918032787%, Test Loss: 3.464664789346548, Test Accuracy: 73.53846153846153%\n",
      "Epoch 997/2000, Train Loss: 3.246526409368046, Train Accuracy: 95.37704918032787%, Test Loss: 3.4644834261674147, Test Accuracy: 73.53846153846153%\n",
      "Epoch 998/2000, Train Loss: 3.246513421418237, Train Accuracy: 95.37704918032787%, Test Loss: 3.4644128542680006, Test Accuracy: 73.61538461538461%\n",
      "Epoch 999/2000, Train Loss: 3.2465058701937317, Train Accuracy: 95.37704918032787%, Test Loss: 3.4645656989170956, Test Accuracy: 73.46153846153847%\n",
      "Epoch 1000/2000, Train Loss: 3.2464986902768493, Train Accuracy: 95.37704918032787%, Test Loss: 3.464380530210642, Test Accuracy: 73.53846153846153%\n",
      "Epoch 1001/2000, Train Loss: 3.2464940704283167, Train Accuracy: 95.37704918032787%, Test Loss: 3.464494686860305, Test Accuracy: 73.46153846153847%\n",
      "Epoch 1002/2000, Train Loss: 3.2464865387463178, Train Accuracy: 95.37704918032787%, Test Loss: 3.464398035636315, Test Accuracy: 73.61538461538461%\n",
      "Epoch 1003/2000, Train Loss: 3.2464824621794652, Train Accuracy: 95.37704918032787%, Test Loss: 3.464455953011146, Test Accuracy: 73.53846153846153%\n",
      "Epoch 1004/2000, Train Loss: 3.2464795894310123, Train Accuracy: 95.37704918032787%, Test Loss: 3.4643368537609396, Test Accuracy: 73.53846153846153%\n",
      "Epoch 1005/2000, Train Loss: 3.2464774632063067, Train Accuracy: 95.37704918032787%, Test Loss: 3.46443995145651, Test Accuracy: 73.61538461538461%\n",
      "Epoch 1006/2000, Train Loss: 3.2464662418990837, Train Accuracy: 95.37704918032787%, Test Loss: 3.4644147707865787, Test Accuracy: 73.61538461538461%\n",
      "Epoch 1007/2000, Train Loss: 3.2427218781142937, Train Accuracy: 95.80327868852459%, Test Loss: 3.4558464013613186, Test Accuracy: 74.46153846153847%\n",
      "Saved the best model with validation loss:  3.4558464013613186\n",
      "Epoch 1008/2000, Train Loss: 3.2376568004733226, Train Accuracy: 96.32786885245902%, Test Loss: 3.45388270341433, Test Accuracy: 74.76923076923077%\n",
      "Saved the best model with validation loss:  3.45388270341433\n",
      "Epoch 1009/2000, Train Loss: 3.2373644758443363, Train Accuracy: 96.32786885245902%, Test Loss: 3.4540136410639835, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1010/2000, Train Loss: 3.236769871633561, Train Accuracy: 96.39344262295081%, Test Loss: 3.4542977809906006, Test Accuracy: 74.53846153846153%\n",
      "Epoch 1011/2000, Train Loss: 3.236192253769421, Train Accuracy: 96.42622950819673%, Test Loss: 3.4542016157737145, Test Accuracy: 74.53846153846153%\n",
      "Epoch 1012/2000, Train Loss: 3.2360360348810917, Train Accuracy: 96.42622950819673%, Test Loss: 3.4541803231606116, Test Accuracy: 74.53846153846153%\n",
      "Epoch 1013/2000, Train Loss: 3.235861023918527, Train Accuracy: 96.45901639344262%, Test Loss: 3.454010294033931, Test Accuracy: 74.61538461538461%\n",
      "Epoch 1014/2000, Train Loss: 3.2358087359881793, Train Accuracy: 96.45901639344262%, Test Loss: 3.4539128725345316, Test Accuracy: 74.53846153846153%\n",
      "Epoch 1015/2000, Train Loss: 3.2357730435543375, Train Accuracy: 96.45901639344262%, Test Loss: 3.4539613081858707, Test Accuracy: 74.61538461538461%\n",
      "Epoch 1016/2000, Train Loss: 3.23573471288212, Train Accuracy: 96.45901639344262%, Test Loss: 3.4539892031596255, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1017/2000, Train Loss: 3.235660443540479, Train Accuracy: 96.49180327868852%, Test Loss: 3.4538026406214786, Test Accuracy: 74.61538461538461%\n",
      "Epoch 1018/2000, Train Loss: 3.2355500479213526, Train Accuracy: 96.49180327868852%, Test Loss: 3.4536599471018863, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1019/2000, Train Loss: 3.2355002926998453, Train Accuracy: 96.49180327868852%, Test Loss: 3.4531792952464175, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1020/2000, Train Loss: 3.2353970379125876, Train Accuracy: 96.49180327868852%, Test Loss: 3.452831690128033, Test Accuracy: 74.76923076923077%\n",
      "Saved the best model with validation loss:  3.452831690128033\n",
      "Epoch 1021/2000, Train Loss: 3.2352477370715533, Train Accuracy: 96.52459016393442%, Test Loss: 3.45257960833036, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1022/2000, Train Loss: 3.235192533399238, Train Accuracy: 96.52459016393442%, Test Loss: 3.4524069932790904, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1023/2000, Train Loss: 3.235143946819618, Train Accuracy: 96.52459016393442%, Test Loss: 3.4523156606234036, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1024/2000, Train Loss: 3.235123689057397, Train Accuracy: 96.52459016393442%, Test Loss: 3.452233369533832, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1025/2000, Train Loss: 3.235076005341577, Train Accuracy: 96.52459016393442%, Test Loss: 3.452176809310913, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1026/2000, Train Loss: 3.234944941567593, Train Accuracy: 96.55737704918033%, Test Loss: 3.451936024885911, Test Accuracy: 74.84615384615384%\n",
      "Epoch 1027/2000, Train Loss: 3.2348410067011097, Train Accuracy: 96.55737704918033%, Test Loss: 3.4521611287043643, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1028/2000, Train Loss: 3.2348136354665287, Train Accuracy: 96.55737704918033%, Test Loss: 3.4520073303809533, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1029/2000, Train Loss: 3.2347983688604636, Train Accuracy: 96.55737704918033%, Test Loss: 3.4519936671623817, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1030/2000, Train Loss: 3.2347905714003766, Train Accuracy: 96.55737704918033%, Test Loss: 3.4519806458399844, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1031/2000, Train Loss: 3.234784450687346, Train Accuracy: 96.55737704918033%, Test Loss: 3.4520592231016893, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1032/2000, Train Loss: 3.2347792953741354, Train Accuracy: 96.55737704918033%, Test Loss: 3.4519992975088267, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1033/2000, Train Loss: 3.2347743042179795, Train Accuracy: 96.55737704918033%, Test Loss: 3.4520121079224806, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1034/2000, Train Loss: 3.2347704543442024, Train Accuracy: 96.55737704918033%, Test Loss: 3.452035803061265, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1035/2000, Train Loss: 3.2347662136202953, Train Accuracy: 96.55737704918033%, Test Loss: 3.4519813244159403, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1036/2000, Train Loss: 3.2347636730944522, Train Accuracy: 96.55737704918033%, Test Loss: 3.4520805523945737, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1037/2000, Train Loss: 3.2347593112070054, Train Accuracy: 96.55737704918033%, Test Loss: 3.4520310988793006, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1038/2000, Train Loss: 3.234755899085373, Train Accuracy: 96.55737704918033%, Test Loss: 3.452057572511526, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1039/2000, Train Loss: 3.234754398220875, Train Accuracy: 96.55737704918033%, Test Loss: 3.4520464952175436, Test Accuracy: 74.6923076923077%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1040/2000, Train Loss: 3.2347488755085427, Train Accuracy: 96.55737704918033%, Test Loss: 3.452111601829529, Test Accuracy: 74.6923076923077%\n",
      "Epoch 1041/2000, Train Loss: 3.234741214846001, Train Accuracy: 96.55737704918033%, Test Loss: 3.451863481448247, Test Accuracy: 74.76923076923077%\n",
      "Epoch 1042/2000, Train Loss: 3.234642388390713, Train Accuracy: 96.59016393442623%, Test Loss: 3.4510263112875132, Test Accuracy: 74.76923076923077%\n",
      "Saved the best model with validation loss:  3.4510263112875132\n",
      "Epoch 1043/2000, Train Loss: 3.2342054804817577, Train Accuracy: 96.62295081967213%, Test Loss: 3.4499733448028564, Test Accuracy: 74.92307692307692%\n",
      "Saved the best model with validation loss:  3.4499733448028564\n",
      "Epoch 1044/2000, Train Loss: 3.234134044803557, Train Accuracy: 96.62295081967213%, Test Loss: 3.4502889009622426, Test Accuracy: 75.0%\n",
      "Epoch 1045/2000, Train Loss: 3.2341121767388015, Train Accuracy: 96.62295081967213%, Test Loss: 3.4502407495792093, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1046/2000, Train Loss: 3.234103085564785, Train Accuracy: 96.62295081967213%, Test Loss: 3.4502762189278235, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1047/2000, Train Loss: 3.234099524920104, Train Accuracy: 96.62295081967213%, Test Loss: 3.450339152262761, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1048/2000, Train Loss: 3.234096757701186, Train Accuracy: 96.62295081967213%, Test Loss: 3.450272092452416, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1049/2000, Train Loss: 3.2340954366277477, Train Accuracy: 96.62295081967213%, Test Loss: 3.450237329189594, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1050/2000, Train Loss: 3.234092966454928, Train Accuracy: 96.62295081967213%, Test Loss: 3.450226435294518, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1051/2000, Train Loss: 3.234091489041438, Train Accuracy: 96.62295081967213%, Test Loss: 3.4502441057792077, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1052/2000, Train Loss: 3.234089339365725, Train Accuracy: 96.62295081967213%, Test Loss: 3.4502314695945153, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1053/2000, Train Loss: 3.2340884443189277, Train Accuracy: 96.62295081967213%, Test Loss: 3.450213835789607, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1054/2000, Train Loss: 3.234087303036549, Train Accuracy: 96.62295081967213%, Test Loss: 3.45011398425469, Test Accuracy: 75.0%\n",
      "Epoch 1055/2000, Train Loss: 3.2340860210481237, Train Accuracy: 96.62295081967213%, Test Loss: 3.450113158959609, Test Accuracy: 75.0%\n",
      "Epoch 1056/2000, Train Loss: 3.2340849110337553, Train Accuracy: 96.62295081967213%, Test Loss: 3.450066144649799, Test Accuracy: 75.0%\n",
      "Epoch 1057/2000, Train Loss: 3.234084133241997, Train Accuracy: 96.62295081967213%, Test Loss: 3.4501238694557776, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1058/2000, Train Loss: 3.234082569841479, Train Accuracy: 96.62295081967213%, Test Loss: 3.4500272365716786, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1059/2000, Train Loss: 3.23408130739556, Train Accuracy: 96.62295081967213%, Test Loss: 3.450031069608835, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1060/2000, Train Loss: 3.2340803693552487, Train Accuracy: 96.62295081967213%, Test Loss: 3.449939590234023, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1061/2000, Train Loss: 3.2340791303603376, Train Accuracy: 96.62295081967213%, Test Loss: 3.449890219248258, Test Accuracy: 75.0%\n",
      "Epoch 1062/2000, Train Loss: 3.2340787473272106, Train Accuracy: 96.62295081967213%, Test Loss: 3.449574424670293, Test Accuracy: 75.0%\n",
      "Epoch 1063/2000, Train Loss: 3.234076077820825, Train Accuracy: 96.62295081967213%, Test Loss: 3.4500065308350782, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1064/2000, Train Loss: 3.234075937114778, Train Accuracy: 96.62295081967213%, Test Loss: 3.449648517828721, Test Accuracy: 75.0%\n",
      "Epoch 1065/2000, Train Loss: 3.234074233008213, Train Accuracy: 96.62295081967213%, Test Loss: 3.450144786101121, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1066/2000, Train Loss: 3.2340747958323997, Train Accuracy: 96.62295081967213%, Test Loss: 3.4497445730062632, Test Accuracy: 75.0%\n",
      "Epoch 1067/2000, Train Loss: 3.2340731777128626, Train Accuracy: 96.62295081967213%, Test Loss: 3.449945037181561, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1068/2000, Train Loss: 3.2340731581703563, Train Accuracy: 96.62295081967213%, Test Loss: 3.4496964583030114, Test Accuracy: 75.0%\n",
      "Epoch 1069/2000, Train Loss: 3.2340731581703563, Train Accuracy: 96.62295081967213%, Test Loss: 3.4500211844077477, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1070/2000, Train Loss: 3.234072653973689, Train Accuracy: 96.62295081967213%, Test Loss: 3.44960369513585, Test Accuracy: 75.0%\n",
      "Epoch 1071/2000, Train Loss: 3.2340712156452116, Train Accuracy: 96.62295081967213%, Test Loss: 3.4499800296930165, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1072/2000, Train Loss: 3.2340701212648484, Train Accuracy: 96.62295081967213%, Test Loss: 3.449759208239042, Test Accuracy: 75.0%\n",
      "Epoch 1073/2000, Train Loss: 3.234070469121464, Train Accuracy: 96.62295081967213%, Test Loss: 3.450012280390813, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1074/2000, Train Loss: 3.2340706332785185, Train Accuracy: 96.62295081967213%, Test Loss: 3.4496187155063334, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1075/2000, Train Loss: 3.2340706567295263, Train Accuracy: 96.62295081967213%, Test Loss: 3.450020487491901, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1076/2000, Train Loss: 3.2340689135379477, Train Accuracy: 96.62295081967213%, Test Loss: 3.4495743054610033, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1077/2000, Train Loss: 3.234068217824717, Train Accuracy: 96.62295081967213%, Test Loss: 3.450098046889672, Test Accuracy: 75.0%\n",
      "Epoch 1078/2000, Train Loss: 3.2340673266864215, Train Accuracy: 96.62295081967213%, Test Loss: 3.4495817789664636, Test Accuracy: 75.0%\n",
      "Epoch 1079/2000, Train Loss: 3.2340663651951025, Train Accuracy: 96.62295081967213%, Test Loss: 3.4499774804482093, Test Accuracy: 75.0%\n",
      "Epoch 1080/2000, Train Loss: 3.2340651926447133, Train Accuracy: 96.62295081967213%, Test Loss: 3.4497392636079054, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1081/2000, Train Loss: 3.234076710998035, Train Accuracy: 96.62295081967213%, Test Loss: 3.4501933868114767, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1082/2000, Train Loss: 3.234074842734415, Train Accuracy: 96.62295081967213%, Test Loss: 3.449681648841271, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1083/2000, Train Loss: 3.2340646845395447, Train Accuracy: 96.62295081967213%, Test Loss: 3.4497702030035167, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1084/2000, Train Loss: 3.2340644852059786, Train Accuracy: 96.62295081967213%, Test Loss: 3.449713239303002, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1085/2000, Train Loss: 3.234063898930784, Train Accuracy: 96.62295081967213%, Test Loss: 3.4496690401664147, Test Accuracy: 75.0%\n",
      "Epoch 1086/2000, Train Loss: 3.2340637230482256, Train Accuracy: 96.62295081967213%, Test Loss: 3.4496870774489183, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1087/2000, Train Loss: 3.234063148498535, Train Accuracy: 96.62295081967213%, Test Loss: 3.449533838492173, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1088/2000, Train Loss: 3.2340627185633926, Train Accuracy: 96.62295081967213%, Test Loss: 3.4495697755080004, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1089/2000, Train Loss: 3.2340632423025664, Train Accuracy: 96.62295081967213%, Test Loss: 3.4494895109763513, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1090/2000, Train Loss: 3.234063410368122, Train Accuracy: 96.62295081967213%, Test Loss: 3.449625116128188, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1091/2000, Train Loss: 3.2340628084589222, Train Accuracy: 96.62295081967213%, Test Loss: 3.449667050288274, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1092/2000, Train Loss: 3.2340620853861823, Train Accuracy: 96.62295081967213%, Test Loss: 3.4496338092363796, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1093/2000, Train Loss: 3.234061327136931, Train Accuracy: 96.62295081967213%, Test Loss: 3.449646289532001, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1094/2000, Train Loss: 3.234060815123261, Train Accuracy: 96.62295081967213%, Test Loss: 3.449616322150597, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1095/2000, Train Loss: 3.234060776038248, Train Accuracy: 96.62295081967213%, Test Loss: 3.4495441180009108, Test Accuracy: 75.07692307692308%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1096/2000, Train Loss: 3.2340624801448135, Train Accuracy: 96.62295081967213%, Test Loss: 3.4492670939518857, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1097/2000, Train Loss: 3.2340597676449137, Train Accuracy: 96.62295081967213%, Test Loss: 3.449630379676819, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1098/2000, Train Loss: 3.2340602366650693, Train Accuracy: 96.62295081967213%, Test Loss: 3.4493063321480384, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1099/2000, Train Loss: 3.2340606783257155, Train Accuracy: 96.62295081967213%, Test Loss: 3.449990520110497, Test Accuracy: 74.84615384615384%\n",
      "Epoch 1100/2000, Train Loss: 3.2340621791902135, Train Accuracy: 96.62295081967213%, Test Loss: 3.4492625823387733, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1101/2000, Train Loss: 3.2340600295145006, Train Accuracy: 96.62295081967213%, Test Loss: 3.4496820248090305, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1102/2000, Train Loss: 3.2340593728862825, Train Accuracy: 96.62295081967213%, Test Loss: 3.449289945455698, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1103/2000, Train Loss: 3.2340589742191503, Train Accuracy: 96.62295081967213%, Test Loss: 3.449797327701862, Test Accuracy: 75.0%\n",
      "Epoch 1104/2000, Train Loss: 3.2340589116831295, Train Accuracy: 96.62295081967213%, Test Loss: 3.4493337869644165, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1105/2000, Train Loss: 3.2340585442840077, Train Accuracy: 96.62295081967213%, Test Loss: 3.44962595976316, Test Accuracy: 75.0%\n",
      "Epoch 1106/2000, Train Loss: 3.234059380703285, Train Accuracy: 96.62295081967213%, Test Loss: 3.449262701548063, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1107/2000, Train Loss: 3.234057668779717, Train Accuracy: 96.62295081967213%, Test Loss: 3.4498736216471744, Test Accuracy: 74.84615384615384%\n",
      "Epoch 1108/2000, Train Loss: 3.2340585950945244, Train Accuracy: 96.62295081967213%, Test Loss: 3.449567519701444, Test Accuracy: 75.0%\n",
      "Epoch 1109/2000, Train Loss: 3.234056695562894, Train Accuracy: 96.62295081967213%, Test Loss: 3.4496817405407247, Test Accuracy: 75.0%\n",
      "Epoch 1110/2000, Train Loss: 3.2340563672487854, Train Accuracy: 96.62295081967213%, Test Loss: 3.4494461646446815, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1111/2000, Train Loss: 3.2340563867912917, Train Accuracy: 96.62295081967213%, Test Loss: 3.4498002987641554, Test Accuracy: 75.0%\n",
      "Epoch 1112/2000, Train Loss: 3.2340570512365123, Train Accuracy: 96.62295081967213%, Test Loss: 3.4493178679392886, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1113/2000, Train Loss: 3.2340575671586835, Train Accuracy: 96.62295081967213%, Test Loss: 3.449753477023198, Test Accuracy: 75.0%\n",
      "Epoch 1114/2000, Train Loss: 3.2340574069101304, Train Accuracy: 96.62295081967213%, Test Loss: 3.449502101311317, Test Accuracy: 75.0%\n",
      "Epoch 1115/2000, Train Loss: 3.234055757522583, Train Accuracy: 96.62295081967213%, Test Loss: 3.449850504214947, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1116/2000, Train Loss: 3.2340567776414213, Train Accuracy: 96.62295081967213%, Test Loss: 3.449167563365056, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1117/2000, Train Loss: 3.234055851326614, Train Accuracy: 96.62295081967213%, Test Loss: 3.4496368261484, Test Accuracy: 75.0%\n",
      "Epoch 1118/2000, Train Loss: 3.234056801092429, Train Accuracy: 96.62295081967213%, Test Loss: 3.4493768765376163, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1119/2000, Train Loss: 3.234054584972194, Train Accuracy: 96.62295081967213%, Test Loss: 3.4497464436751146, Test Accuracy: 75.0%\n",
      "Epoch 1120/2000, Train Loss: 3.234055562097518, Train Accuracy: 96.62295081967213%, Test Loss: 3.449477580877451, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1121/2000, Train Loss: 3.234053650840384, Train Accuracy: 96.62295081967213%, Test Loss: 3.4495985966462355, Test Accuracy: 75.0%\n",
      "Epoch 1122/2000, Train Loss: 3.2341356355635846, Train Accuracy: 96.62295081967213%, Test Loss: 3.44883394241333, Test Accuracy: 75.23076923076923%\n",
      "Saved the best model with validation loss:  3.44883394241333\n",
      "Epoch 1123/2000, Train Loss: 3.2348910394262096, Train Accuracy: 96.52459016393442%, Test Loss: 3.4485215773949256, Test Accuracy: 75.3076923076923%\n",
      "Epoch 1124/2000, Train Loss: 3.2343929204784456, Train Accuracy: 96.62295081967213%, Test Loss: 3.449456031505878, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1125/2000, Train Loss: 3.23406804585066, Train Accuracy: 96.62295081967213%, Test Loss: 3.4496257305145264, Test Accuracy: 74.92307692307692%\n",
      "Epoch 1126/2000, Train Loss: 3.2340596933833887, Train Accuracy: 96.62295081967213%, Test Loss: 3.449612177335299, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1127/2000, Train Loss: 3.2340564258763047, Train Accuracy: 96.62295081967213%, Test Loss: 3.44967415699592, Test Accuracy: 75.0%\n",
      "Epoch 1128/2000, Train Loss: 3.2340538697164565, Train Accuracy: 96.62295081967213%, Test Loss: 3.449818546955402, Test Accuracy: 74.84615384615384%\n",
      "Epoch 1129/2000, Train Loss: 3.2340530098461713, Train Accuracy: 96.62295081967213%, Test Loss: 3.449603264148419, Test Accuracy: 75.0%\n",
      "Epoch 1130/2000, Train Loss: 3.2340524157539745, Train Accuracy: 96.62295081967213%, Test Loss: 3.4495600828757653, Test Accuracy: 75.0%\n",
      "Epoch 1131/2000, Train Loss: 3.2340520209953434, Train Accuracy: 96.62295081967213%, Test Loss: 3.4496528735527625, Test Accuracy: 75.0%\n",
      "Epoch 1132/2000, Train Loss: 3.234051645779219, Train Accuracy: 96.62295081967213%, Test Loss: 3.449417334336501, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1133/2000, Train Loss: 3.234051845112785, Train Accuracy: 96.62295081967213%, Test Loss: 3.4495040086599498, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1134/2000, Train Loss: 3.234051462079658, Train Accuracy: 96.62295081967213%, Test Loss: 3.4494741146381083, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1135/2000, Train Loss: 3.234051700498237, Train Accuracy: 96.62295081967213%, Test Loss: 3.449481019606957, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1136/2000, Train Loss: 3.2340512744715957, Train Accuracy: 96.62295081967213%, Test Loss: 3.4492907249010525, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1137/2000, Train Loss: 3.234051211935575, Train Accuracy: 96.62295081967213%, Test Loss: 3.4493478444906382, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1138/2000, Train Loss: 3.2340508758044635, Train Accuracy: 96.62295081967213%, Test Loss: 3.4492737421622643, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1139/2000, Train Loss: 3.23405029343777, Train Accuracy: 96.62295081967213%, Test Loss: 3.449342681811406, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1140/2000, Train Loss: 3.234050148823222, Train Accuracy: 96.62295081967213%, Test Loss: 3.449241179686326, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1141/2000, Train Loss: 3.2340508601704583, Train Accuracy: 96.62295081967213%, Test Loss: 3.4493392339119544, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1142/2000, Train Loss: 3.2340503755162975, Train Accuracy: 96.62295081967213%, Test Loss: 3.4491598239311805, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1143/2000, Train Loss: 3.234050183999734, Train Accuracy: 96.62295081967213%, Test Loss: 3.4492243711764994, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1144/2000, Train Loss: 3.234049765790095, Train Accuracy: 96.62295081967213%, Test Loss: 3.449225801687974, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1145/2000, Train Loss: 3.2340501136467106, Train Accuracy: 96.62295081967213%, Test Loss: 3.4492583183141856, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1146/2000, Train Loss: 3.234049593816038, Train Accuracy: 96.62295081967213%, Test Loss: 3.4490229074771586, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1147/2000, Train Loss: 3.2340503403397856, Train Accuracy: 96.62295081967213%, Test Loss: 3.4492456087699304, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1148/2000, Train Loss: 3.2340493358549525, Train Accuracy: 96.62295081967213%, Test Loss: 3.449098596206078, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1149/2000, Train Loss: 3.2340499963916716, Train Accuracy: 96.62295081967213%, Test Loss: 3.4492757320404053, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1150/2000, Train Loss: 3.2340494765609993, Train Accuracy: 96.62295081967213%, Test Loss: 3.4490405871317935, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1151/2000, Train Loss: 3.2340497423390873, Train Accuracy: 96.62295081967213%, Test Loss: 3.4492087272497325, Test Accuracy: 75.23076923076923%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1152/2000, Train Loss: 3.2340489762728333, Train Accuracy: 96.62295081967213%, Test Loss: 3.449065428513747, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1153/2000, Train Loss: 3.23404975015609, Train Accuracy: 96.62295081967213%, Test Loss: 3.449203454531156, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1154/2000, Train Loss: 3.234049632901051, Train Accuracy: 96.62295081967213%, Test Loss: 3.4490791375820455, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1155/2000, Train Loss: 3.234049503920508, Train Accuracy: 96.62295081967213%, Test Loss: 3.449219226837158, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1156/2000, Train Loss: 3.2340485307036855, Train Accuracy: 96.62295081967213%, Test Loss: 3.4489409923553467, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1157/2000, Train Loss: 3.2340491287043838, Train Accuracy: 96.62295081967213%, Test Loss: 3.449245782998892, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1158/2000, Train Loss: 3.2340488941943057, Train Accuracy: 96.62295081967213%, Test Loss: 3.448996773132911, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1159/2000, Train Loss: 3.2340487300372516, Train Accuracy: 96.62295081967213%, Test Loss: 3.449083401606633, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1160/2000, Train Loss: 3.234049109161877, Train Accuracy: 96.62295081967213%, Test Loss: 3.4491371925060568, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1161/2000, Train Loss: 3.234049460926994, Train Accuracy: 96.62295081967213%, Test Loss: 3.4491763573426466, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1162/2000, Train Loss: 3.2340491521553916, Train Accuracy: 96.62295081967213%, Test Loss: 3.4490072543804464, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1163/2000, Train Loss: 3.234048620599215, Train Accuracy: 96.62295081967213%, Test Loss: 3.449089683019198, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1164/2000, Train Loss: 3.234049132612885, Train Accuracy: 96.62295081967213%, Test Loss: 3.448918654368474, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1165/2000, Train Loss: 3.2340488981028073, Train Accuracy: 96.62295081967213%, Test Loss: 3.4490698942771325, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1166/2000, Train Loss: 3.2340493749399655, Train Accuracy: 96.62295081967213%, Test Loss: 3.4489171413274913, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1167/2000, Train Loss: 3.234048147670558, Train Accuracy: 96.62295081967213%, Test Loss: 3.4489696667744565, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1168/2000, Train Loss: 3.234048120311049, Train Accuracy: 96.62295081967213%, Test Loss: 3.4487466628734884, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1169/2000, Train Loss: 3.234047620022883, Train Accuracy: 96.62295081967213%, Test Loss: 3.449055891770583, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1170/2000, Train Loss: 3.2340484798931683, Train Accuracy: 96.62295081967213%, Test Loss: 3.4487903026434092, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1171/2000, Train Loss: 3.2340475887548727, Train Accuracy: 96.62295081967213%, Test Loss: 3.448892538364117, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1172/2000, Train Loss: 3.234048487710171, Train Accuracy: 96.62295081967213%, Test Loss: 3.4487442603478065, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1173/2000, Train Loss: 3.234048659684228, Train Accuracy: 96.62295081967213%, Test Loss: 3.4488621216553907, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1174/2000, Train Loss: 3.234049171697898, Train Accuracy: 96.62295081967213%, Test Loss: 3.448625399516179, Test Accuracy: 75.3076923076923%\n",
      "Epoch 1175/2000, Train Loss: 3.2340478349904545, Train Accuracy: 96.62295081967213%, Test Loss: 3.449060403383695, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1176/2000, Train Loss: 3.2340483978146413, Train Accuracy: 96.62295081967213%, Test Loss: 3.448900763805096, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1177/2000, Train Loss: 3.234048460350662, Train Accuracy: 96.62295081967213%, Test Loss: 3.448965466939486, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1178/2000, Train Loss: 3.2340461269753877, Train Accuracy: 96.62295081967213%, Test Loss: 3.4486668568391066, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1179/2000, Train Loss: 3.234047799813943, Train Accuracy: 96.62295081967213%, Test Loss: 3.4488841478641215, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1180/2000, Train Loss: 3.234048382180636, Train Accuracy: 96.62295081967213%, Test Loss: 3.448706553532527, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1181/2000, Train Loss: 3.234048343095623, Train Accuracy: 96.62295081967213%, Test Loss: 3.448855060797471, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1182/2000, Train Loss: 3.2340474675913327, Train Accuracy: 96.62295081967213%, Test Loss: 3.4487251226718607, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1183/2000, Train Loss: 3.2340484329911527, Train Accuracy: 96.62295081967213%, Test Loss: 3.4481882682213416, Test Accuracy: 75.3076923076923%\n",
      "Epoch 1184/2000, Train Loss: 3.2340484916186725, Train Accuracy: 96.62295081967213%, Test Loss: 3.4484926828971276, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1185/2000, Train Loss: 3.2340481672130648, Train Accuracy: 96.62295081967213%, Test Loss: 3.448858756285447, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1186/2000, Train Loss: 3.2340465295510215, Train Accuracy: 96.62295081967213%, Test Loss: 3.448527822127709, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1187/2000, Train Loss: 3.234047221355751, Train Accuracy: 96.62295081967213%, Test Loss: 3.448760592020475, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1188/2000, Train Loss: 3.234047264349265, Train Accuracy: 96.62295081967213%, Test Loss: 3.448505685879634, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1189/2000, Train Loss: 3.23404598236084, Train Accuracy: 96.62295081967213%, Test Loss: 3.448778610963088, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1190/2000, Train Loss: 3.2340467406100912, Train Accuracy: 96.62295081967213%, Test Loss: 3.4484404233785777, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1191/2000, Train Loss: 3.2340473464277926, Train Accuracy: 96.62295081967213%, Test Loss: 3.4485780275785007, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1192/2000, Train Loss: 3.2340472917087744, Train Accuracy: 96.62295081967213%, Test Loss: 3.4486767787199755, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1193/2000, Train Loss: 3.234045458621666, Train Accuracy: 96.62295081967213%, Test Loss: 3.4481785205694346, Test Accuracy: 75.3076923076923%\n",
      "Epoch 1194/2000, Train Loss: 3.2340474558658285, Train Accuracy: 96.62295081967213%, Test Loss: 3.4482019589497495, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1195/2000, Train Loss: 3.2340455485171957, Train Accuracy: 96.62295081967213%, Test Loss: 3.4484828893954935, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1196/2000, Train Loss: 3.23404733861079, Train Accuracy: 96.62295081967213%, Test Loss: 3.4485864639282227, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1197/2000, Train Loss: 3.234044981784508, Train Accuracy: 96.62295081967213%, Test Loss: 3.4484079892818746, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1198/2000, Train Loss: 3.2340527440680833, Train Accuracy: 96.62295081967213%, Test Loss: 3.448314455839304, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1199/2000, Train Loss: 3.2340451459415624, Train Accuracy: 96.62295081967213%, Test Loss: 3.448411097893348, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1200/2000, Train Loss: 3.2340440515611992, Train Accuracy: 96.62295081967213%, Test Loss: 3.4481841325759888, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1201/2000, Train Loss: 3.2340451693925703, Train Accuracy: 96.62295081967213%, Test Loss: 3.4486149916282067, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1202/2000, Train Loss: 3.2340470923752083, Train Accuracy: 96.62295081967213%, Test Loss: 3.448234090438256, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1203/2000, Train Loss: 3.234044954424999, Train Accuracy: 96.62295081967213%, Test Loss: 3.4484750766020555, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1204/2000, Train Loss: 3.2340457087657493, Train Accuracy: 96.62295081967213%, Test Loss: 3.4481038313645582, Test Accuracy: 75.23076923076923%\n",
      "Epoch 1205/2000, Train Loss: 3.23404369197908, Train Accuracy: 96.62295081967213%, Test Loss: 3.4486009982916026, Test Accuracy: 75.15384615384616%\n",
      "Epoch 1206/2000, Train Loss: 3.2340442430777627, Train Accuracy: 96.62295081967213%, Test Loss: 3.4480535708940945, Test Accuracy: 75.23076923076923%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1207/2000, Train Loss: 3.234043637260062, Train Accuracy: 96.62295081967213%, Test Loss: 3.4485129851561327, Test Accuracy: 75.07692307692308%\n",
      "Epoch 1208/2000, Train Loss: 3.2287159669594687, Train Accuracy: 97.24590163934427%, Test Loss: 3.4411361400897684, Test Accuracy: 76.0%\n",
      "Saved the best model with validation loss:  3.4411361400897684\n",
      "Epoch 1209/2000, Train Loss: 3.223288704137333, Train Accuracy: 97.77049180327869%, Test Loss: 3.4413584837546716, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1210/2000, Train Loss: 3.2225653851618534, Train Accuracy: 97.80327868852459%, Test Loss: 3.4422544791148257, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1211/2000, Train Loss: 3.222101543770462, Train Accuracy: 97.8688524590164%, Test Loss: 3.4419705225871158, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1212/2000, Train Loss: 3.221901729458668, Train Accuracy: 97.8688524590164%, Test Loss: 3.4421859979629517, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1213/2000, Train Loss: 3.2217963249956973, Train Accuracy: 97.8688524590164%, Test Loss: 3.442119763447688, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1214/2000, Train Loss: 3.2217465267806755, Train Accuracy: 97.8688524590164%, Test Loss: 3.4424324310742893, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1215/2000, Train Loss: 3.2216747158863503, Train Accuracy: 97.90163934426229%, Test Loss: 3.4428195036374607, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1216/2000, Train Loss: 3.2215659657462696, Train Accuracy: 97.90163934426229%, Test Loss: 3.442616315988394, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1217/2000, Train Loss: 3.221496585939751, Train Accuracy: 97.90163934426229%, Test Loss: 3.4431478610405555, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1218/2000, Train Loss: 3.2214697618953516, Train Accuracy: 97.90163934426229%, Test Loss: 3.443068504333496, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1219/2000, Train Loss: 3.2214496487476785, Train Accuracy: 97.90163934426229%, Test Loss: 3.4431460545613217, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1220/2000, Train Loss: 3.221427170956721, Train Accuracy: 97.90163934426229%, Test Loss: 3.443379888167748, Test Accuracy: 75.53846153846153%\n",
      "Epoch 1221/2000, Train Loss: 3.2213824342508786, Train Accuracy: 97.90163934426229%, Test Loss: 3.443592612559979, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1222/2000, Train Loss: 3.221283076239414, Train Accuracy: 97.93442622950819%, Test Loss: 3.4437728845156155, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1223/2000, Train Loss: 3.221197753656106, Train Accuracy: 97.93442622950819%, Test Loss: 3.443968039292556, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1224/2000, Train Loss: 3.2211566948499835, Train Accuracy: 97.93442622950819%, Test Loss: 3.4437028628129225, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1225/2000, Train Loss: 3.221143171435497, Train Accuracy: 97.93442622950819%, Test Loss: 3.444139764859126, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1226/2000, Train Loss: 3.2211318289647335, Train Accuracy: 97.93442622950819%, Test Loss: 3.4440852311941295, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1227/2000, Train Loss: 3.2211224837381334, Train Accuracy: 97.93442622950819%, Test Loss: 3.444144945878249, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1228/2000, Train Loss: 3.2211167538752323, Train Accuracy: 97.93442622950819%, Test Loss: 3.443860466663654, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1229/2000, Train Loss: 3.221115944815464, Train Accuracy: 97.93442622950819%, Test Loss: 3.4440479828761172, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1230/2000, Train Loss: 3.221108667186049, Train Accuracy: 97.93442622950819%, Test Loss: 3.4441087429340067, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1231/2000, Train Loss: 3.221106400255297, Train Accuracy: 97.93442622950819%, Test Loss: 3.4439696807127733, Test Accuracy: 75.53846153846153%\n",
      "Epoch 1232/2000, Train Loss: 3.2211018468512864, Train Accuracy: 97.93442622950819%, Test Loss: 3.443909470851605, Test Accuracy: 75.46153846153847%\n",
      "Epoch 1233/2000, Train Loss: 3.221099192978906, Train Accuracy: 97.93442622950819%, Test Loss: 3.443818129025973, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1234/2000, Train Loss: 3.2210973325322887, Train Accuracy: 97.93442622950819%, Test Loss: 3.443597674369812, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1235/2000, Train Loss: 3.221095503353682, Train Accuracy: 97.93442622950819%, Test Loss: 3.4434935679802527, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1236/2000, Train Loss: 3.221094612215386, Train Accuracy: 97.93442622950819%, Test Loss: 3.443174875699557, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1237/2000, Train Loss: 3.221091712107424, Train Accuracy: 97.93442622950819%, Test Loss: 3.4430176661564755, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1238/2000, Train Loss: 3.2210886204828983, Train Accuracy: 97.93442622950819%, Test Loss: 3.443057683797983, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1239/2000, Train Loss: 3.221089793033287, Train Accuracy: 97.93442622950819%, Test Loss: 3.4430160705859842, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1240/2000, Train Loss: 3.221086091682559, Train Accuracy: 97.93442622950819%, Test Loss: 3.4430834421744714, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1241/2000, Train Loss: 3.221088065475714, Train Accuracy: 97.93442622950819%, Test Loss: 3.443070677610544, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1242/2000, Train Loss: 3.221084719798604, Train Accuracy: 97.93442622950819%, Test Loss: 3.443066221017104, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1243/2000, Train Loss: 3.2210828046329687, Train Accuracy: 97.93442622950819%, Test Loss: 3.443112987738389, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1244/2000, Train Loss: 3.2210824098743376, Train Accuracy: 97.93442622950819%, Test Loss: 3.443076170407809, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1245/2000, Train Loss: 3.221081163062424, Train Accuracy: 97.93442622950819%, Test Loss: 3.443144651559683, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1246/2000, Train Loss: 3.22108051815971, Train Accuracy: 97.93442622950819%, Test Loss: 3.4430345938755917, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1247/2000, Train Loss: 3.2210797442764534, Train Accuracy: 97.93442622950819%, Test Loss: 3.443135903431819, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1248/2000, Train Loss: 3.2210791189162458, Train Accuracy: 97.93442622950819%, Test Loss: 3.4430532363744883, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1249/2000, Train Loss: 3.221077938548854, Train Accuracy: 97.93442622950819%, Test Loss: 3.4430577754974365, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1250/2000, Train Loss: 3.221077668862265, Train Accuracy: 97.93442622950819%, Test Loss: 3.4430858538700986, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1251/2000, Train Loss: 3.2210629572633835, Train Accuracy: 97.93442622950819%, Test Loss: 3.4431068347050595, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1252/2000, Train Loss: 3.2208006655583614, Train Accuracy: 97.9672131147541%, Test Loss: 3.4433210537983823, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1253/2000, Train Loss: 3.220768611939227, Train Accuracy: 97.9672131147541%, Test Loss: 3.442058700781602, Test Accuracy: 76.0%\n",
      "Epoch 1254/2000, Train Loss: 3.220758590541902, Train Accuracy: 97.9672131147541%, Test Loss: 3.442822841497568, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1255/2000, Train Loss: 3.220758586633401, Train Accuracy: 97.9672131147541%, Test Loss: 3.4423843805606547, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1256/2000, Train Loss: 3.22075505725673, Train Accuracy: 97.9672131147541%, Test Loss: 3.44251825259282, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1257/2000, Train Loss: 3.2207613890288305, Train Accuracy: 97.9672131147541%, Test Loss: 3.4427843735768247, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1258/2000, Train Loss: 3.2207557764209684, Train Accuracy: 97.9672131147541%, Test Loss: 3.4427988529205322, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1259/2000, Train Loss: 3.2207516099585862, Train Accuracy: 97.9672131147541%, Test Loss: 3.442923985994779, Test Accuracy: 75.61538461538461%\n",
      "Epoch 1260/2000, Train Loss: 3.220748236921967, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429679283728967, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1261/2000, Train Loss: 3.2207474786727155, Train Accuracy: 97.9672131147541%, Test Loss: 3.4428909833614645, Test Accuracy: 75.84615384615384%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1262/2000, Train Loss: 3.220746771233981, Train Accuracy: 97.9672131147541%, Test Loss: 3.4430377024870653, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1263/2000, Train Loss: 3.2207462748543163, Train Accuracy: 97.9672131147541%, Test Loss: 3.443066652004535, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1264/2000, Train Loss: 3.220745852736176, Train Accuracy: 97.9672131147541%, Test Loss: 3.443129750398489, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1265/2000, Train Loss: 3.220745457977545, Train Accuracy: 97.9672131147541%, Test Loss: 3.4430789397313046, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1266/2000, Train Loss: 3.220745969991215, Train Accuracy: 97.9672131147541%, Test Loss: 3.4431284941159763, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1267/2000, Train Loss: 3.2207447114537975, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429922837477465, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1268/2000, Train Loss: 3.2207440665510836, Train Accuracy: 97.9672131147541%, Test Loss: 3.4430515674444346, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1269/2000, Train Loss: 3.220744004015063, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429590426958523, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1270/2000, Train Loss: 3.2207432496743125, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429728984832764, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1271/2000, Train Loss: 3.2207426712161205, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429738154778113, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1272/2000, Train Loss: 3.2207433473868448, Train Accuracy: 97.9672131147541%, Test Loss: 3.442992476316599, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1273/2000, Train Loss: 3.220742202195965, Train Accuracy: 97.9672131147541%, Test Loss: 3.4428988236647387, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1274/2000, Train Loss: 3.2207422100129675, Train Accuracy: 97.9672131147541%, Test Loss: 3.44303927054772, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1275/2000, Train Loss: 3.2207418035288327, Train Accuracy: 97.9672131147541%, Test Loss: 3.4430021322690525, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1276/2000, Train Loss: 3.2207414087702015, Train Accuracy: 97.9672131147541%, Test Loss: 3.442872166633606, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1277/2000, Train Loss: 3.2207413657766875, Train Accuracy: 97.9672131147541%, Test Loss: 3.442899300501897, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1278/2000, Train Loss: 3.220741217253638, Train Accuracy: 97.9672131147541%, Test Loss: 3.442785776578463, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1279/2000, Train Loss: 3.2207406856974616, Train Accuracy: 97.9672131147541%, Test Loss: 3.443013677230248, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1280/2000, Train Loss: 3.220741295423664, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429829670832706, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1281/2000, Train Loss: 3.2207406661549554, Train Accuracy: 97.9672131147541%, Test Loss: 3.442784492786114, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1282/2000, Train Loss: 3.2207403261153424, Train Accuracy: 97.9672131147541%, Test Loss: 3.442952797963069, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1283/2000, Train Loss: 3.2207406935144642, Train Accuracy: 97.9672131147541%, Test Loss: 3.442883555705731, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1284/2000, Train Loss: 3.2207400056182363, Train Accuracy: 97.9672131147541%, Test Loss: 3.4427490601172814, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1285/2000, Train Loss: 3.2207398922716983, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429089839641867, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1286/2000, Train Loss: 3.220739954807719, Train Accuracy: 97.9672131147541%, Test Loss: 3.4428758712915273, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1287/2000, Train Loss: 3.2207401463242826, Train Accuracy: 97.9672131147541%, Test Loss: 3.4430286242411685, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1288/2000, Train Loss: 3.220739071486426, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429922562379103, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1289/2000, Train Loss: 3.2207399469907165, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429917335510254, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1290/2000, Train Loss: 3.2207431441447776, Train Accuracy: 97.9672131147541%, Test Loss: 3.4426430738889255, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1291/2000, Train Loss: 3.2207393880750312, Train Accuracy: 97.9672131147541%, Test Loss: 3.442739174916194, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1292/2000, Train Loss: 3.2207395209640755, Train Accuracy: 97.9672131147541%, Test Loss: 3.442719872181232, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1293/2000, Train Loss: 3.220738516479242, Train Accuracy: 97.9672131147541%, Test Loss: 3.4426462925397434, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1294/2000, Train Loss: 3.2207385594727564, Train Accuracy: 97.9672131147541%, Test Loss: 3.442685319827153, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1295/2000, Train Loss: 3.2207400447032493, Train Accuracy: 97.9672131147541%, Test Loss: 3.442625174155602, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1296/2000, Train Loss: 3.220738254609655, Train Accuracy: 97.9672131147541%, Test Loss: 3.442826133507949, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1297/2000, Train Loss: 3.220738731446813, Train Accuracy: 97.9672131147541%, Test Loss: 3.4427786698708167, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1298/2000, Train Loss: 3.2207390011334027, Train Accuracy: 97.9672131147541%, Test Loss: 3.4424333847486057, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1299/2000, Train Loss: 3.2207384656687252, Train Accuracy: 97.9672131147541%, Test Loss: 3.4423534961847158, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1300/2000, Train Loss: 3.220739517055574, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429525228647084, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1301/2000, Train Loss: 3.220739610859605, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429309459832997, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1302/2000, Train Loss: 3.2207412719726562, Train Accuracy: 97.9672131147541%, Test Loss: 3.442659460581266, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1303/2000, Train Loss: 3.220752321305822, Train Accuracy: 97.9672131147541%, Test Loss: 3.4424103498458862, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1304/2000, Train Loss: 3.2207399860757295, Train Accuracy: 97.9672131147541%, Test Loss: 3.442645714833186, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1305/2000, Train Loss: 3.2207397320231452, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422803933803854, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1306/2000, Train Loss: 3.220742139659944, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422180377520046, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1307/2000, Train Loss: 3.220741256338651, Train Accuracy: 97.9672131147541%, Test Loss: 3.442782814686115, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1308/2000, Train Loss: 3.220739290362499, Train Accuracy: 97.9672131147541%, Test Loss: 3.442597664319552, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1309/2000, Train Loss: 3.2207359173258796, Train Accuracy: 97.9672131147541%, Test Loss: 3.442521627132709, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1310/2000, Train Loss: 3.220735299782675, Train Accuracy: 97.9672131147541%, Test Loss: 3.442549595466027, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1311/2000, Train Loss: 3.2207344399123894, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422979721656213, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1312/2000, Train Loss: 3.2207334666955667, Train Accuracy: 97.9672131147541%, Test Loss: 3.4424676711742697, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1313/2000, Train Loss: 3.2207333455320266, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422456667973447, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1314/2000, Train Loss: 3.220747064371578, Train Accuracy: 97.9672131147541%, Test Loss: 3.442578306564918, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1315/2000, Train Loss: 3.2208023227629115, Train Accuracy: 97.9672131147541%, Test Loss: 3.44296186703902, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1316/2000, Train Loss: 3.221635959187492, Train Accuracy: 97.8688524590164%, Test Loss: 3.4429223262346706, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1317/2000, Train Loss: 3.221075921762185, Train Accuracy: 97.93442622950819%, Test Loss: 3.4421352331454935, Test Accuracy: 75.92307692307692%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1318/2000, Train Loss: 3.2210112321572226, Train Accuracy: 97.93442622950819%, Test Loss: 3.4421095848083496, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1319/2000, Train Loss: 3.220740451187384, Train Accuracy: 97.9672131147541%, Test Loss: 3.4419665061510525, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1320/2000, Train Loss: 3.2207362378229862, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422284089601956, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1321/2000, Train Loss: 3.220733208734481, Train Accuracy: 97.9672131147541%, Test Loss: 3.442044349817129, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1322/2000, Train Loss: 3.220732860877866, Train Accuracy: 97.9672131147541%, Test Loss: 3.4420844408181996, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1323/2000, Train Loss: 3.2207318681185364, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421628346809974, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1324/2000, Train Loss: 3.2207321729816374, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422752490410438, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1325/2000, Train Loss: 3.220731782131508, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421174434515147, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1326/2000, Train Loss: 3.2207307424701628, Train Accuracy: 97.9672131147541%, Test Loss: 3.442172573162959, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1327/2000, Train Loss: 3.2207309496207315, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421771673055797, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1328/2000, Train Loss: 3.2207302812670098, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421211939591627, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1329/2000, Train Loss: 3.220730378979542, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421336559148936, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1330/2000, Train Loss: 3.220731574980939, Train Accuracy: 97.9672131147541%, Test Loss: 3.442175241617056, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1331/2000, Train Loss: 3.220730390705046, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421815138596754, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1332/2000, Train Loss: 3.2207301718289734, Train Accuracy: 97.9672131147541%, Test Loss: 3.44212334889632, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1333/2000, Train Loss: 3.220730976980241, Train Accuracy: 97.9672131147541%, Test Loss: 3.442213095151461, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1334/2000, Train Loss: 3.220730597855615, Train Accuracy: 97.9672131147541%, Test Loss: 3.442118525505066, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1335/2000, Train Loss: 3.2207304649665707, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422905995295596, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1336/2000, Train Loss: 3.2207301679204723, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421755350553074, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1337/2000, Train Loss: 3.2207294839327454, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421894458624034, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1338/2000, Train Loss: 3.2207293823117116, Train Accuracy: 97.9672131147541%, Test Loss: 3.442230087060195, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1339/2000, Train Loss: 3.220734299206343, Train Accuracy: 97.9672131147541%, Test Loss: 3.442358053647555, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1340/2000, Train Loss: 3.220729995946415, Train Accuracy: 97.9672131147541%, Test Loss: 3.442277055520278, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1341/2000, Train Loss: 3.2207308167316873, Train Accuracy: 97.9672131147541%, Test Loss: 3.442304932154142, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1342/2000, Train Loss: 3.2207305392280956, Train Accuracy: 97.9672131147541%, Test Loss: 3.442410313166105, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1343/2000, Train Loss: 3.2207300272144255, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422720120503354, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1344/2000, Train Loss: 3.2207293197756908, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422710033563466, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1345/2000, Train Loss: 3.2207296832663115, Train Accuracy: 97.9672131147541%, Test Loss: 3.442290030992948, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1346/2000, Train Loss: 3.220730715110654, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422799807328444, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1347/2000, Train Loss: 3.2207290774486106, Train Accuracy: 97.9672131147541%, Test Loss: 3.442272965724652, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1348/2000, Train Loss: 3.2207318563930323, Train Accuracy: 97.9672131147541%, Test Loss: 3.442259128277118, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1349/2000, Train Loss: 3.220729335409696, Train Accuracy: 97.9672131147541%, Test Loss: 3.442286968231201, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1350/2000, Train Loss: 3.2207295816452777, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422071163470926, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1351/2000, Train Loss: 3.2207297262598256, Train Accuracy: 97.9672131147541%, Test Loss: 3.442295688849229, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1352/2000, Train Loss: 3.2207305861301108, Train Accuracy: 97.9672131147541%, Test Loss: 3.442345930979802, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1353/2000, Train Loss: 3.2207325286552555, Train Accuracy: 97.9672131147541%, Test Loss: 3.442302153660701, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1354/2000, Train Loss: 3.2207304258815577, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422353230989895, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1355/2000, Train Loss: 3.2207301249269578, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422295552033644, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1356/2000, Train Loss: 3.2207295621027714, Train Accuracy: 97.9672131147541%, Test Loss: 3.442023231432988, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1357/2000, Train Loss: 3.2207289250170597, Train Accuracy: 97.9672131147541%, Test Loss: 3.4423525425103993, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1358/2000, Train Loss: 3.220729335409696, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421047339072595, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1359/2000, Train Loss: 3.220730617398121, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422784860317526, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1360/2000, Train Loss: 3.2207296480897996, Train Accuracy: 97.9672131147541%, Test Loss: 3.442223934026865, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1361/2000, Train Loss: 3.22073049232608, Train Accuracy: 97.9672131147541%, Test Loss: 3.441884288421044, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1362/2000, Train Loss: 3.220729440939231, Train Accuracy: 97.9672131147541%, Test Loss: 3.4420970311531653, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1363/2000, Train Loss: 3.220734060787764, Train Accuracy: 97.9672131147541%, Test Loss: 3.442448982825646, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1364/2000, Train Loss: 3.220730453241067, Train Accuracy: 97.9672131147541%, Test Loss: 3.4425110083359938, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1365/2000, Train Loss: 3.22073040243055, Train Accuracy: 97.9672131147541%, Test Loss: 3.442556151976952, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1366/2000, Train Loss: 3.220733380708538, Train Accuracy: 97.9672131147541%, Test Loss: 3.442676168221694, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1367/2000, Train Loss: 3.2207326068252815, Train Accuracy: 97.9672131147541%, Test Loss: 3.4424195197912364, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1368/2000, Train Loss: 3.220730175737475, Train Accuracy: 97.9672131147541%, Test Loss: 3.442156195640564, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1369/2000, Train Loss: 3.2207306994766487, Train Accuracy: 97.9672131147541%, Test Loss: 3.4418870485745945, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1370/2000, Train Loss: 3.2207307385616617, Train Accuracy: 97.9672131147541%, Test Loss: 3.441721741969769, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1371/2000, Train Loss: 3.220732825701354, Train Accuracy: 97.9672131147541%, Test Loss: 3.442625724352323, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1372/2000, Train Loss: 3.2207335057805797, Train Accuracy: 97.9672131147541%, Test Loss: 3.442036115206205, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1373/2000, Train Loss: 3.220730840182695, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422695545049815, Test Accuracy: 75.76923076923077%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1374/2000, Train Loss: 3.2207319462885624, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421289700728197, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1375/2000, Train Loss: 3.2207333181725173, Train Accuracy: 97.9672131147541%, Test Loss: 3.4419608299548807, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1376/2000, Train Loss: 3.2210896288762325, Train Accuracy: 97.93442622950819%, Test Loss: 3.4429250130286584, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1377/2000, Train Loss: 3.220762815631804, Train Accuracy: 97.9672131147541%, Test Loss: 3.4426967730888953, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1378/2000, Train Loss: 3.22074755293424, Train Accuracy: 97.9672131147541%, Test Loss: 3.4424388225261984, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1379/2000, Train Loss: 3.2207363746205315, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422873441989603, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1380/2000, Train Loss: 3.22073281397585, Train Accuracy: 97.9672131147541%, Test Loss: 3.4429074709232035, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1381/2000, Train Loss: 3.220730953529233, Train Accuracy: 97.9672131147541%, Test Loss: 3.4424713208125186, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1382/2000, Train Loss: 3.220729734076828, Train Accuracy: 97.9672131147541%, Test Loss: 3.4423198975049534, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1383/2000, Train Loss: 3.220728033878764, Train Accuracy: 97.9672131147541%, Test Loss: 3.4425917955545278, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1384/2000, Train Loss: 3.220727310806024, Train Accuracy: 97.9672131147541%, Test Loss: 3.4424154116557193, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1385/2000, Train Loss: 3.2207267636158425, Train Accuracy: 97.9672131147541%, Test Loss: 3.4424692667447605, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1386/2000, Train Loss: 3.220726833968866, Train Accuracy: 97.9672131147541%, Test Loss: 3.442304858794579, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1387/2000, Train Loss: 3.22072672453083, Train Accuracy: 97.9672131147541%, Test Loss: 3.4423557153114905, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1388/2000, Train Loss: 3.2207269981259206, Train Accuracy: 97.9672131147541%, Test Loss: 3.4423771454737735, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1389/2000, Train Loss: 3.220726661994809, Train Accuracy: 97.9672131147541%, Test Loss: 3.4422373863366933, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1390/2000, Train Loss: 3.2207266659033102, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421661450312686, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1391/2000, Train Loss: 3.220726044451604, Train Accuracy: 97.9672131147541%, Test Loss: 3.442166071671706, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1392/2000, Train Loss: 3.2207267479818373, Train Accuracy: 97.9672131147541%, Test Loss: 3.4420878245280337, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1393/2000, Train Loss: 3.220726661994809, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421023497214684, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1394/2000, Train Loss: 3.2207255480719392, Train Accuracy: 97.9672131147541%, Test Loss: 3.4420386919608483, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1395/2000, Train Loss: 3.2207260522686068, Train Accuracy: 97.9672131147541%, Test Loss: 3.442057802126958, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1396/2000, Train Loss: 3.220725809941526, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421409093416653, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1397/2000, Train Loss: 3.2207264822037494, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421391028624315, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1398/2000, Train Loss: 3.2207264978377546, Train Accuracy: 97.9672131147541%, Test Loss: 3.4420679807662964, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1399/2000, Train Loss: 3.2207267362563337, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421935815077562, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1400/2000, Train Loss: 3.220726294595687, Train Accuracy: 97.9672131147541%, Test Loss: 3.4420206179985633, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1401/2000, Train Loss: 3.220727256087006, Train Accuracy: 97.9672131147541%, Test Loss: 3.442069099499629, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1402/2000, Train Loss: 3.2207279048982214, Train Accuracy: 97.9672131147541%, Test Loss: 3.442318091025719, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1403/2000, Train Loss: 3.2207276156691256, Train Accuracy: 97.9672131147541%, Test Loss: 3.442252049079308, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1404/2000, Train Loss: 3.220727451512071, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421633115181556, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1405/2000, Train Loss: 3.2207267753413467, Train Accuracy: 97.9672131147541%, Test Loss: 3.44194247172429, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1406/2000, Train Loss: 3.220730875359207, Train Accuracy: 97.9672131147541%, Test Loss: 3.4417870136407704, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1407/2000, Train Loss: 3.2207282371208317, Train Accuracy: 97.9672131147541%, Test Loss: 3.442151894936195, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1408/2000, Train Loss: 3.2207289054745534, Train Accuracy: 97.9672131147541%, Test Loss: 3.4423873332830577, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1409/2000, Train Loss: 3.220727772009177, Train Accuracy: 97.9672131147541%, Test Loss: 3.4421216432864847, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1410/2000, Train Loss: 3.220763870927154, Train Accuracy: 97.9672131147541%, Test Loss: 3.443633849804218, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1411/2000, Train Loss: 3.2205484656036876, Train Accuracy: 98.0%, Test Loss: 3.442454072145315, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1412/2000, Train Loss: 3.2207196462349814, Train Accuracy: 97.9672131147541%, Test Loss: 3.4419180063100963, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1413/2000, Train Loss: 3.2204139467145576, Train Accuracy: 98.0%, Test Loss: 3.441727950022771, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1414/2000, Train Loss: 3.2204058600253744, Train Accuracy: 98.0%, Test Loss: 3.4418804187041063, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1415/2000, Train Loss: 3.220403835421703, Train Accuracy: 98.0%, Test Loss: 3.441870478483347, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1416/2000, Train Loss: 3.220441153792084, Train Accuracy: 98.0%, Test Loss: 3.441898043339069, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1417/2000, Train Loss: 3.2204015294059376, Train Accuracy: 98.0%, Test Loss: 3.442048137004559, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1418/2000, Train Loss: 3.2204005640061175, Train Accuracy: 98.0%, Test Loss: 3.441984781852135, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1419/2000, Train Loss: 3.2204015215889354, Train Accuracy: 98.0%, Test Loss: 3.442143467756418, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1420/2000, Train Loss: 3.220399356279217, Train Accuracy: 98.0%, Test Loss: 3.442071245266841, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1421/2000, Train Loss: 3.22039921557317, Train Accuracy: 98.0%, Test Loss: 3.442043341123141, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1422/2000, Train Loss: 3.220398934161077, Train Accuracy: 98.0%, Test Loss: 3.4420038736783543, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1423/2000, Train Loss: 3.220398531585443, Train Accuracy: 98.0%, Test Loss: 3.441982333476727, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1424/2000, Train Loss: 3.2203989068015675, Train Accuracy: 98.0%, Test Loss: 3.442032520587628, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1425/2000, Train Loss: 3.220398770004022, Train Accuracy: 98.0%, Test Loss: 3.442036454494183, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1426/2000, Train Loss: 3.2203986644744873, Train Accuracy: 98.0%, Test Loss: 3.442045523570134, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1427/2000, Train Loss: 3.2203987231020066, Train Accuracy: 98.0%, Test Loss: 3.4420271928493795, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1428/2000, Train Loss: 3.220398953703583, Train Accuracy: 98.0%, Test Loss: 3.4420412136958194, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1429/2000, Train Loss: 3.220398187637329, Train Accuracy: 98.0%, Test Loss: 3.4420329057253323, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1430/2000, Train Loss: 3.2203988638080534, Train Accuracy: 98.0%, Test Loss: 3.44206872353187, Test Accuracy: 75.76923076923077%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1431/2000, Train Loss: 3.22039879345503, Train Accuracy: 98.0%, Test Loss: 3.4420341620078454, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1432/2000, Train Loss: 3.22039816027782, Train Accuracy: 98.0%, Test Loss: 3.4420986083837657, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1433/2000, Train Loss: 3.2203975388261137, Train Accuracy: 98.0%, Test Loss: 3.442026220835172, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1434/2000, Train Loss: 3.220400134070975, Train Accuracy: 98.0%, Test Loss: 3.442091336617103, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1435/2000, Train Loss: 3.2203991569456507, Train Accuracy: 98.0%, Test Loss: 3.442123770713806, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1436/2000, Train Loss: 3.220399872201388, Train Accuracy: 98.0%, Test Loss: 3.4422069696279674, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1437/2000, Train Loss: 3.2203990084226013, Train Accuracy: 98.0%, Test Loss: 3.4421752232771654, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1438/2000, Train Loss: 3.2203986527489836, Train Accuracy: 98.0%, Test Loss: 3.4420574444990892, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1439/2000, Train Loss: 3.220397730342677, Train Accuracy: 98.0%, Test Loss: 3.442055867268489, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1440/2000, Train Loss: 3.220398609755469, Train Accuracy: 98.0%, Test Loss: 3.4420552253723145, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1441/2000, Train Loss: 3.2203980781992927, Train Accuracy: 98.0%, Test Loss: 3.4421492906717153, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1442/2000, Train Loss: 3.220399567338287, Train Accuracy: 98.0%, Test Loss: 3.4420482562138486, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1443/2000, Train Loss: 3.220398183728828, Train Accuracy: 98.0%, Test Loss: 3.4420232497728787, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1444/2000, Train Loss: 3.220398304892368, Train Accuracy: 98.0%, Test Loss: 3.442058856670673, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1445/2000, Train Loss: 3.2203986527489836, Train Accuracy: 98.0%, Test Loss: 3.4420496775553775, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1446/2000, Train Loss: 3.220399129586142, Train Accuracy: 98.0%, Test Loss: 3.4422306372569156, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1447/2000, Train Loss: 3.220400489744593, Train Accuracy: 98.0%, Test Loss: 3.4421125191908617, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1448/2000, Train Loss: 3.2203992194816715, Train Accuracy: 98.0%, Test Loss: 3.4420193342062144, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1449/2000, Train Loss: 3.220399450083248, Train Accuracy: 98.0%, Test Loss: 3.441995153060326, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1450/2000, Train Loss: 3.220399055324617, Train Accuracy: 98.0%, Test Loss: 3.4422221733973575, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1451/2000, Train Loss: 3.2203987465530144, Train Accuracy: 98.0%, Test Loss: 3.441902463252728, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1452/2000, Train Loss: 3.220405430090232, Train Accuracy: 98.0%, Test Loss: 3.442300567260155, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1453/2000, Train Loss: 3.220404839906536, Train Accuracy: 98.0%, Test Loss: 3.441766078655536, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1454/2000, Train Loss: 3.2203984573239186, Train Accuracy: 98.0%, Test Loss: 3.4420319612209616, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1455/2000, Train Loss: 3.220398476866425, Train Accuracy: 98.0%, Test Loss: 3.4422790912481456, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1456/2000, Train Loss: 3.2203996181488037, Train Accuracy: 98.0%, Test Loss: 3.442153811454773, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1457/2000, Train Loss: 3.2203995829722922, Train Accuracy: 98.0%, Test Loss: 3.4419974455466638, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1458/2000, Train Loss: 3.2203987231020066, Train Accuracy: 98.0%, Test Loss: 3.4422769271410427, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1459/2000, Train Loss: 3.220399387547227, Train Accuracy: 98.0%, Test Loss: 3.4419204088357778, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1460/2000, Train Loss: 3.220403639996638, Train Accuracy: 98.0%, Test Loss: 3.4423637756934533, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1461/2000, Train Loss: 3.220403518833098, Train Accuracy: 98.0%, Test Loss: 3.441910881262559, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1462/2000, Train Loss: 3.2203987231020066, Train Accuracy: 98.0%, Test Loss: 3.4418746599784265, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1463/2000, Train Loss: 3.220421380684024, Train Accuracy: 98.0%, Test Loss: 3.4430135671909037, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1464/2000, Train Loss: 3.2207154211450795, Train Accuracy: 97.9672131147541%, Test Loss: 3.4423536429038415, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1465/2000, Train Loss: 3.2204011424643095, Train Accuracy: 98.0%, Test Loss: 3.4421540040236254, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1466/2000, Train Loss: 3.2204010642942835, Train Accuracy: 98.0%, Test Loss: 3.442101524426387, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1467/2000, Train Loss: 3.2203991647626533, Train Accuracy: 98.0%, Test Loss: 3.4423317450743456, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1468/2000, Train Loss: 3.2203995595212844, Train Accuracy: 98.0%, Test Loss: 3.4416887301665087, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1469/2000, Train Loss: 3.2203990045141, Train Accuracy: 98.0%, Test Loss: 3.442093720802894, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1470/2000, Train Loss: 3.220399856567383, Train Accuracy: 98.0%, Test Loss: 3.442562451729408, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1471/2000, Train Loss: 3.2203985980299654, Train Accuracy: 98.0%, Test Loss: 3.44225707420936, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1472/2000, Train Loss: 3.2203981524608176, Train Accuracy: 98.0%, Test Loss: 3.4419378409018884, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1473/2000, Train Loss: 3.2203987621870196, Train Accuracy: 98.0%, Test Loss: 3.4421452834055972, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1474/2000, Train Loss: 3.220398515951438, Train Accuracy: 98.0%, Test Loss: 3.4421485570760875, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1475/2000, Train Loss: 3.220397683440662, Train Accuracy: 98.0%, Test Loss: 3.4419608116149902, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1476/2000, Train Loss: 3.220397730342677, Train Accuracy: 98.0%, Test Loss: 3.4418883323669434, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1477/2000, Train Loss: 3.2204031944274902, Train Accuracy: 98.0%, Test Loss: 3.4421821649257955, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1478/2000, Train Loss: 3.2204018811710546, Train Accuracy: 98.0%, Test Loss: 3.44192190353687, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1479/2000, Train Loss: 3.2204007477056784, Train Accuracy: 98.0%, Test Loss: 3.44189453125, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1480/2000, Train Loss: 3.220400114528468, Train Accuracy: 98.0%, Test Loss: 3.442169739649846, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1481/2000, Train Loss: 3.2203973785775606, Train Accuracy: 98.0%, Test Loss: 3.4417613561336813, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1482/2000, Train Loss: 3.2203993132857027, Train Accuracy: 98.0%, Test Loss: 3.4423325336896458, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1483/2000, Train Loss: 3.220397488015597, Train Accuracy: 98.0%, Test Loss: 3.4422891598481398, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1484/2000, Train Loss: 3.2203972417800153, Train Accuracy: 98.0%, Test Loss: 3.4418808772013736, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1485/2000, Train Loss: 3.220398172003324, Train Accuracy: 98.0%, Test Loss: 3.4420524010291467, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1486/2000, Train Loss: 3.220402217302166, Train Accuracy: 98.0%, Test Loss: 3.442375293144813, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1487/2000, Train Loss: 3.220399227298674, Train Accuracy: 98.0%, Test Loss: 3.4417283718402567, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1488/2000, Train Loss: 3.2204017443735093, Train Accuracy: 98.0%, Test Loss: 3.441869167181162, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1489/2000, Train Loss: 3.2204005444636112, Train Accuracy: 98.0%, Test Loss: 3.441809443327097, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1490/2000, Train Loss: 3.2204017482820104, Train Accuracy: 98.0%, Test Loss: 3.4419113856095533, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1491/2000, Train Loss: 3.2207181219194756, Train Accuracy: 97.9672131147541%, Test Loss: 3.442180037498474, Test Accuracy: 75.84615384615384%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1492/2000, Train Loss: 3.220730269541506, Train Accuracy: 97.9672131147541%, Test Loss: 3.441740503677955, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1493/2000, Train Loss: 3.2204862813480566, Train Accuracy: 98.0%, Test Loss: 3.4424383365190945, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1494/2000, Train Loss: 3.2205411645232656, Train Accuracy: 98.0%, Test Loss: 3.441468385549692, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1495/2000, Train Loss: 3.2204009431307434, Train Accuracy: 98.0%, Test Loss: 3.4419325498434215, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1496/2000, Train Loss: 3.2203985354939446, Train Accuracy: 98.0%, Test Loss: 3.4419916593111477, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1497/2000, Train Loss: 3.220391914492748, Train Accuracy: 98.0%, Test Loss: 3.4418216393544125, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1498/2000, Train Loss: 3.220101829435005, Train Accuracy: 98.0327868852459%, Test Loss: 3.441978243681101, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1499/2000, Train Loss: 3.2200936450332893, Train Accuracy: 98.0327868852459%, Test Loss: 3.4413267924235416, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1500/2000, Train Loss: 3.2200770417197804, Train Accuracy: 98.0327868852459%, Test Loss: 3.4419421416062574, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1501/2000, Train Loss: 3.2200753102537063, Train Accuracy: 98.0327868852459%, Test Loss: 3.441862161342914, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1502/2000, Train Loss: 3.220074020448278, Train Accuracy: 98.0327868852459%, Test Loss: 3.44195988545051, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1503/2000, Train Loss: 3.2200732504735226, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420912907673764, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1504/2000, Train Loss: 3.2200727462768555, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420944360586314, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1505/2000, Train Loss: 3.220072593845305, Train Accuracy: 98.0327868852459%, Test Loss: 3.442175406676072, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1506/2000, Train Loss: 3.2200724804987675, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421384793061476, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1507/2000, Train Loss: 3.220072175635666, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421275028815637, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1508/2000, Train Loss: 3.2200722225376817, Train Accuracy: 98.0327868852459%, Test Loss: 3.442148969723628, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1509/2000, Train Loss: 3.2200721912696713, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421737835957456, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1510/2000, Train Loss: 3.2200719958446067, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421875935334425, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1511/2000, Train Loss: 3.2200721795441676, Train Accuracy: 98.0327868852459%, Test Loss: 3.44216474202963, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1512/2000, Train Loss: 3.2200720505636244, Train Accuracy: 98.0327868852459%, Test Loss: 3.442228308090797, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1513/2000, Train Loss: 3.220072093557139, Train Accuracy: 98.0327868852459%, Test Loss: 3.442196534230159, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1514/2000, Train Loss: 3.220072199086674, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421942784236026, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1515/2000, Train Loss: 3.2200718864065703, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421597627493052, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1516/2000, Train Loss: 3.220071874681066, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421869149574866, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1517/2000, Train Loss: 3.2200720740146322, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422128659028273, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1518/2000, Train Loss: 3.22007179651104, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422008165946374, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1519/2000, Train Loss: 3.2200719645765963, Train Accuracy: 98.0327868852459%, Test Loss: 3.442218386209928, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1520/2000, Train Loss: 3.220071894223573, Train Accuracy: 98.0327868852459%, Test Loss: 3.44217600272252, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1521/2000, Train Loss: 3.2200718473215573, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422597610033474, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1522/2000, Train Loss: 3.2200718121450453, Train Accuracy: 98.0327868852459%, Test Loss: 3.442138195037842, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1523/2000, Train Loss: 3.220071831687552, Train Accuracy: 98.0327868852459%, Test Loss: 3.442250508528489, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1524/2000, Train Loss: 3.220071972393599, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421972769957323, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1525/2000, Train Loss: 3.2200719059490766, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421703540361843, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1526/2000, Train Loss: 3.2200717066155105, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421960482230554, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1527/2000, Train Loss: 3.2200715659094636, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421881162203274, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1528/2000, Train Loss: 3.220071937217087, Train Accuracy: 98.0327868852459%, Test Loss: 3.442145054156964, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1529/2000, Train Loss: 3.2200718903150714, Train Accuracy: 98.0327868852459%, Test Loss: 3.442227712044349, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1530/2000, Train Loss: 3.2200717183410146, Train Accuracy: 98.0327868852459%, Test Loss: 3.442186557329618, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1531/2000, Train Loss: 3.2200719020405755, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421350405766415, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1532/2000, Train Loss: 3.2200722029951754, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420522084602942, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1533/2000, Train Loss: 3.2200723007077077, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422162771224976, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1534/2000, Train Loss: 3.2200719841191026, Train Accuracy: 98.0327868852459%, Test Loss: 3.442140450844398, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1535/2000, Train Loss: 3.220071948942591, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421241833613467, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1536/2000, Train Loss: 3.220072515675279, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420268077116747, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1537/2000, Train Loss: 3.2200726446558217, Train Accuracy: 98.0327868852459%, Test Loss: 3.442312662418072, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1538/2000, Train Loss: 3.2200729182509127, Train Accuracy: 98.0327868852459%, Test Loss: 3.4419416189193726, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1539/2000, Train Loss: 3.2200736295981485, Train Accuracy: 98.0327868852459%, Test Loss: 3.441901894716116, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1540/2000, Train Loss: 3.2200737898467016, Train Accuracy: 98.0327868852459%, Test Loss: 3.441643008818993, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1541/2000, Train Loss: 3.220074337036883, Train Accuracy: 98.0327868852459%, Test Loss: 3.442838430404663, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1542/2000, Train Loss: 3.2200759082544046, Train Accuracy: 98.0327868852459%, Test Loss: 3.442281585473281, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1543/2000, Train Loss: 3.220076400725568, Train Accuracy: 98.0327868852459%, Test Loss: 3.4412914422842174, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1544/2000, Train Loss: 3.2200766196016404, Train Accuracy: 98.0327868852459%, Test Loss: 3.442106301967914, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1545/2000, Train Loss: 3.220078288531694, Train Accuracy: 98.0327868852459%, Test Loss: 3.4417027693528395, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1546/2000, Train Loss: 3.220078702832832, Train Accuracy: 98.0327868852459%, Test Loss: 3.441201631839459, Test Accuracy: 76.0%\n",
      "Epoch 1547/2000, Train Loss: 3.2200797542196806, Train Accuracy: 98.0327868852459%, Test Loss: 3.442633968133193, Test Accuracy: 75.76923076923077%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1548/2000, Train Loss: 3.2200809775805865, Train Accuracy: 98.0327868852459%, Test Loss: 3.4424459017240086, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1549/2000, Train Loss: 3.2200777257075077, Train Accuracy: 98.0327868852459%, Test Loss: 3.441999462934641, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1550/2000, Train Loss: 3.220080051265779, Train Accuracy: 98.0327868852459%, Test Loss: 3.442704466673044, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1551/2000, Train Loss: 3.2200779328580764, Train Accuracy: 98.0327868852459%, Test Loss: 3.4432677305661716, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1552/2000, Train Loss: 3.220075849626885, Train Accuracy: 98.0327868852459%, Test Loss: 3.4423258121197042, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1553/2000, Train Loss: 3.2200761193134744, Train Accuracy: 98.0327868852459%, Test Loss: 3.442639323381277, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1554/2000, Train Loss: 3.2200769596412533, Train Accuracy: 98.0327868852459%, Test Loss: 3.442234965471121, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1555/2000, Train Loss: 3.220076220934508, Train Accuracy: 98.0327868852459%, Test Loss: 3.4423206586104174, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1556/2000, Train Loss: 3.2200741259778134, Train Accuracy: 98.0327868852459%, Test Loss: 3.442125549683204, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1557/2000, Train Loss: 3.22007345371559, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422024121651282, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1558/2000, Train Loss: 3.220072972969931, Train Accuracy: 98.0327868852459%, Test Loss: 3.442245914385869, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1559/2000, Train Loss: 3.220073160577993, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422924426885753, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1560/2000, Train Loss: 3.2200719763021, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422470331192017, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1561/2000, Train Loss: 3.2200738289317146, Train Accuracy: 98.0327868852459%, Test Loss: 3.4423843897306003, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1562/2000, Train Loss: 3.2200742315073483, Train Accuracy: 98.0327868852459%, Test Loss: 3.4423346244371853, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1563/2000, Train Loss: 3.2200720857401364, Train Accuracy: 98.0327868852459%, Test Loss: 3.44218582373399, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1564/2000, Train Loss: 3.2200711828763366, Train Accuracy: 98.0327868852459%, Test Loss: 3.4423710199502797, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1565/2000, Train Loss: 3.2200718512300583, Train Accuracy: 98.0327868852459%, Test Loss: 3.44226833490225, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1566/2000, Train Loss: 3.2200714056609105, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422963582552395, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1567/2000, Train Loss: 3.220071972393599, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421717662077684, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1568/2000, Train Loss: 3.2200714447459236, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422513521634617, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1569/2000, Train Loss: 3.220072437505253, Train Accuracy: 98.0327868852459%, Test Loss: 3.442313102575449, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1570/2000, Train Loss: 3.2200721521846583, Train Accuracy: 98.0327868852459%, Test Loss: 3.442194250913767, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1571/2000, Train Loss: 3.220071581543469, Train Accuracy: 98.0327868852459%, Test Loss: 3.442488642839285, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1572/2000, Train Loss: 3.2200723397927207, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422058142148533, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1573/2000, Train Loss: 3.220071870772565, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422734333918643, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1574/2000, Train Loss: 3.220071698798508, Train Accuracy: 98.0327868852459%, Test Loss: 3.4424600509496837, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1575/2000, Train Loss: 3.2200726798323336, Train Accuracy: 98.0327868852459%, Test Loss: 3.44221307681157, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1576/2000, Train Loss: 3.2200733481860553, Train Accuracy: 98.0327868852459%, Test Loss: 3.441717872252831, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1577/2000, Train Loss: 3.220432856043831, Train Accuracy: 98.0%, Test Loss: 3.4424525132546058, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1578/2000, Train Loss: 3.2202436845810687, Train Accuracy: 98.0%, Test Loss: 3.442332203571613, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1579/2000, Train Loss: 3.221891360204728, Train Accuracy: 97.8688524590164%, Test Loss: 3.4437684462620664, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1580/2000, Train Loss: 3.2217300954412242, Train Accuracy: 97.8688524590164%, Test Loss: 3.4434013733497033, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1581/2000, Train Loss: 3.221697643154957, Train Accuracy: 97.8688524590164%, Test Loss: 3.4431400299072266, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1582/2000, Train Loss: 3.2207137131300128, Train Accuracy: 97.9672131147541%, Test Loss: 3.44357154919551, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1583/2000, Train Loss: 3.2204175034507374, Train Accuracy: 98.0%, Test Loss: 3.4436514652692356, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1584/2000, Train Loss: 3.22040934250003, Train Accuracy: 98.0%, Test Loss: 3.4429943469854503, Test Accuracy: 75.6923076923077%\n",
      "Epoch 1585/2000, Train Loss: 3.2204030537214434, Train Accuracy: 98.0%, Test Loss: 3.442800521850586, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1586/2000, Train Loss: 3.2203921099178126, Train Accuracy: 98.0%, Test Loss: 3.4425094494452844, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1587/2000, Train Loss: 3.2202404561589977, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420796632766724, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1588/2000, Train Loss: 3.2200793555525484, Train Accuracy: 98.0327868852459%, Test Loss: 3.4417655834784875, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1589/2000, Train Loss: 3.2200735357941173, Train Accuracy: 98.0327868852459%, Test Loss: 3.441982837823721, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1590/2000, Train Loss: 3.2200724414137545, Train Accuracy: 98.0327868852459%, Test Loss: 3.442093253135681, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1591/2000, Train Loss: 3.220071120340316, Train Accuracy: 98.0327868852459%, Test Loss: 3.442156333189744, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1592/2000, Train Loss: 3.22007062005215, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420745005974402, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1593/2000, Train Loss: 3.220070397267576, Train Accuracy: 98.0327868852459%, Test Loss: 3.442067182981051, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1594/2000, Train Loss: 3.2200698891624073, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420059277461124, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1595/2000, Train Loss: 3.2200697210968516, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420464680745053, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1596/2000, Train Loss: 3.2200694553187637, Train Accuracy: 98.0327868852459%, Test Loss: 3.442056756753188, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1597/2000, Train Loss: 3.2200691895406752, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420492007182193, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1598/2000, Train Loss: 3.2200690566516315, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420551520127516, Test Accuracy: 76.0%\n",
      "Epoch 1599/2000, Train Loss: 3.2200689550305976, Train Accuracy: 98.0327868852459%, Test Loss: 3.442036720422598, Test Accuracy: 76.0%\n",
      "Epoch 1600/2000, Train Loss: 3.2200689393965924, Train Accuracy: 98.0327868852459%, Test Loss: 3.442061965282147, Test Accuracy: 76.0%\n",
      "Epoch 1601/2000, Train Loss: 3.2200689159455846, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420619561122012, Test Accuracy: 76.0%\n",
      "Epoch 1602/2000, Train Loss: 3.2200689120370836, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420484487827006, Test Accuracy: 76.0%\n",
      "Epoch 1603/2000, Train Loss: 3.2200688182330524, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420149418023915, Test Accuracy: 76.0%\n",
      "Epoch 1604/2000, Train Loss: 3.220068736154525, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420348772635827, Test Accuracy: 76.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1605/2000, Train Loss: 3.2200687166120185, Train Accuracy: 98.0327868852459%, Test Loss: 3.442045908707839, Test Accuracy: 76.0%\n",
      "Epoch 1606/2000, Train Loss: 3.220068724429021, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420052216603207, Test Accuracy: 76.0%\n",
      "Epoch 1607/2000, Train Loss: 3.220068685344008, Train Accuracy: 98.0327868852459%, Test Loss: 3.441995904995845, Test Accuracy: 76.0%\n",
      "Epoch 1608/2000, Train Loss: 3.220068607173982, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420308333176832, Test Accuracy: 76.0%\n",
      "Epoch 1609/2000, Train Loss: 3.220068525095455, Train Accuracy: 98.0327868852459%, Test Loss: 3.441994942151583, Test Accuracy: 76.0%\n",
      "Epoch 1610/2000, Train Loss: 3.22006863062499, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420134195914636, Test Accuracy: 76.0%\n",
      "Epoch 1611/2000, Train Loss: 3.220068513369951, Train Accuracy: 98.0327868852459%, Test Loss: 3.4419567676690908, Test Accuracy: 76.0%\n",
      "Epoch 1612/2000, Train Loss: 3.2200685016444472, Train Accuracy: 98.0327868852459%, Test Loss: 3.4419883214510403, Test Accuracy: 76.0%\n",
      "Epoch 1613/2000, Train Loss: 3.220068486010442, Train Accuracy: 98.0327868852459%, Test Loss: 3.4419601605488706, Test Accuracy: 76.0%\n",
      "Epoch 1614/2000, Train Loss: 3.2200684156574186, Train Accuracy: 98.0327868852459%, Test Loss: 3.441972952622634, Test Accuracy: 76.0%\n",
      "Epoch 1615/2000, Train Loss: 3.220068364846902, Train Accuracy: 98.0327868852459%, Test Loss: 3.4419512015122633, Test Accuracy: 76.0%\n",
      "Epoch 1616/2000, Train Loss: 3.2200683492128968, Train Accuracy: 98.0327868852459%, Test Loss: 3.4419418023182797, Test Accuracy: 76.0%\n",
      "Epoch 1617/2000, Train Loss: 3.2200685055529483, Train Accuracy: 98.0327868852459%, Test Loss: 3.4419716046406674, Test Accuracy: 76.0%\n",
      "Epoch 1618/2000, Train Loss: 3.220068446925429, Train Accuracy: 98.0327868852459%, Test Loss: 3.441901289499723, Test Accuracy: 76.0%\n",
      "Epoch 1619/2000, Train Loss: 3.22006850946145, Train Accuracy: 98.0327868852459%, Test Loss: 3.4419681567412157, Test Accuracy: 76.0%\n",
      "Epoch 1620/2000, Train Loss: 3.22006841956592, Train Accuracy: 98.0327868852459%, Test Loss: 3.4419450209690976, Test Accuracy: 76.0%\n",
      "Epoch 1621/2000, Train Loss: 3.220068341395894, Train Accuracy: 98.0327868852459%, Test Loss: 3.441980086840116, Test Accuracy: 76.0%\n",
      "Epoch 1622/2000, Train Loss: 3.2200682006898473, Train Accuracy: 98.0327868852459%, Test Loss: 3.441951953447782, Test Accuracy: 76.0%\n",
      "Epoch 1623/2000, Train Loss: 3.220068431291424, Train Accuracy: 98.0327868852459%, Test Loss: 3.441981031344487, Test Accuracy: 76.0%\n",
      "Epoch 1624/2000, Train Loss: 3.220068286676876, Train Accuracy: 98.0327868852459%, Test Loss: 3.441988908327543, Test Accuracy: 76.0%\n",
      "Epoch 1625/2000, Train Loss: 3.2200682436833614, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420924461804905, Test Accuracy: 76.0%\n",
      "Epoch 1626/2000, Train Loss: 3.220068185055842, Train Accuracy: 98.0327868852459%, Test Loss: 3.4418700016461887, Test Accuracy: 76.0%\n",
      "Epoch 1627/2000, Train Loss: 3.2200685368209587, Train Accuracy: 98.0327868852459%, Test Loss: 3.442201696909391, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1628/2000, Train Loss: 3.2200684351999254, Train Accuracy: 98.0327868852459%, Test Loss: 3.44202026954064, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1629/2000, Train Loss: 3.2200682202323536, Train Accuracy: 98.0327868852459%, Test Loss: 3.441956877708435, Test Accuracy: 76.0%\n",
      "Epoch 1630/2000, Train Loss: 3.220068407840416, Train Accuracy: 98.0327868852459%, Test Loss: 3.442157690341656, Test Accuracy: 76.0%\n",
      "Epoch 1631/2000, Train Loss: 3.2200681186113203, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420344737859874, Test Accuracy: 76.0%\n",
      "Epoch 1632/2000, Train Loss: 3.2200687830565404, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421705924547634, Test Accuracy: 76.0%\n",
      "Epoch 1633/2000, Train Loss: 3.2200682788598733, Train Accuracy: 98.0327868852459%, Test Loss: 3.4421539398340077, Test Accuracy: 76.0%\n",
      "Epoch 1634/2000, Train Loss: 3.2200683218533874, Train Accuracy: 98.0327868852459%, Test Loss: 3.442013768049387, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1635/2000, Train Loss: 3.220068626716489, Train Accuracy: 98.0327868852459%, Test Loss: 3.441816339126, Test Accuracy: 76.0%\n",
      "Epoch 1636/2000, Train Loss: 3.2200685446379613, Train Accuracy: 98.0327868852459%, Test Loss: 3.442012906074524, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1637/2000, Train Loss: 3.2200687009780133, Train Accuracy: 98.0327868852459%, Test Loss: 3.4420612775362454, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1638/2000, Train Loss: 3.220069013658117, Train Accuracy: 98.0327868852459%, Test Loss: 3.4419988302084117, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1639/2000, Train Loss: 3.2200698461688932, Train Accuracy: 98.0327868852459%, Test Loss: 3.441533721410311, Test Accuracy: 76.0%\n",
      "Epoch 1640/2000, Train Loss: 3.2200702213850176, Train Accuracy: 98.0327868852459%, Test Loss: 3.441630400144137, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1641/2000, Train Loss: 3.22007131185688, Train Accuracy: 98.0327868852459%, Test Loss: 3.441920280456543, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1642/2000, Train Loss: 3.220071933308586, Train Accuracy: 98.0327868852459%, Test Loss: 3.4418452978134155, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1643/2000, Train Loss: 3.2200714681969314, Train Accuracy: 98.0327868852459%, Test Loss: 3.4422063552416287, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1644/2000, Train Loss: 3.2200723437012218, Train Accuracy: 98.0327868852459%, Test Loss: 3.4414289547846866, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1645/2000, Train Loss: 3.220070670862667, Train Accuracy: 98.0327868852459%, Test Loss: 3.441593491114103, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1646/2000, Train Loss: 3.220155809746414, Train Accuracy: 98.0327868852459%, Test Loss: 3.4395615504338193, Test Accuracy: 76.23076923076923%\n",
      "Saved the best model with validation loss:  3.4395615504338193\n",
      "Epoch 1647/2000, Train Loss: 3.220407329621862, Train Accuracy: 98.0%, Test Loss: 3.4422230537121115, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1648/2000, Train Loss: 3.2200624708269463, Train Accuracy: 98.0327868852459%, Test Loss: 3.4411151500848622, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1649/2000, Train Loss: 3.219973970632084, Train Accuracy: 98.0327868852459%, Test Loss: 3.4408006484691915, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1650/2000, Train Loss: 3.2197861397852665, Train Accuracy: 98.06557377049181%, Test Loss: 3.440724308674152, Test Accuracy: 76.0%\n",
      "Epoch 1651/2000, Train Loss: 3.2197808906680248, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409472667253933, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1652/2000, Train Loss: 3.2197723818607016, Train Accuracy: 98.06557377049181%, Test Loss: 3.4405624041190515, Test Accuracy: 76.0%\n",
      "Epoch 1653/2000, Train Loss: 3.219771107689279, Train Accuracy: 98.06557377049181%, Test Loss: 3.4406653367556057, Test Accuracy: 76.0%\n",
      "Epoch 1654/2000, Train Loss: 3.2197659093825544, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410518316122203, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1655/2000, Train Loss: 3.219770505780079, Train Accuracy: 98.06557377049181%, Test Loss: 3.4408312944265513, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1656/2000, Train Loss: 3.2197685984314464, Train Accuracy: 98.06557377049181%, Test Loss: 3.4411427883001475, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1657/2000, Train Loss: 3.219767777646174, Train Accuracy: 98.06557377049181%, Test Loss: 3.440849551787743, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1658/2000, Train Loss: 3.2197633180461946, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409418289478007, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1659/2000, Train Loss: 3.2197621806723173, Train Accuracy: 98.06557377049181%, Test Loss: 3.441064577836257, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1660/2000, Train Loss: 3.219759331374872, Train Accuracy: 98.06557377049181%, Test Loss: 3.440825178073003, Test Accuracy: 76.0%\n",
      "Epoch 1661/2000, Train Loss: 3.2197611136514634, Train Accuracy: 98.06557377049181%, Test Loss: 3.441035509109497, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1662/2000, Train Loss: 3.2197622783848496, Train Accuracy: 98.06557377049181%, Test Loss: 3.441027907224802, Test Accuracy: 75.92307692307692%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1663/2000, Train Loss: 3.2197620516917747, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409299355286818, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1664/2000, Train Loss: 3.21975938609389, Train Accuracy: 98.06557377049181%, Test Loss: 3.440954327583313, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1665/2000, Train Loss: 3.219759104681797, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409177945210385, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1666/2000, Train Loss: 3.2197567556725173, Train Accuracy: 98.06557377049181%, Test Loss: 3.44111574613131, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1667/2000, Train Loss: 3.2197590108777656, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410182054226217, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1668/2000, Train Loss: 3.219756384364894, Train Accuracy: 98.06557377049181%, Test Loss: 3.44113566325261, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1669/2000, Train Loss: 3.219758068928953, Train Accuracy: 98.06557377049181%, Test Loss: 3.4408475252298207, Test Accuracy: 76.0%\n",
      "Epoch 1670/2000, Train Loss: 3.2197561693973227, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410944901979885, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1671/2000, Train Loss: 3.21975676348952, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410468431619496, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1672/2000, Train Loss: 3.219761508410094, Train Accuracy: 98.06557377049181%, Test Loss: 3.4411564606886644, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1673/2000, Train Loss: 3.2197550242064428, Train Accuracy: 98.06557377049181%, Test Loss: 3.44088091300084, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1674/2000, Train Loss: 3.2197540627151238, Train Accuracy: 98.06557377049181%, Test Loss: 3.4412002746875467, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1675/2000, Train Loss: 3.2197549812129287, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410046889231753, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1676/2000, Train Loss: 3.2197528588967246, Train Accuracy: 98.06557377049181%, Test Loss: 3.441153407096863, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1677/2000, Train Loss: 3.2197535819694645, Train Accuracy: 98.06557377049181%, Test Loss: 3.4411775790728054, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1678/2000, Train Loss: 3.219753128583314, Train Accuracy: 98.06557377049181%, Test Loss: 3.441122816159175, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1679/2000, Train Loss: 3.219752952700756, Train Accuracy: 98.06557377049181%, Test Loss: 3.441035747528076, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1680/2000, Train Loss: 3.2197528198117116, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410770122821512, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1681/2000, Train Loss: 3.2197513306727177, Train Accuracy: 98.06557377049181%, Test Loss: 3.441144053752606, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1682/2000, Train Loss: 3.2197516824378343, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409916951106143, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1683/2000, Train Loss: 3.2197524211445794, Train Accuracy: 98.06557377049181%, Test Loss: 3.4411458327220035, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1684/2000, Train Loss: 3.2197516863463354, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410469348614035, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1685/2000, Train Loss: 3.2197523468830545, Train Accuracy: 98.06557377049181%, Test Loss: 3.441020048581637, Test Accuracy: 76.0%\n",
      "Epoch 1686/2000, Train Loss: 3.219751823143881, Train Accuracy: 98.06557377049181%, Test Loss: 3.4411231187673716, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1687/2000, Train Loss: 3.2197510688031308, Train Accuracy: 98.06557377049181%, Test Loss: 3.441203612547654, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1688/2000, Train Loss: 3.2197503300963857, Train Accuracy: 98.06557377049181%, Test Loss: 3.4411235405848575, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1689/2000, Train Loss: 3.2197511039796423, Train Accuracy: 98.06557377049181%, Test Loss: 3.441087612738976, Test Accuracy: 76.0%\n",
      "Epoch 1690/2000, Train Loss: 3.219750349638892, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410145649543176, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1691/2000, Train Loss: 3.219749270892534, Train Accuracy: 98.06557377049181%, Test Loss: 3.4411808252334595, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1692/2000, Train Loss: 3.219750021324783, Train Accuracy: 98.06557377049181%, Test Loss: 3.441114379809453, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1693/2000, Train Loss: 3.219750388723905, Train Accuracy: 98.06557377049181%, Test Loss: 3.441443140690143, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1694/2000, Train Loss: 3.219749677376669, Train Accuracy: 98.06557377049181%, Test Loss: 3.440858483314514, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1695/2000, Train Loss: 3.2197499392462556, Train Accuracy: 98.06557377049181%, Test Loss: 3.441101257617657, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1696/2000, Train Loss: 3.2197508421100554, Train Accuracy: 98.06557377049181%, Test Loss: 3.4412739918782163, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1697/2000, Train Loss: 3.2197492552585287, Train Accuracy: 98.06557377049181%, Test Loss: 3.440706207202031, Test Accuracy: 76.0%\n",
      "Epoch 1698/2000, Train Loss: 3.2197497633636973, Train Accuracy: 98.06557377049181%, Test Loss: 3.441804418197045, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1699/2000, Train Loss: 3.2197497985402093, Train Accuracy: 98.06557377049181%, Test Loss: 3.4412636848596425, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1700/2000, Train Loss: 3.219750443442923, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409965185018687, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1701/2000, Train Loss: 3.219750244109357, Train Accuracy: 98.06557377049181%, Test Loss: 3.4412859036372256, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1702/2000, Train Loss: 3.219751416659746, Train Accuracy: 98.06557377049181%, Test Loss: 3.441386442918044, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1703/2000, Train Loss: 3.219752456321091, Train Accuracy: 98.06557377049181%, Test Loss: 3.4406021650020895, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1704/2000, Train Loss: 3.2197519599414264, Train Accuracy: 98.06557377049181%, Test Loss: 3.44097821529095, Test Accuracy: 76.0%\n",
      "Epoch 1705/2000, Train Loss: 3.2197543050422044, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409113572194028, Test Accuracy: 76.0%\n",
      "Epoch 1706/2000, Train Loss: 3.219752530582616, Train Accuracy: 98.06557377049181%, Test Loss: 3.441504029127268, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1707/2000, Train Loss: 3.219752706465174, Train Accuracy: 98.06557377049181%, Test Loss: 3.4413488919918356, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1708/2000, Train Loss: 3.2197535077079396, Train Accuracy: 98.06557377049181%, Test Loss: 3.4412849958126364, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1709/2000, Train Loss: 3.2197522999810393, Train Accuracy: 98.06557377049181%, Test Loss: 3.441411935366117, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1710/2000, Train Loss: 3.2197505919659726, Train Accuracy: 98.06557377049181%, Test Loss: 3.441399675149184, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1711/2000, Train Loss: 3.2197501073118118, Train Accuracy: 98.06557377049181%, Test Loss: 3.4405564619944644, Test Accuracy: 76.0%\n",
      "Epoch 1712/2000, Train Loss: 3.2197512564111928, Train Accuracy: 98.06557377049181%, Test Loss: 3.441266582562373, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1713/2000, Train Loss: 3.2197485400027914, Train Accuracy: 98.06557377049181%, Test Loss: 3.4407671139790463, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1714/2000, Train Loss: 3.2197479302765895, Train Accuracy: 98.06557377049181%, Test Loss: 3.4411176443099976, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1715/2000, Train Loss: 3.2197473909034104, Train Accuracy: 98.06557377049181%, Test Loss: 3.4411201660449686, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1716/2000, Train Loss: 3.219747386994909, Train Accuracy: 98.06557377049181%, Test Loss: 3.4404309529524584, Test Accuracy: 76.0%\n",
      "Epoch 1717/2000, Train Loss: 3.2197471173083194, Train Accuracy: 98.06557377049181%, Test Loss: 3.4405965071458082, Test Accuracy: 76.0%\n",
      "Epoch 1718/2000, Train Loss: 3.21974642550359, Train Accuracy: 98.06557377049181%, Test Loss: 3.4408818116554847, Test Accuracy: 76.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1719/2000, Train Loss: 3.2197470078702835, Train Accuracy: 98.06557377049181%, Test Loss: 3.440800795188317, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1720/2000, Train Loss: 3.219746159725502, Train Accuracy: 98.06557377049181%, Test Loss: 3.441137799849877, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1721/2000, Train Loss: 3.2204348298369863, Train Accuracy: 98.0%, Test Loss: 3.441997528076172, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1722/2000, Train Loss: 3.2204168546395224, Train Accuracy: 98.0%, Test Loss: 3.4418225655188928, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1723/2000, Train Loss: 3.2204101632853024, Train Accuracy: 98.0%, Test Loss: 3.441848709033086, Test Accuracy: 75.76923076923077%\n",
      "Epoch 1724/2000, Train Loss: 3.2204088383033627, Train Accuracy: 98.0%, Test Loss: 3.4407835648610043, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1725/2000, Train Loss: 3.220409713807653, Train Accuracy: 98.0%, Test Loss: 3.4406784222676206, Test Accuracy: 76.0%\n",
      "Epoch 1726/2000, Train Loss: 3.220407181098813, Train Accuracy: 98.0%, Test Loss: 3.440523019203773, Test Accuracy: 76.0%\n",
      "Epoch 1727/2000, Train Loss: 3.220407325713361, Train Accuracy: 98.0%, Test Loss: 3.441105164014376, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1728/2000, Train Loss: 3.2204054613582422, Train Accuracy: 98.0%, Test Loss: 3.440525926076449, Test Accuracy: 76.0%\n",
      "Epoch 1729/2000, Train Loss: 3.2204047890960195, Train Accuracy: 98.0%, Test Loss: 3.440936877177312, Test Accuracy: 76.0%\n",
      "Epoch 1730/2000, Train Loss: 3.2203997823058583, Train Accuracy: 98.0%, Test Loss: 3.440718439909128, Test Accuracy: 76.0%\n",
      "Epoch 1731/2000, Train Loss: 3.220384961268941, Train Accuracy: 98.0%, Test Loss: 3.4410700981433573, Test Accuracy: 76.0%\n",
      "Epoch 1732/2000, Train Loss: 3.2198152151264128, Train Accuracy: 98.06557377049181%, Test Loss: 3.440680797283466, Test Accuracy: 76.0%\n",
      "Epoch 1733/2000, Train Loss: 3.2197540627151238, Train Accuracy: 98.06557377049181%, Test Loss: 3.4412883795224705, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1734/2000, Train Loss: 3.219749231807521, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410533813329844, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1735/2000, Train Loss: 3.2197471798443402, Train Accuracy: 98.06557377049181%, Test Loss: 3.441003056672903, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1736/2000, Train Loss: 3.219747082131808, Train Accuracy: 98.06557377049181%, Test Loss: 3.441068310004014, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1737/2000, Train Loss: 3.2197461831765097, Train Accuracy: 98.06557377049181%, Test Loss: 3.4412713601039004, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1738/2000, Train Loss: 3.219746226170024, Train Accuracy: 98.06557377049181%, Test Loss: 3.441361739085271, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1739/2000, Train Loss: 3.2197452998552167, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410513089253354, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1740/2000, Train Loss: 3.2197449050965856, Train Accuracy: 98.06557377049181%, Test Loss: 3.441027127779447, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1741/2000, Train Loss: 3.219744537697464, Train Accuracy: 98.06557377049181%, Test Loss: 3.441204602901752, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1742/2000, Train Loss: 3.219744264102373, Train Accuracy: 98.06557377049181%, Test Loss: 3.441128529035128, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1743/2000, Train Loss: 3.219744092128316, Train Accuracy: 98.06557377049181%, Test Loss: 3.441087236771217, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1744/2000, Train Loss: 3.219744037409298, Train Accuracy: 98.06557377049181%, Test Loss: 3.441058177214402, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1745/2000, Train Loss: 3.2197439162457577, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410886581127462, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1746/2000, Train Loss: 3.2197438498012354, Train Accuracy: 98.06557377049181%, Test Loss: 3.4411027156389675, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1747/2000, Train Loss: 3.219743912337256, Train Accuracy: 98.06557377049181%, Test Loss: 3.441036609502939, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1748/2000, Train Loss: 3.2197437989907187, Train Accuracy: 98.06557377049181%, Test Loss: 3.441024798613328, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1749/2000, Train Loss: 3.2197437442717005, Train Accuracy: 98.06557377049181%, Test Loss: 3.441020103601309, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1750/2000, Train Loss: 3.219743775539711, Train Accuracy: 98.06557377049181%, Test Loss: 3.441017040839562, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1751/2000, Train Loss: 3.219743630925163, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409786095986, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1752/2000, Train Loss: 3.219743568389142, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409978206341085, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1753/2000, Train Loss: 3.2197436465591682, Train Accuracy: 98.06557377049181%, Test Loss: 3.440949696760911, Test Accuracy: 76.0%\n",
      "Epoch 1754/2000, Train Loss: 3.2197436856441812, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409791689652662, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1755/2000, Train Loss: 3.2197437051866875, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409401508478017, Test Accuracy: 76.0%\n",
      "Epoch 1756/2000, Train Loss: 3.2197436387421656, Train Accuracy: 98.06557377049181%, Test Loss: 3.4409765555308414, Test Accuracy: 76.0%\n",
      "Epoch 1757/2000, Train Loss: 3.2197436074741552, Train Accuracy: 98.06557377049181%, Test Loss: 3.440909404021043, Test Accuracy: 76.0%\n",
      "Epoch 1758/2000, Train Loss: 3.2197434824021136, Train Accuracy: 98.06557377049181%, Test Loss: 3.4408848469073954, Test Accuracy: 76.0%\n",
      "Epoch 1759/2000, Train Loss: 3.219743619199659, Train Accuracy: 98.06557377049181%, Test Loss: 3.4408285251030555, Test Accuracy: 76.0%\n",
      "Epoch 1760/2000, Train Loss: 3.2197435058531214, Train Accuracy: 98.06557377049181%, Test Loss: 3.4408266911139855, Test Accuracy: 76.0%\n",
      "Epoch 1761/2000, Train Loss: 3.219743537121132, Train Accuracy: 98.06557377049181%, Test Loss: 3.440751791000366, Test Accuracy: 76.0%\n",
      "Epoch 1762/2000, Train Loss: 3.2197435722976433, Train Accuracy: 98.06557377049181%, Test Loss: 3.4407473435768714, Test Accuracy: 76.0%\n",
      "Epoch 1763/2000, Train Loss: 3.219743435500098, Train Accuracy: 98.06557377049181%, Test Loss: 3.440692479793842, Test Accuracy: 76.0%\n",
      "Epoch 1764/2000, Train Loss: 3.219743779448212, Train Accuracy: 98.06557377049181%, Test Loss: 3.440639991026658, Test Accuracy: 76.0%\n",
      "Epoch 1765/2000, Train Loss: 3.219743556663638, Train Accuracy: 98.06557377049181%, Test Loss: 3.4405928758474498, Test Accuracy: 76.0%\n",
      "Epoch 1766/2000, Train Loss: 3.219743615291158, Train Accuracy: 98.06557377049181%, Test Loss: 3.440598671252911, Test Accuracy: 76.0%\n",
      "Epoch 1767/2000, Train Loss: 3.2197435488466355, Train Accuracy: 98.06557377049181%, Test Loss: 3.4405434865217943, Test Accuracy: 76.0%\n",
      "Epoch 1768/2000, Train Loss: 3.2197433456045683, Train Accuracy: 98.06557377049181%, Test Loss: 3.4405173521775465, Test Accuracy: 76.0%\n",
      "Epoch 1769/2000, Train Loss: 3.219743369055576, Train Accuracy: 98.06557377049181%, Test Loss: 3.440528924648578, Test Accuracy: 76.0%\n",
      "Epoch 1770/2000, Train Loss: 3.2197435879316485, Train Accuracy: 98.06557377049181%, Test Loss: 3.4404612412819495, Test Accuracy: 76.0%\n",
      "Epoch 1771/2000, Train Loss: 3.219743552755137, Train Accuracy: 98.06557377049181%, Test Loss: 3.4405068892699022, Test Accuracy: 76.0%\n",
      "Epoch 1772/2000, Train Loss: 3.2197434628596073, Train Accuracy: 98.06557377049181%, Test Loss: 3.4405588003305287, Test Accuracy: 76.0%\n",
      "Epoch 1773/2000, Train Loss: 3.219743408140589, Train Accuracy: 98.06557377049181%, Test Loss: 3.440539305026715, Test Accuracy: 76.0%\n",
      "Epoch 1774/2000, Train Loss: 3.2197434824021136, Train Accuracy: 98.06557377049181%, Test Loss: 3.4403560895186205, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1775/2000, Train Loss: 3.219743779448212, Train Accuracy: 98.06557377049181%, Test Loss: 3.44054822738354, Test Accuracy: 76.0%\n",
      "Epoch 1776/2000, Train Loss: 3.2197436074741552, Train Accuracy: 98.06557377049181%, Test Loss: 3.440351724624634, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1777/2000, Train Loss: 3.219743959239272, Train Accuracy: 98.06557377049181%, Test Loss: 3.4404386373666616, Test Accuracy: 76.15384615384616%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1778/2000, Train Loss: 3.2197439045202536, Train Accuracy: 98.06557377049181%, Test Loss: 3.440512382067167, Test Accuracy: 76.0%\n",
      "Epoch 1779/2000, Train Loss: 3.2197437247291942, Train Accuracy: 98.06557377049181%, Test Loss: 3.4404318149273214, Test Accuracy: 76.0%\n",
      "Epoch 1780/2000, Train Loss: 3.2197441077623212, Train Accuracy: 98.06557377049181%, Test Loss: 3.440151040370648, Test Accuracy: 76.15384615384616%\n",
      "Epoch 1781/2000, Train Loss: 3.2197446236844924, Train Accuracy: 98.06557377049181%, Test Loss: 3.440632517521198, Test Accuracy: 76.0%\n",
      "Epoch 1782/2000, Train Loss: 3.2197449324560945, Train Accuracy: 98.06557377049181%, Test Loss: 3.4410468890116763, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1783/2000, Train Loss: 3.219745131789661, Train Accuracy: 98.06557377049181%, Test Loss: 3.4404730246617246, Test Accuracy: 76.0%\n",
      "Epoch 1784/2000, Train Loss: 3.219746097189481, Train Accuracy: 98.06557377049181%, Test Loss: 3.4399180228893576, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1785/2000, Train Loss: 3.219746554484133, Train Accuracy: 98.06557377049181%, Test Loss: 3.4403953093748827, Test Accuracy: 76.0%\n",
      "Epoch 1786/2000, Train Loss: 3.2197495562131286, Train Accuracy: 98.06557377049181%, Test Loss: 3.440665290905879, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1787/2000, Train Loss: 3.2197490676504668, Train Accuracy: 98.06557377049181%, Test Loss: 3.4403259662481456, Test Accuracy: 76.0%\n",
      "Epoch 1788/2000, Train Loss: 3.2197501972073415, Train Accuracy: 98.06557377049181%, Test Loss: 3.44028363778041, Test Accuracy: 76.0%\n",
      "Epoch 1789/2000, Train Loss: 3.219751494829772, Train Accuracy: 98.06557377049181%, Test Loss: 3.440523936198308, Test Accuracy: 76.0%\n",
      "Epoch 1790/2000, Train Loss: 3.2197501346713207, Train Accuracy: 98.06557377049181%, Test Loss: 3.4403914488278904, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1791/2000, Train Loss: 3.2197510961626397, Train Accuracy: 98.06557377049181%, Test Loss: 3.440153818864089, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1792/2000, Train Loss: 3.2197494155070823, Train Accuracy: 98.06557377049181%, Test Loss: 3.4402821705891538, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1793/2000, Train Loss: 3.2197473674524026, Train Accuracy: 98.06557377049181%, Test Loss: 3.4402741193771362, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1794/2000, Train Loss: 3.2197450262601257, Train Accuracy: 98.06557377049181%, Test Loss: 3.440317933376019, Test Accuracy: 76.0%\n",
      "Epoch 1795/2000, Train Loss: 3.219745319397723, Train Accuracy: 98.06557377049181%, Test Loss: 3.4401143147395206, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1796/2000, Train Loss: 3.2197451396066636, Train Accuracy: 98.06557377049181%, Test Loss: 3.4403775196809034, Test Accuracy: 76.0%\n",
      "Epoch 1797/2000, Train Loss: 3.2197450106261205, Train Accuracy: 98.06557377049181%, Test Loss: 3.44063977094797, Test Accuracy: 76.0%\n",
      "Epoch 1798/2000, Train Loss: 3.2197446627695054, Train Accuracy: 98.06557377049181%, Test Loss: 3.4405586902911844, Test Accuracy: 76.0%\n",
      "Epoch 1799/2000, Train Loss: 3.2197422824922155, Train Accuracy: 98.06557377049181%, Test Loss: 3.4403426188689012, Test Accuracy: 76.0%\n",
      "Epoch 1800/2000, Train Loss: 3.219617140097696, Train Accuracy: 98.06557377049181%, Test Loss: 3.4406121510725756, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1801/2000, Train Loss: 3.219656729307331, Train Accuracy: 98.06557377049181%, Test Loss: 3.441363288806035, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1802/2000, Train Loss: 3.219423786538546, Train Accuracy: 98.09836065573771%, Test Loss: 3.4410753341821523, Test Accuracy: 76.0%\n",
      "Epoch 1803/2000, Train Loss: 3.21942223095503, Train Accuracy: 98.09836065573771%, Test Loss: 3.441122284302345, Test Accuracy: 76.0%\n",
      "Epoch 1804/2000, Train Loss: 3.219420980234615, Train Accuracy: 98.09836065573771%, Test Loss: 3.440908193588257, Test Accuracy: 76.0%\n",
      "Epoch 1805/2000, Train Loss: 3.219419475461616, Train Accuracy: 98.09836065573771%, Test Loss: 3.440877987788274, Test Accuracy: 76.0%\n",
      "Epoch 1806/2000, Train Loss: 3.2194192565855433, Train Accuracy: 98.09836065573771%, Test Loss: 3.440858950981727, Test Accuracy: 76.0%\n",
      "Epoch 1807/2000, Train Loss: 3.219418979081951, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409207564133863, Test Accuracy: 76.0%\n",
      "Epoch 1808/2000, Train Loss: 3.2194188149248966, Train Accuracy: 98.09836065573771%, Test Loss: 3.440902068064763, Test Accuracy: 76.0%\n",
      "Epoch 1809/2000, Train Loss: 3.219418322453733, Train Accuracy: 98.09836065573771%, Test Loss: 3.440899995657114, Test Accuracy: 76.0%\n",
      "Epoch 1810/2000, Train Loss: 3.2194189439054397, Train Accuracy: 98.09836065573771%, Test Loss: 3.440859922995934, Test Accuracy: 76.0%\n",
      "Epoch 1811/2000, Train Loss: 3.219418900911925, Train Accuracy: 98.09836065573771%, Test Loss: 3.440986073934115, Test Accuracy: 76.0%\n",
      "Epoch 1812/2000, Train Loss: 3.2194188149248966, Train Accuracy: 98.09836065573771%, Test Loss: 3.440838621212886, Test Accuracy: 76.0%\n",
      "Epoch 1813/2000, Train Loss: 3.2194182677347154, Train Accuracy: 98.09836065573771%, Test Loss: 3.440938582787147, Test Accuracy: 76.0%\n",
      "Epoch 1814/2000, Train Loss: 3.2194190494349746, Train Accuracy: 98.09836065573771%, Test Loss: 3.440882160113408, Test Accuracy: 76.0%\n",
      "Epoch 1815/2000, Train Loss: 3.219418201290193, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409166391079244, Test Accuracy: 76.0%\n",
      "Epoch 1816/2000, Train Loss: 3.2194184670682815, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408845534691443, Test Accuracy: 76.0%\n",
      "Epoch 1817/2000, Train Loss: 3.2194180097736296, Train Accuracy: 98.09836065573771%, Test Loss: 3.440874319810134, Test Accuracy: 76.0%\n",
      "Epoch 1818/2000, Train Loss: 3.219418080126653, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408685060647817, Test Accuracy: 76.0%\n",
      "Epoch 1819/2000, Train Loss: 3.2194182442837076, Train Accuracy: 98.09836065573771%, Test Loss: 3.4407701675708475, Test Accuracy: 76.0%\n",
      "Epoch 1820/2000, Train Loss: 3.2217517680809147, Train Accuracy: 97.8688524590164%, Test Loss: 3.4405208000769982, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1821/2000, Train Loss: 3.221220618388692, Train Accuracy: 97.90163934426229%, Test Loss: 3.4409595911319437, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1822/2000, Train Loss: 3.2194461236234573, Train Accuracy: 98.09836065573771%, Test Loss: 3.4413806750224185, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1823/2000, Train Loss: 3.219428601812144, Train Accuracy: 98.09836065573771%, Test Loss: 3.4410693370378933, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1824/2000, Train Loss: 3.2194258033252154, Train Accuracy: 98.09836065573771%, Test Loss: 3.440967715703524, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1825/2000, Train Loss: 3.219424892644413, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411822924247155, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1826/2000, Train Loss: 3.219423966329606, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411233571859507, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1827/2000, Train Loss: 3.2194235442114656, Train Accuracy: 98.09836065573771%, Test Loss: 3.440869560608497, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1828/2000, Train Loss: 3.219422649164669, Train Accuracy: 98.09836065573771%, Test Loss: 3.4410329782045803, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1829/2000, Train Loss: 3.219421965176942, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409580322412343, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1830/2000, Train Loss: 3.219421331999732, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409613334215603, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1831/2000, Train Loss: 3.2194204447699373, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409665969701915, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1832/2000, Train Loss: 3.219418623408333, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409150710472693, Test Accuracy: 76.0%\n",
      "Epoch 1833/2000, Train Loss: 3.2194178104400635, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408839574226966, Test Accuracy: 76.0%\n",
      "Epoch 1834/2000, Train Loss: 3.2194177869890557, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409081752483663, Test Accuracy: 76.0%\n",
      "Epoch 1835/2000, Train Loss: 3.219417701002027, Train Accuracy: 98.09836065573771%, Test Loss: 3.440895777482253, Test Accuracy: 76.0%\n",
      "Epoch 1836/2000, Train Loss: 3.2194179824141207, Train Accuracy: 98.09836065573771%, Test Loss: 3.440924204312838, Test Accuracy: 76.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1837/2000, Train Loss: 3.219417845616575, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409077167510986, Test Accuracy: 76.0%\n",
      "Epoch 1838/2000, Train Loss: 3.219417661917014, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408942736112156, Test Accuracy: 76.0%\n",
      "Epoch 1839/2000, Train Loss: 3.219417775263552, Train Accuracy: 98.09836065573771%, Test Loss: 3.440909559910114, Test Accuracy: 76.0%\n",
      "Epoch 1840/2000, Train Loss: 3.2194177674465494, Train Accuracy: 98.09836065573771%, Test Loss: 3.440912411763118, Test Accuracy: 76.0%\n",
      "Epoch 1841/2000, Train Loss: 3.219417673642518, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409023156532874, Test Accuracy: 76.0%\n",
      "Epoch 1842/2000, Train Loss: 3.2194176189235, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408896336188683, Test Accuracy: 76.0%\n",
      "Epoch 1843/2000, Train Loss: 3.219417540753474, Train Accuracy: 98.09836065573771%, Test Loss: 3.440897519771869, Test Accuracy: 76.0%\n",
      "Epoch 1844/2000, Train Loss: 3.2194175329364714, Train Accuracy: 98.09836065573771%, Test Loss: 3.440887313622695, Test Accuracy: 76.0%\n",
      "Epoch 1845/2000, Train Loss: 3.219417501668461, Train Accuracy: 98.09836065573771%, Test Loss: 3.44085785975823, Test Accuracy: 76.0%\n",
      "Epoch 1846/2000, Train Loss: 3.2194178534335776, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409157037734985, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1847/2000, Train Loss: 3.219417724453035, Train Accuracy: 98.09836065573771%, Test Loss: 3.440820886538579, Test Accuracy: 76.0%\n",
      "Epoch 1848/2000, Train Loss: 3.219417579838487, Train Accuracy: 98.09836065573771%, Test Loss: 3.440864948125986, Test Accuracy: 76.0%\n",
      "Epoch 1849/2000, Train Loss: 3.219417474308952, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408822609828067, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1850/2000, Train Loss: 3.2194174430409417, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408320371921244, Test Accuracy: 76.0%\n",
      "Epoch 1851/2000, Train Loss: 3.2194173687794168, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408254531713633, Test Accuracy: 76.0%\n",
      "Epoch 1852/2000, Train Loss: 3.219417923786601, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408555122522206, Test Accuracy: 76.0%\n",
      "Epoch 1853/2000, Train Loss: 3.219417697093526, Train Accuracy: 98.09836065573771%, Test Loss: 3.440848396374629, Test Accuracy: 76.0%\n",
      "Epoch 1854/2000, Train Loss: 3.2194175446619755, Train Accuracy: 98.09836065573771%, Test Loss: 3.440835485091576, Test Accuracy: 76.0%\n",
      "Epoch 1855/2000, Train Loss: 3.2194175055769625, Train Accuracy: 98.09836065573771%, Test Loss: 3.440811670743502, Test Accuracy: 76.0%\n",
      "Epoch 1856/2000, Train Loss: 3.219417736178539, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408358610593357, Test Accuracy: 76.0%\n",
      "Epoch 1857/2000, Train Loss: 3.2194178260740687, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409042230019202, Test Accuracy: 76.0%\n",
      "Epoch 1858/2000, Train Loss: 3.2194176541000115, Train Accuracy: 98.09836065573771%, Test Loss: 3.4407504980380716, Test Accuracy: 76.0%\n",
      "Epoch 1859/2000, Train Loss: 3.2194179316036036, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408612984877367, Test Accuracy: 76.0%\n",
      "Epoch 1860/2000, Train Loss: 3.2194179433291077, Train Accuracy: 98.09836065573771%, Test Loss: 3.440802601667551, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1861/2000, Train Loss: 3.219418095760658, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408400700642514, Test Accuracy: 76.0%\n",
      "Epoch 1862/2000, Train Loss: 3.2194177869890557, Train Accuracy: 98.09836065573771%, Test Loss: 3.44083873125223, Test Accuracy: 76.0%\n",
      "Epoch 1863/2000, Train Loss: 3.219417701002027, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409388303756714, Test Accuracy: 76.0%\n",
      "Epoch 1864/2000, Train Loss: 3.219418459251279, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408239951500525, Test Accuracy: 76.0%\n",
      "Epoch 1865/2000, Train Loss: 3.2194180605841463, Train Accuracy: 98.09836065573771%, Test Loss: 3.440742144217858, Test Accuracy: 76.0%\n",
      "Epoch 1866/2000, Train Loss: 3.2194179667801155, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409099908975453, Test Accuracy: 76.0%\n",
      "Epoch 1867/2000, Train Loss: 3.2194184866107878, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408732927762546, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1868/2000, Train Loss: 3.219418474885284, Train Accuracy: 98.09836065573771%, Test Loss: 3.4407702959500828, Test Accuracy: 76.0%\n",
      "Epoch 1869/2000, Train Loss: 3.2194176775510193, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408919994647684, Test Accuracy: 76.0%\n",
      "Epoch 1870/2000, Train Loss: 3.2194186273168346, Train Accuracy: 98.09836065573771%, Test Loss: 3.440691691178542, Test Accuracy: 76.0%\n",
      "Epoch 1871/2000, Train Loss: 3.2194181895646894, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409599946095395, Test Accuracy: 76.0%\n",
      "Epoch 1872/2000, Train Loss: 3.219418502244793, Train Accuracy: 98.09836065573771%, Test Loss: 3.440820033733661, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1873/2000, Train Loss: 3.219418478793785, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408097083751974, Test Accuracy: 76.0%\n",
      "Epoch 1874/2000, Train Loss: 3.2194187875653877, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409645887521596, Test Accuracy: 76.0%\n",
      "Epoch 1875/2000, Train Loss: 3.219420007017792, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409533463991604, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1876/2000, Train Loss: 3.2194205502994726, Train Accuracy: 98.09836065573771%, Test Loss: 3.4412609797257643, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1877/2000, Train Loss: 3.2194210896726516, Train Accuracy: 98.09836065573771%, Test Loss: 3.440210250707773, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1878/2000, Train Loss: 3.219422082431981, Train Accuracy: 98.09836065573771%, Test Loss: 3.440307773076571, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1879/2000, Train Loss: 3.219421347633737, Train Accuracy: 98.09836065573771%, Test Loss: 3.4405014790021458, Test Accuracy: 76.0%\n",
      "Epoch 1880/2000, Train Loss: 3.219423098642318, Train Accuracy: 98.09836065573771%, Test Loss: 3.440667601732107, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1881/2000, Train Loss: 3.2194234973094504, Train Accuracy: 98.09836065573771%, Test Loss: 3.440637762729938, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1882/2000, Train Loss: 3.2194251623310026, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409998655319214, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1883/2000, Train Loss: 3.2194204564954414, Train Accuracy: 98.09836065573771%, Test Loss: 3.4407368256495547, Test Accuracy: 76.0%\n",
      "Epoch 1884/2000, Train Loss: 3.219419862403244, Train Accuracy: 98.09836065573771%, Test Loss: 3.440828039095952, Test Accuracy: 76.0%\n",
      "Epoch 1885/2000, Train Loss: 3.2194216837648484, Train Accuracy: 98.09836065573771%, Test Loss: 3.4408823618522058, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1886/2000, Train Loss: 3.2194212577382073, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409247178297777, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1887/2000, Train Loss: 3.219421066221644, Train Accuracy: 98.09836065573771%, Test Loss: 3.4415012414638815, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1888/2000, Train Loss: 3.2194222231380274, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409307149740367, Test Accuracy: 76.0%\n",
      "Epoch 1889/2000, Train Loss: 3.219422266131542, Train Accuracy: 98.09836065573771%, Test Loss: 3.4410179945138784, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1890/2000, Train Loss: 3.2194202532533738, Train Accuracy: 98.09836065573771%, Test Loss: 3.440760979285607, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1891/2000, Train Loss: 3.2194213359082333, Train Accuracy: 98.09836065573771%, Test Loss: 3.441071785413302, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1892/2000, Train Loss: 3.2194206011099893, Train Accuracy: 98.09836065573771%, Test Loss: 3.4410755450908956, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1893/2000, Train Loss: 3.21942168767335, Train Accuracy: 98.09836065573771%, Test Loss: 3.441077544138982, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1894/2000, Train Loss: 3.2194242868267122, Train Accuracy: 98.09836065573771%, Test Loss: 3.4409907230964074, Test Accuracy: 76.0%\n",
      "Epoch 1895/2000, Train Loss: 3.2194510522435924, Train Accuracy: 98.09836065573771%, Test Loss: 3.4415565729141235, Test Accuracy: 75.92307692307692%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1896/2000, Train Loss: 3.2195566873081396, Train Accuracy: 98.09836065573771%, Test Loss: 3.440390889461224, Test Accuracy: 76.07692307692308%\n",
      "Epoch 1897/2000, Train Loss: 3.219476414508507, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411924527241635, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1898/2000, Train Loss: 3.2194241343951617, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411038618821363, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1899/2000, Train Loss: 3.2194233800544114, Train Accuracy: 98.09836065573771%, Test Loss: 3.4412164321312537, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1900/2000, Train Loss: 3.2194228836747465, Train Accuracy: 98.09836065573771%, Test Loss: 3.441140156525832, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1901/2000, Train Loss: 3.219422414654591, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411804309258094, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1902/2000, Train Loss: 3.219422047255469, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411501792760997, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1903/2000, Train Loss: 3.2194217736603785, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411687575853787, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1904/2000, Train Loss: 3.219421648588337, Train Accuracy: 98.09836065573771%, Test Loss: 3.441183539537283, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1905/2000, Train Loss: 3.219421543058802, Train Accuracy: 98.09836065573771%, Test Loss: 3.441154810098501, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1906/2000, Train Loss: 3.2194214766142797, Train Accuracy: 98.09836065573771%, Test Loss: 3.441175946822533, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1907/2000, Train Loss: 3.219421371084745, Train Accuracy: 98.09836065573771%, Test Loss: 3.441183484517611, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1908/2000, Train Loss: 3.2194213124572255, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411961482121396, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1909/2000, Train Loss: 3.219421246012703, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411865931290846, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1910/2000, Train Loss: 3.2194211834766824, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411777349618764, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1911/2000, Train Loss: 3.219421027136631, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411938282159658, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1912/2000, Train Loss: 3.2194208668880777, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411760018422055, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1913/2000, Train Loss: 3.219420565933478, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411650529274573, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1914/2000, Train Loss: 3.219419885854252, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411513530291042, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1915/2000, Train Loss: 3.2194185491468086, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411088961821337, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1916/2000, Train Loss: 3.219417134269339, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411014960362363, Test Accuracy: 76.0%\n",
      "Epoch 1917/2000, Train Loss: 3.219417145994843, Train Accuracy: 98.09836065573771%, Test Loss: 3.441091913443345, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1918/2000, Train Loss: 3.219417079550321, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411002489236684, Test Accuracy: 76.0%\n",
      "Epoch 1919/2000, Train Loss: 3.2194171420863418, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411336458646336, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1920/2000, Train Loss: 3.2194171303608377, Train Accuracy: 98.09836065573771%, Test Loss: 3.4410982590455275, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1921/2000, Train Loss: 3.219417161628848, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411626228919396, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1922/2000, Train Loss: 3.219417040465308, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411650529274573, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1923/2000, Train Loss: 3.2194171772628533, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411986882870016, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1924/2000, Train Loss: 3.219417145994843, Train Accuracy: 98.09836065573771%, Test Loss: 3.441163787474999, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1925/2000, Train Loss: 3.219417157720347, Train Accuracy: 98.09836065573771%, Test Loss: 3.441208555148198, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1926/2000, Train Loss: 3.2194173336029053, Train Accuracy: 98.09836065573771%, Test Loss: 3.441113306925847, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1927/2000, Train Loss: 3.2194173257859027, Train Accuracy: 98.09836065573771%, Test Loss: 3.44110251390017, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1928/2000, Train Loss: 3.2194179706886166, Train Accuracy: 98.09836065573771%, Test Loss: 3.441269737023574, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1929/2000, Train Loss: 3.219417634557505, Train Accuracy: 98.09836065573771%, Test Loss: 3.4411506286034217, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1930/2000, Train Loss: 3.2194170091972976, Train Accuracy: 98.09836065573771%, Test Loss: 3.441210471666776, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1931/2000, Train Loss: 3.219107917097748, Train Accuracy: 98.1311475409836%, Test Loss: 3.4407943853965173, Test Accuracy: 76.0%\n",
      "Epoch 1932/2000, Train Loss: 3.2191193025620257, Train Accuracy: 98.1311475409836%, Test Loss: 3.4411151225750265, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1933/2000, Train Loss: 3.219091700725868, Train Accuracy: 98.1311475409836%, Test Loss: 3.440887918839088, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1934/2000, Train Loss: 3.2190923651710888, Train Accuracy: 98.1311475409836%, Test Loss: 3.440985973064716, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1935/2000, Train Loss: 3.219090895574601, Train Accuracy: 98.1311475409836%, Test Loss: 3.4409970320188084, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1936/2000, Train Loss: 3.219090739234549, Train Accuracy: 98.1311475409836%, Test Loss: 3.4410299154428334, Test Accuracy: 76.0%\n",
      "Epoch 1937/2000, Train Loss: 3.2190904421884508, Train Accuracy: 98.1311475409836%, Test Loss: 3.4408327524478617, Test Accuracy: 76.0%\n",
      "Epoch 1938/2000, Train Loss: 3.219090645430518, Train Accuracy: 98.1311475409836%, Test Loss: 3.4412220074580264, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1939/2000, Train Loss: 3.2190906766985283, Train Accuracy: 98.1311475409836%, Test Loss: 3.4412678205049954, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1940/2000, Train Loss: 3.219090395286435, Train Accuracy: 98.1311475409836%, Test Loss: 3.4411918199979343, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1941/2000, Train Loss: 3.219090485181965, Train Accuracy: 98.1311475409836%, Test Loss: 3.4411642368023214, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1942/2000, Train Loss: 3.2190915326603124, Train Accuracy: 98.1311475409836%, Test Loss: 3.4411122432121863, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1943/2000, Train Loss: 3.2190909737446267, Train Accuracy: 98.1311475409836%, Test Loss: 3.4411464654482327, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1944/2000, Train Loss: 3.2190908486725855, Train Accuracy: 98.1311475409836%, Test Loss: 3.440929440351633, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1945/2000, Train Loss: 3.219090301482404, Train Accuracy: 98.1311475409836%, Test Loss: 3.4411541040127096, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1946/2000, Train Loss: 3.219090422645944, Train Accuracy: 98.1311475409836%, Test Loss: 3.4411499316875753, Test Accuracy: 76.0%\n",
      "Epoch 1947/2000, Train Loss: 3.2190897855602327, Train Accuracy: 98.1311475409836%, Test Loss: 3.44116682272691, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1948/2000, Train Loss: 3.2190909033916038, Train Accuracy: 98.1311475409836%, Test Loss: 3.4413473147612352, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1949/2000, Train Loss: 3.219089828553747, Train Accuracy: 98.1311475409836%, Test Loss: 3.44121306676131, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1950/2000, Train Loss: 3.2190899145407754, Train Accuracy: 98.1311475409836%, Test Loss: 3.441266041535598, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1951/2000, Train Loss: 3.219089582318165, Train Accuracy: 98.1311475409836%, Test Loss: 3.441236294232882, Test Accuracy: 75.84615384615384%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1952/2000, Train Loss: 3.2190894885141343, Train Accuracy: 98.1311475409836%, Test Loss: 3.4411540856728187, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1953/2000, Train Loss: 3.2193215557786283, Train Accuracy: 98.1311475409836%, Test Loss: 3.4411437603143544, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1954/2000, Train Loss: 3.2190900083448066, Train Accuracy: 98.1311475409836%, Test Loss: 3.4413523307213416, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1955/2000, Train Loss: 3.2190914544902864, Train Accuracy: 98.1311475409836%, Test Loss: 3.441135149735671, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1956/2000, Train Loss: 3.2190908721235933, Train Accuracy: 98.1311475409836%, Test Loss: 3.4413093145077047, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1957/2000, Train Loss: 3.219090383560931, Train Accuracy: 98.1311475409836%, Test Loss: 3.4410606072499204, Test Accuracy: 76.0%\n",
      "Epoch 1958/2000, Train Loss: 3.2190908525810866, Train Accuracy: 98.1311475409836%, Test Loss: 3.441424323962285, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1959/2000, Train Loss: 3.2190915951963333, Train Accuracy: 98.1311475409836%, Test Loss: 3.4413573283415575, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1960/2000, Train Loss: 3.219090340567417, Train Accuracy: 98.1311475409836%, Test Loss: 3.4409921994576087, Test Accuracy: 76.0%\n",
      "Epoch 1961/2000, Train Loss: 3.2190900083448066, Train Accuracy: 98.1311475409836%, Test Loss: 3.4412828225355883, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1962/2000, Train Loss: 3.2190898090112405, Train Accuracy: 98.1311475409836%, Test Loss: 3.441338979280912, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1963/2000, Train Loss: 3.219089457246124, Train Accuracy: 98.1311475409836%, Test Loss: 3.441157047565167, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1964/2000, Train Loss: 3.2190892500955552, Train Accuracy: 98.1311475409836%, Test Loss: 3.4411820081564097, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1965/2000, Train Loss: 3.2190891328405162, Train Accuracy: 98.1311475409836%, Test Loss: 3.441246747970581, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1966/2000, Train Loss: 3.2190890507619887, Train Accuracy: 98.1311475409836%, Test Loss: 3.441274303656358, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1967/2000, Train Loss: 3.219089105481007, Train Accuracy: 98.1311475409836%, Test Loss: 3.441259475854727, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1968/2000, Train Loss: 3.219089039036485, Train Accuracy: 98.1311475409836%, Test Loss: 3.4412732216028066, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1969/2000, Train Loss: 3.219089140657519, Train Accuracy: 98.1311475409836%, Test Loss: 3.441269122637235, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1970/2000, Train Loss: 3.219089042944986, Train Accuracy: 98.1311475409836%, Test Loss: 3.441295082752521, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1971/2000, Train Loss: 3.2190890507619887, Train Accuracy: 98.1311475409836%, Test Loss: 3.4412910571465125, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1972/2000, Train Loss: 3.2190890273109813, Train Accuracy: 98.1311475409836%, Test Loss: 3.441313541852511, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1973/2000, Train Loss: 3.219089066395994, Train Accuracy: 98.1311475409836%, Test Loss: 3.4413040968088002, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1974/2000, Train Loss: 3.219089082029999, Train Accuracy: 98.1311475409836%, Test Loss: 3.4413217397836537, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1975/2000, Train Loss: 3.2190889921344694, Train Accuracy: 98.1311475409836%, Test Loss: 3.4413159627180834, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1976/2000, Train Loss: 3.219089039036485, Train Accuracy: 98.1311475409836%, Test Loss: 3.441263804068932, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1977/2000, Train Loss: 3.219089015585477, Train Accuracy: 98.1311475409836%, Test Loss: 3.4412956971388597, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1978/2000, Train Loss: 3.219088976500464, Train Accuracy: 98.1311475409836%, Test Loss: 3.441336650114793, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1979/2000, Train Loss: 3.2190890742129965, Train Accuracy: 98.1311475409836%, Test Loss: 3.441321611404419, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1980/2000, Train Loss: 3.2190890312194824, Train Accuracy: 98.1311475409836%, Test Loss: 3.441357667629535, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1981/2000, Train Loss: 3.2190890976640043, Train Accuracy: 98.1311475409836%, Test Loss: 3.441336989402771, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1982/2000, Train Loss: 3.2190890038599735, Train Accuracy: 98.1311475409836%, Test Loss: 3.441343976901128, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1983/2000, Train Loss: 3.2190890546704902, Train Accuracy: 98.1311475409836%, Test Loss: 3.4413125331585226, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1984/2000, Train Loss: 3.2190890312194824, Train Accuracy: 98.1311475409836%, Test Loss: 3.441324206498953, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1985/2000, Train Loss: 3.2190890351279835, Train Accuracy: 98.1311475409836%, Test Loss: 3.441321712273818, Test Accuracy: 76.0%\n",
      "Epoch 1986/2000, Train Loss: 3.2190890937555032, Train Accuracy: 98.1311475409836%, Test Loss: 3.4412177892831655, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1987/2000, Train Loss: 3.2190894025271057, Train Accuracy: 98.1311475409836%, Test Loss: 3.4412869856907773, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1988/2000, Train Loss: 3.2190891758340303, Train Accuracy: 98.1311475409836%, Test Loss: 3.441479884661161, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1989/2000, Train Loss: 3.2190890507619887, Train Accuracy: 98.1311475409836%, Test Loss: 3.441302858866178, Test Accuracy: 76.0%\n",
      "Epoch 1990/2000, Train Loss: 3.219089066395994, Train Accuracy: 98.1311475409836%, Test Loss: 3.4413818212655873, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1991/2000, Train Loss: 3.2190895315076484, Train Accuracy: 98.1311475409836%, Test Loss: 3.4415797270261326, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1992/2000, Train Loss: 3.219089500239638, Train Accuracy: 98.1311475409836%, Test Loss: 3.4414248649890604, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1993/2000, Train Loss: 3.2190894416121187, Train Accuracy: 98.1311475409836%, Test Loss: 3.441520929336548, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1994/2000, Train Loss: 3.2190894806971317, Train Accuracy: 98.1311475409836%, Test Loss: 3.441578672482417, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1995/2000, Train Loss: 3.2190901920443675, Train Accuracy: 98.1311475409836%, Test Loss: 3.4414720076781053, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1996/2000, Train Loss: 3.2190915482943177, Train Accuracy: 98.1311475409836%, Test Loss: 3.4411323345624485, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1997/2000, Train Loss: 3.21908989890677, Train Accuracy: 98.1311475409836%, Test Loss: 3.4414534385387716, Test Accuracy: 75.84615384615384%\n",
      "Epoch 1998/2000, Train Loss: 3.2190904695479596, Train Accuracy: 98.1311475409836%, Test Loss: 3.441394429940444, Test Accuracy: 75.92307692307692%\n",
      "Epoch 1999/2000, Train Loss: 3.2190897855602327, Train Accuracy: 98.1311475409836%, Test Loss: 3.4414203258661122, Test Accuracy: 75.92307692307692%\n",
      "Epoch 2000/2000, Train Loss: 3.2190897542922223, Train Accuracy: 98.1311475409836%, Test Loss: 3.441066338465764, Test Accuracy: 75.92307692307692%\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ensemble_net.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 2000  # Number of epochs\n",
    "\n",
    "best_valid_loss = float('inf')  # Initialize best validation loss to infinity\n",
    "\n",
    "# Lists for tracking metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ensemble_net.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for inputs, labels in train_loader_pseudoLabel:\n",
    "        # Forward pass\n",
    "        outputs = ensemble_net(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        loss.backward()        # Perform backpropagation\n",
    "        optimizer.step()       # Update the weights\n",
    "\n",
    "        # Accumulate loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average train loss and accuracy for the epoch\n",
    "    avg_train_loss = train_loss / len(train_loader_pseudoLabel)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Evaluation\n",
    "    ensemble_net.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader_trueLabel:\n",
    "            outputs = ensemble_net(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average test loss and accuracy for the epoch\n",
    "    avg_test_loss = test_loss / len(test_loader_trueLabel)\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    test_losses.append(avg_test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}%, Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "    # Save the best model\n",
    "    if best_valid_loss - avg_test_loss > 0.001:\n",
    "        best_valid_loss = avg_test_loss\n",
    "        torch.save(ensemble_net.state_dict(), 'savedModels/best_ensemble_model.pth')\n",
    "        print(\"Saved the best model with validation loss: \", best_valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac6793",
   "metadata": {},
   "source": [
    "# Convergence after 400-600 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f32f1879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB8MAAAHgCAYAAAAxC8wFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3xT1f/H8VfS3VKgraUtUmjZAiJLEBQLIhsciH4RURBRFBQQFUH9yhBBUQE3DgQc4MSBskHG9wfKUhRQRKXMliIbutLk/v4ojYQOQkmT3vJ+Ph59SE7Ovfdz0tiTm88ZFsMwDERERERERERERERERERERMoQq68DEBERERERERERERERERER8TQlw0VEREREREREREREREREpMxRMlxERERERERERERERERERMocJcNFRERERERERERERERERKTMUTJcRERERERERERERERERETKHCXDRURERERERERERERERESkzFEyXEREREREREREREREREREyhwlw0VEREREREREREREREREpMxRMlxERERERERERERERERERMocJcNFREREREQ8xGKxuPWzYsWKC7rOmDFjsFgsxTp2xYoVHomhtOvXrx8JCQmFPj9z5ky3fldFneN8rFmzhjFjxnD06NF8z7Vp04Y2bdp45Drnq02bNi7tDQkJ4YorrmDq1Kk4HA6PXuunn34iKSmJChUqYLFYmDp1qkfPLyIiIiIiInI2f18HICIiIiIiUlasXbvW5fEzzzzD999/z/Lly13K69Wrd0HXGTBgAJ06dSrWsU2aNGHt2rUXHIPZde3aNd/vq2XLlvTs2ZNHHnnEWRYUFOSR661Zs4axY8fSr18/Klas6PLcG2+84ZFrFFf16tX56KOPAEhLS2PatGk8/PDDpKSk8Pzzz3vsOv379+fUqVN8/PHHREREeGyggYiIiIiIiEhhlAwXERERERHxkKuuusrlcXR0NFarNV/52dLT0wkNDXX7OlWqVKFKlSrFirF8+fLnjOdiEB0dTXR0dL7ymJgYr78+vh6YEBIS4tLmzp07U7duXV577TXGjx9PQEBAsc9tt9vJyckhKCiILVu2cO+999K5c2dPhI3NZsNiseDvr682REREREREpGBaJl1ERERERMSL2rRpQ4MGDVi1ahWtWrUiNDSU/v37A/DJJ5/QoUMH4uLiCAkJ4bLLLmPkyJGcOnXK5RwFLZOekJBAt27dWLhwIU2aNCEkJIS6devy3nvvudQraJn0fv36Ua5cOf7880+6dOlCuXLliI+P55FHHiErK8vl+L1799KzZ0/Cw8OpWLEid9xxB+vXr8disTBz5swi237w4EEGDRpEvXr1KFeuHJUqVeK6665j9erVLvWSk5OxWCy8+OKLTJ48mcTERMqVK0fLli354Ycf8p135syZ1KlTh6CgIC677DLef//9IuM4Hzt27KB3795UqlTJef7XX3/dpY7D4WD8+PHUqVOHkJAQKlasSMOGDXn55ZeB3N/XY489BkBiYmK+5fLPXib9fNv/zjvvULt2bYKCgqhXrx6zZ88+5zLxRQkICKBp06akp6dz8OBBAFJTUxk4cCBVqlQhMDCQxMRExo4dS05OTr64J02axPjx40lMTCQoKIgZM2ZgsVjIycnhzTffdLY/z5YtW7jxxhuJiIggODiYRo0aMWvWLJeY8t63H3zwAY888giXXnopQUFB/Pnnn8737++//07Hjh0JCwsjLi6O5557DoAffviBa665hrCwMGrXrp3v3CX1vvzxxx/p3r07UVFRBAcHU6NGDYYNG+ZSx533l4iIiIiIiBSfhk+LiIiIiIh4WUpKCn369GHEiBFMmDABqzV3nPKOHTvo0qULw4YNIywsjN9//53nn3+edevW5VtqvSCbN2/mkUceYeTIkcTExPDuu+9yzz33ULNmTa699toij7XZbNxwww3cc889PPLII6xatYpnnnmGChUq8PTTTwNw6tQp2rZty+HDh3n++eepWbMmCxcu5D//+Y9b7T58+DAAo0ePJjY2lpMnT/Lll1/Spk0bli1blm/f7Ndff526des695b+73//S5cuXdi5cycVKlQAchPhd999NzfeeCMvvfQSx44dY8yYMWRlZTlf1+Latm0brVq1omrVqrz00kvExsayaNEihgwZwj///MPo0aMBmDRpEmPGjOGpp57i2muvxWaz8fvvvzv3Bx8wYACHDx/m1VdfZe7cucTFxQHnnhHuTvvffvttBg4cyC233MKUKVM4duwYY8eOzTeI4Xz99ddf+Pv7ExERQWpqKs2bN8dqtfL0009To0YN1q5dy/jx40lOTmbGjBkux77yyivUrl2bF198kfLly1OxYkXWrl1b4DL027dvp1WrVlSqVIlXXnmFqKgoPvzwQ/r168eBAwcYMWKEy7lHjRpFy5YtmTZtGlarlUqVKgG5798ePXpw//3389hjjzF79mxGjRrF8ePH+eKLL3j88cepUqUKr776Kv369aNBgwY0bdoUKJn35aJFi+jevTuXXXYZkydPpmrVqiQnJ7N48WLnedx9f4mIiIiIiMgFMERERERERKRE9O3b1wgLC3MpS0pKMgBj2bJlRR7rcDgMm81mrFy50gCMzZs3O58bPXq0cfbtXLVq1Yzg4GBj165dzrKMjAwjMjLSGDhwoLPs+++/NwDj+++/d4kTMD799FOXc3bp0sWoU6eO8/Hrr79uAMaCBQtc6g0cONAAjBkzZhTZprPl5OQYNpvNaNeunXHzzTc7y3fu3GkAxuWXX27k5OQ4y9etW2cAxpw5cwzDMAy73W5UrlzZaNKkieFwOJz1kpOTjYCAAKNatWrnFQ9gDB482Pm4Y8eORpUqVYxjx4651HvwwQeN4OBg4/Dhw4ZhGEa3bt2MRo0aFXnuF154wQCMnTt35nsuKSnJSEpKcj4+n/bHxsYaLVq0cDnfrl273G5/UlKSUb9+fcNmsxk2m83Yv3+/MXLkSAMwbr31VsMwcn+/5cqVc3lvGYZhvPjiiwZgbN261SXuGjVqGNnZ2fmudfbraxiG0atXLyMoKMjYvXu3S3nnzp2N0NBQ4+jRo4Zh/Pu+vfbaa/OdN+/9+8UXXzjLbDabER0dbQDGpk2bnOWHDh0y/Pz8jOHDhxf6mlzo+9IwDKNGjRpGjRo1jIyMjEKv4+77S0RERERERIpPy6SLiIiIiIh4WUREBNddd12+8r///pvevXsTGxuLn58fAQEBJCUlAfDbb7+d87yNGjWiatWqzsfBwcHUrl2bXbt2nfNYi8VC9+7dXcoaNmzocuzKlSsJDw+nU6dOLvVuv/32c54/z7Rp02jSpAnBwcH4+/sTEBDAsmXLCmxf165d8fPzc4kHcMa0fft29u/fT+/evV2W3a5WrRqtWrVyO6aCZGZmsmzZMm6++WZCQ0PJyclx/nTp0oXMzEzn0tjNmzdn8+bNDBo0iEWLFnH8+PELunYed9qfmprKbbfd5nJc1apVufrqq92+ztatWwkICCAgIIDKlSvz0ksvcccdd/DOO+8A8O2339K2bVsqV67s8jrk7f29cuVKl/PdcMMNbu8zvnz5ctq1a0d8fLxLeb9+/UhPT2ft2rUu5bfcckuB57FYLHTp0sX52N/fn5o1axIXF0fjxo2d5ZGRkVSqVCnf/xOefF/+8ccf/PXXX9xzzz0EBwcXGO/5vL9ERERERESk+JQMFxERERER8bK8ZbLPdPLkSVq3bs2PP/7I+PHjWbFiBevXr2fu3LkAZGRknPO8UVFR+cqCgoLcOjY0NDRf4i4oKIjMzEzn40OHDhETE5Pv2ILKCjJ58mQeeOABWrRowRdffMEPP/zA+vXr6dSpU4Exnt2eoKAg4N/X4tChQwDExsbmO7agsvNx6NAhcnJyePXVV52J4ryfvKTrP//8A+Qu3f3iiy/yww8/0LlzZ6KiomjXrh0bNmy4oBjcbf+F/E4AatSowfr169mwYQNbtmzh6NGjfPjhh84lvw8cOMC8efPyvQ7169cH/n0d8hT0/i7MoUOHCqxfuXJl5/PunLug929gYCCRkZH56gYGBrq8rz39vszbZ71KlSoFxprXLnffXyIiIiIiIlJ82jNcRERERETEy86cxZxn+fLl7N+/nxUrVjhngwPOfadLg6ioKNatW5evPDU11a3jP/zwQ9q0acObb77pUn7ixIlix1PY9d2NqTARERH4+flx5513Mnjw4ALrJCYmArmzkIcPH87w4cM5evQoS5cu5YknnqBjx47s2bOH0NDQC4qlMHntP3DgQL7nzqf9wcHBNGvWrNDnL7nkEho2bMizzz5b4PN5ies8Bb2/CxMVFUVKSkq+8v379zuvXdxzu8vT78vo6GgA9u7dW2id83l/iYiIiIiISPEpGS4iIiIiIlIK5CX58maZ5nnrrbd8EU6BkpKS+PTTT1mwYIFziWyAjz/+2K3jLRZLvvb98ssvrF27Nt8y2e6oU6cOcXFxzJkzh+HDhztfw127drFmzZp8SdrzERoaStu2bfnpp59o2LAhgYGBbh1XsWJFevbsyb59+xg2bBjJycnUq1cv3+xhT6hTpw6xsbF8+umnDB8+3Fm+e/fuC27/mbp168b8+fOpUaMGERERHjlnnnbt2vHll1+yf/9+l3jff/99QkNDueqqqzx6vYJ4+n1Zu3ZtatSowXvvvcfw4cPznRuK//4SERERERGR86NkuIiIiIiISCnQqlUrIiIiuP/++xk9ejQBAQF89NFHbN682dehOfXt25cpU6bQp08fxo8fT82aNVmwYAGLFi0CwGoteieubt268cwzzzB69GiSkpLYvn0748aNIzExkZycnPOOx2q18swzzzBgwABuvvlm7r33Xo4ePcqYMWMueJl0gJdffplrrrmG1q1b88ADD5CQkMCJEyf4888/mTdvHsuXLwege/fuNGjQgGbNmhEdHc2uXbuYOnUq1apVo1atWgBcfvnlznP27duXgIAA6tSpQ3h4eLHjs1qtjB07loEDB9KzZ0/69+/P0aNHGTt2LHFxcef8fbhr3LhxLFmyhFatWjFkyBDq1KlDZmYmycnJzJ8/n2nTphW5JHhRRo8e7dyT/OmnnyYyMpKPPvqI7777jkmTJjmXai9Jnn5fArz++ut0796dq666iocffpiqVauye/duFi1axEcffQS4//4SERERERGR4lMyXEREREREpBSIioriu+++45FHHqFPnz6EhYVx44038sknn9CkSRNfhwdAWFgYy5cvZ9iwYYwYMQKLxUKHDh1444036NKlCxUrVizy+CeffJL09HSmT5/OpEmTqFevHtOmTePLL79kxYoVxYrpnnvuAeD555+nR48eJCQk8MQTT7By5cpinzNPvXr12LRpE8888wxPPfUUaWlpVKxYkVq1ajn3dQZo27YtX3zxBe+++y7Hjx8nNjaW9u3b89///peAgAAA2rRpw6hRo5g1axbvvPMODoeD77//njZt2lxQjPfddx8Wi4VJkyZx8803k5CQwMiRI/n666/ZvXv3BZ07T1xcHBs2bOCZZ57hhRdeYO/evYSHh5OYmEinTp0uaLZ4nTp1WLNmDU888QSDBw8mIyODyy67jBkzZtCvXz+PxH8uJfG+7NixI6tWrWLcuHEMGTKEzMxMqlSpwg033OCs4+77S0RERERERIrPYhiG4esgRERERERExLwmTJjAU089xe7du4s9Q1g85+jRo9SuXZubbrqJt99+29fhiIiIiIiIiPiMZoaLiIiIiIiI21577TUA6tati81mY/ny5bzyyiv06dNHiXAfSE1N5dlnn6Vt27ZERUWxa9cupkyZwokTJxg6dKivwxMRERERERHxKSXDRURERERExG2hoaFMmTKF5ORksrKyqFq1Ko8//jhPPfWUr0O7KAUFBZGcnMygQYM4fPgwoaGhXHXVVUybNo369ev7OjwRERERERERn9Iy6SIiIiIiIiIiIiIiIiIiUuZYfR2AiIiIiIiIiIiIiIiIiIiIpykZLiIiIiIiIiIiIiIiIiIiZY6S4SIiIiIiIiIiIiIiIiIiUuYoGS4iIiIiIiIiIiIiIiIiImWOkuEiIiIiIiIiIiIiIiIiIlLmKBkuIiIiIiIiIiIiIiIiIiJljpLhIiIiIiIiIiIiIiIiIiJS5igZLiIiIiIiIiIiIiIiIiIiZY6S4SIiIiIiIiIiIiIiIiIiUuYoGS4iIiIiIiIiIiIiIiIiImWOkuEiIiIiIiIiIiIiIiIiIlLmKBkuIiIiIiIiIiIiIiIiIiJljpLhIiIiIiIiIiIiIiIiIiJS5igZLiIiIiIiIiIiIiIiIiIiZY6S4SIiIiIiIiIiIiIiIiIiUuYoGS4iIiIiIiIiIiIiIiIiImWOkuEiIiIiIiIiIiIiIiIiIlLmKBkuIiIiIiIiIiIiIiIiIiJljpLhIiIiIiIiIiIiIiIiIiJS5igZLiIiIiIiIiIiIiIiIiIiZY6S4SIiIiIiIiIiIiIiIiIiUuYoGS4iIiIiIiIiIiIiIiIiImWOkuEiIiIiIiIiIiIiIiIiIlLmKBkuIiIiIiIiIiIiIiIiIiJljpLhIiIiIiIiIiIiIiIiIiJS5igZLiIiIiIiIiIiIiIiIiIiZY6S4SIiIiIiIiIiIiIiIiIiUuYoGS4iIiIiIiIiIiIiIiIiImWOkuEiIiIiIiIiIiIiIiIiIlLmKBkuIiIiIiIiIiIiIiIiIiJljpLhIiIiIiIiIiIiIiIiIiJS5igZLiIiIiIiIiIiIiIiIiIiZY6S4SIiIiIiIiIiIiIiIiIiUuYoGS4iIiIiIiIiIiIiIiIiImWOkuEiIiIiIiIiIiIiIiIiIlLmKBkuIiIiIiIiIiIiIiIiIiJljpLhIiIiIiIiIiIiIiIiIiJS5igZLiIiIiIiIiIiIiIiIiIiZY6S4SIiIiIiIiIiIiIiIiIiUuYoGS4iIiIiIiIiIiIiIiIiImWOkuEiIiIiIiIiIiIiIiIiIlLmKBkuIiIiIiIiIiIiIiIiIiJljpLhIiIiIiIiIiIiIiIiIiJS5igZLiIiIiIiIiIiIiIiIiIiZY6S4SLnyWKxuPWzYsWKC7rOmDFjsFgsxTp2xYoVHonhQn3zzTdYLBaioqLIysryaSwiIiJ5vNWXA6SnpzNmzJgCzzVz5kwsFgvJyckXfJ0L8corr2CxWGjQoIFP4xARESkpuo9379qff/65168tIiLiDt3H/3vtDRs2eP3aImbn7+sARMxm7dq1Lo+feeYZvv/+e5YvX+5SXq9evQu6zoABA+jUqVOxjm3SpAlr16694Bgu1PTp0wE4fPgwX331Ff/5z398Go+IiAh4ry+H3JvosWPHAtCmTRuX57p27cratWuJi4u74OtciPfeew+ArVu38uOPP9KiRQufxiMiIuJpuo8XERExN93Hi8iFUDJc5DxdddVVLo+jo6OxWq35ys+Wnp5OaGio29epUqUKVapUKVaM5cuXP2c8JS01NZX58+dz3XXXsWbNGqZPn15qk+Hn+7sRERFzK25f7mnR0dFER0d79Zpn27BhA5s3b6Zr16589913TJ8+vdQmw9Vfi4hIcek+XkRExNx0Hy8iF0LLpIuUgDZt2tCgQQNWrVpFq1atCA0NpX///gB88skndOjQgbi4OEJCQrjssssYOXIkp06dcjlHQcurJSQk0K1bNxYuXEiTJk0ICQmhbt26zhldeQpaXq1fv36UK1eOP//8ky5dulCuXDni4+N55JFH8i1hvnfvXnr27El4eDgVK1bkjjvuYP369VgsFmbOnOnWazBr1ixycnJ4+OGH6dGjB8uWLWPXrl356h09epRHHnmE6tWrExQURKVKlejSpQu///67s05WVhbjxo3jsssuIzg4mKioKNq2bcuaNWsASE5OLjQ2i8XCmDFj8r2umzZtomfPnkRERFCjRg0gNyHQq1cvEhISCAkJISEhgdtvv73AuPft28d9991HfHw8gYGBVK5cmZ49e3LgwAFOnjxJxYoVGThwYL7jkpOT8fPz44UXXnDrdRQREd/Izs5m/Pjx1K1bl6CgIKKjo7n77rs5ePCgS73ly5fTpk0boqKiCAkJoWrVqtxyyy2kp6eTnJzsvEkeO3asc9m2fv36AQUvr5b3GWL9+vW0bt2a0NBQqlevznPPPYfD4XC59tatW+nQoQOhoaFER0czePBgvvvuu/NaGi5vFZfnnnuOVq1a8fHHH5Oenp6vXlH9Xp5z9emFLf9aUD+e97nl119/pUOHDoSHh9OuXTsAlixZwo033kiVKlUIDg6mZs2aDBw4kH/++Sdf3L///ju33347MTExBAUFUbVqVe666y6ysrJITk7G39+fiRMn5jtu1apVWCwWPvvsM7deRxERMT/dx5/bli1buPHGG4mIiCA4OJhGjRoxa9YslzoOh4Px48dTp04dQkJCqFixIg0bNuTll1921jl48KDzc0Xe56yrr76apUuXeiROERG5OF0s9/Hn8r///Y927doRHh5OaGgorVq14rvvvnOpk56ezqOPPkpiYiLBwcFERkbSrFkz5syZ46zz999/06tXLypXrkxQUBAxMTG0a9eOn3/+2SNxiniTZoaLlJCUlBT69OnDiBEjmDBhAlZr7tiTHTt20KVLF4YNG0ZYWBi///47zz//POvWrcu3rEtBNm/ezCOPPMLIkSOJiYnh3Xff5Z577qFmzZpce+21RR5rs9m44YYbuOeee3jkkUdYtWoVzzzzDBUqVODpp58G4NSpU7Rt25bDhw/z/PPPU7NmTRYuXHjes7rfe+894uLi6Ny5MyEhIcyePZuZM2cyevRoZ50TJ05wzTXXkJyczOOPP06LFi04efIkq1atIiUlhbp165KTk0Pnzp1ZvXo1w4YN47rrriMnJ4cffviB3bt306pVq/OKK0+PHj3o1asX999/v/MLjOTkZOrUqUOvXr2IjIwkJSWFN998kyuvvJJt27ZxySWXALkJgSuvvBKbzcYTTzxBw4YNOXToEIsWLeLIkSPExMTQv39/3n77bSZNmkSFChWc133jjTcIDAx0fqkiIiKlj8Ph4MYbb2T16tWMGDGCVq1asWvXLkaPHk2bNm3YsGEDISEhJCcn07VrV1q3bs17771HxYoV2bdvHwsXLiQ7O5u4uDgWLlxIp06duOeeexgwYADAOUeRp6amcscdd/DII48wevRovvzyS0aNGkXlypW56667gNzPGUlJSYSFhfHmm29SqVIl5syZw4MPPuh2OzMyMpgzZw5XXnklDRo0oH///gwYMIDPPvuMvn37Ouu50++506efr+zsbG644QYGDhzIyJEjycnJAeCvv/6iZcuWDBgwgAoVKpCcnMzkyZO55ppr+PXXXwkICAByPzNdc801XHLJJYwbN45atWqRkpLCN998Q3Z2NgkJCdxwww1MmzaNESNG4Ofn57z2a6+9RuXKlbn55pvPO24RETGvi/0+vijbt2+nVatWVKpUiVdeeYWoqCg+/PBD+vXrx4EDBxgxYgQAkyZNYsyYMTz11FNce+212Gw2fv/9d44ePeo815133smmTZt49tlnqV27NkePHmXTpk0cOnTIY/GKiMjF5WK5jz+XlStX0r59exo2bMj06dMJCgrijTfeoHv37syZM8f52WD48OF88MEHjB8/nsaNG3Pq1Cm2bNni0hd36dIFu93OpEmTqFq1Kv/88w9r1qxx6dNFTMMQkQvSt29fIywszKUsKSnJAIxly5YVeazD4TBsNpuxcuVKAzA2b97sfG706NHG2f+LVqtWzQgODjZ27drlLMvIyDAiIyONgQMHOsu+//57AzC+//57lzgB49NPP3U5Z5cuXYw6deo4H7/++usGYCxYsMCl3sCBAw3AmDFjRpFtMgzDWLVqlQEYI0eOdLYzMTHRqFatmuFwOJz1xo0bZwDGkiVLCj3X+++/bwDGO++8U2idnTt3FhobYIwePdr5OO91ffrpp8/ZjpycHOPkyZNGWFiY8fLLLzvL+/fvbwQEBBjbtm0r9Ni//vrLsFqtxpQpU5xlGRkZRlRUlHH33Xef89oiIuI9Z/flc+bMMQDjiy++cKm3fv16AzDeeOMNwzAM4/PPPzcA4+effy703AcPHszXF+WZMWOGARg7d+50luV9hvjxxx9d6tarV8/o2LGj8/Fjjz1mWCwWY+vWrS71OnbsmO8zQGHy+thp06YZhmEYJ06cMMqVK2e0bt3apZ47/Z47fXpBn08Mo+B+PO9zy3vvvVdkG/I+S+3atcsAjK+//tr53HXXXWdUrFjRSEtLO2dMX375pbNs3759hr+/vzF27Ngiry0iIual+3hXedf+7LPPCq3Tq1cvIygoyNi9e7dLeefOnY3Q0FDj6NGjhmEYRrdu3YxGjRoVeb1y5coZw4YNK7KOiIhIUS7G+/i8a69fv77QOldddZVRqVIl48SJE86ynJwco0GDBkaVKlWc3803aNDAuOmmmwo9zz///GMAxtSpU4uMScQstEy6SAmJiIjguuuuy1f+999/07t3b2JjY/Hz8yMgIICkpCQAfvvtt3Oet1GjRlStWtX5ODg4mNq1axe4lPfZLBYL3bt3dylr2LChy7ErV64kPDycTp06udS7/fbbz3n+PHlLrubNfs5bSmbXrl0sW7bMWW/BggXUrl2b66+/vtBzLViwgODgYI/PpL7lllvylZ08eZLHH3+cmjVr4u/vj7+/P+XKlePUqVMuv5sFCxbQtm1bLrvsskLPX716dbp168Ybb7yBYRgAzJ49m0OHDnl0tJ+IiHjet99+S8WKFenevTs5OTnOn0aNGhEbG+tcuqxRo0YEBgZy3333MWvWLP7++2+PXD82NpbmzZu7lBXUXzdo0IB69eq51Dvf/jokJIRevXoBUK5cOW699VZWr17Njh07nPXc6ffc6dOLo6D+Oi0tjfvvv5/4+Hj8/f0JCAigWrVqwL+fpdLT01m5ciW33XZbkSP427RpwxVXXMHrr7/uLJs2bRoWi4X77rvPo20REZHS72K+jz+X5cuX065dO+Lj413K+/XrR3p6OmvXrgWgefPmbN68mUGDBrFo0SKOHz+e71zNmzdn5syZjB8/nh9++AGbzeaxOEVE5OJ0sdzHF+XUqVP8+OOP9OzZk3LlyjnL/fz8uPPOO9m7dy/bt28HcvviBQsWMHLkSFasWEFGRobLuSIjI6lRowYvvPACkydP5qeffsq35LuImSgZLlJC4uLi8pWdPHmS1q1b8+OPPzJ+/HhWrFjB+vXrmTt3LkC+TqcgUVFR+cqCgoLcOjY0NJTg4OB8x2ZmZjofHzp0iJiYmHzHFlRWkBMnTvDZZ5/RvHlzoqOjOXr0KEePHuXmm2/GYrE4E+WQu09YlSpVijzfwYMHqVy5snN5Ok8p6PfTu3dvXnvtNQYMGMCiRYtYt24d69evJzo62uX1dSdugKFDh7Jjxw6WLFkCwOuvv07Lli1p0qSJ5xoiIiIed+DAAY4ePUpgYCABAQEuP6mpqc69qWvUqMHSpUupVKkSgwcPpkaNGtSoUcNlT8zicKevv9D++s8//2TVqlV07doVwzCc/XXPnj0BXPYxdbe/dqdvPB+hoaGUL1/epczhcNChQwfmzp3LiBEjWLZsGevWreOHH34A/v0sdeTIEex2u1sxDRkyhGXLlrF9+3ZsNhvvvPMOPXv2JDY21qPtERGR0u9ivY93x6FDhwp8fSpXrux8HmDUqFG8+OKL/PDDD3Tu3JmoqCjatWvHhg0bnMd88skn9O3bl3fffZeWLVsSGRnJXXfdRWpqqsfiFRGRi8vFcB9/LkeOHMEwDLf661deeYXHH3+cr776irZt2xIZGclNN93kHBhvsVhYtmwZHTt2ZNKkSTRp0oTo6GiGDBnCiRMnPBKviDdpz3CREmKxWPKVLV++nP3797NixQrnKHKgVO2zERUVxbp16/KVu3tTOmfOHNLT01m3bh0RERH5nv/yyy85cuQIERERREdHs3fv3iLPFx0dzf/+9z8cDkehCfG8LwaysrJcyovab+zs38+xY8f49ttvGT16NCNHjnSWZ2Vlcfjw4XwxnStugOuuu44GDRrw2muvUa5cOTZt2sSHH354zuNERMS3LrnkEqKioli4cGGBz4eHhzv/3bp1a1q3bo3dbmfDhg28+uqrDBs2jJiYGOeM65IQFRXFgQMH8pW721+/9957GIbB559/zueff57v+VmzZjF+/Hj8/Pzc7q/PVaew/jrvS4mzFfRZasuWLWzevJmZM2e67Gv+559/utSLjIzEz8/Prf66d+/ePP7447z++utcddVVpKamMnjw4HMeJyIiZc/Feh/v7jVSUlLyle/fvx/I/fwE4O/vz/Dhwxk+fDhHjx5l6dKlPPHEE3Ts2JE9e/YQGhrKJZdcwtSpU5k6dSq7d+/mm2++YeTIkaSlpRX6+UtERKQoF8N9/LlERERgtVrd6q/DwsIYO3YsY8eO5cCBA85Z4t27d+f3338HoFq1as6JbX/88QeffvopY8aMITs7m2nTpnkkZhFv0cxwES/Ku7EOCgpyKX/rrbd8EU6BkpKSOHHiBAsWLHAp//jjj906fvr06YSHh7Ns2TK+//57l58XXniBrKwsPvroIwA6d+7MH3/8wfLlyws9X+fOncnMzGTmzJmF1omJiSE4OJhffvnFpfzrr792K2bI/d0YhpHvd/Puu+9it9vzxfT99987l5UpypAhQ/juu+8YNWoUMTEx3HrrrW7HJCIivtGtWzcOHTqE3W6nWbNm+X7q1KmT7xg/Pz9atGjhXG5706ZNwL99vjszv85HUlISW7ZsYdu2bS7l7vTXdrudWbNmUaNGjXx99ffff88jjzxCSkqK87OAO/2eO316QkICQL7++ptvvjlnzHnc/SwVEhJCUlISn332WaHJ9jzBwcHOJfImT55Mo0aNuPrqq92OSUREyraL4T7eHe3atXMODDjT+++/T2hoKFdddVW+YypWrEjPnj0ZPHgwhw8fJjk5OV+dqlWr8uCDD9K+fXvn5ycREZHzVdbv490RFhZGixYtmDt3rkvsDoeDDz/8kCpVqlC7du18x8XExNCvXz9uv/12tm/fTnp6er46tWvX5qmnnuLyyy9Xfy2mpJnhIl7UqlUrIiIiuP/++xk9ejQBAQF89NFHbN682dehOfXt25cpU6bQp08fxo8fT82aNVmwYAGLFi0CKHK58i1btrBu3ToeeOCBAvdZu/rqq3nppZeYPn06Dz74IMOGDeOTTz7hxhtvZOTIkTRv3pyMjAxWrlxJt27daNu2LbfffjszZszg/vvvZ/v27bRt2xaHw8GPP/7IZZddRq9evbBYLPTp04f33nuPGjVqcMUVV7Bu3Tpmz57tdrvLly/PtddeywsvvMAll1xCQkICK1euZPr06VSsWNGl7rhx41iwYAHXXnstTzzxBJdffjlHjx5l4cKFDB8+nLp16zrr9unTh1GjRrFq1SqeeuopAgMD3Y5JRER8o1evXnz00Ud06dKFoUOH0rx5cwICAti7dy/ff/89N954IzfffDPTpk1j+fLldO3alapVq5KZmelcXjxv7+zw8HCqVavG119/Tbt27YiMjHT2Mxdi2LBhvPfee3Tu3Jlx48YRExPD7NmznSO4i+qvFyxYwP79+3n++edp06ZNvufzVjWZPn063bp1c6vfc6dPj42N5frrr2fixIlERERQrVo1li1b5lxm1h1169alRo0ajBw5EsMwiIyMZN68ec4tSc40efJkrrnmGlq0aMHIkSOpWbMmBw4c4JtvvuGtt95ymRkwaNAgJk2axMaNG3n33XfdjkdERMq+sn4ff6a8bUfOlpSUxOjRo/n2229p27YtTz/9NJGRkXz00Ud89913TJo0iQoVKgDQvXt3GjRoQLNmzYiOjmbXrl1MnTqVatWqUatWLY4dO0bbtm3p3bs3devWJTw8nPXr17Nw4UJ69OjhmRdEREQuOmX9Pv5My5cvL3CAWZcuXZg4cSLt27enbdu2PProowQGBvLGG2+wZcsW5syZ4xzk16JFC7p160bDhg2JiIjgt99+44MPPqBly5aEhobyyy+/8OCDD3LrrbdSq1YtAgMDWb58Ob/88ovLqqoipmGIyAXp27evERYW5lKWlJRk1K9fv8D6a9asMVq2bGmEhoYa0dHRxoABA4xNmzYZgDFjxgxnvdGjRxtn/y9arVo1o2vXrvnOmZSUZCQlJTkff//99wZgfP/990XGWdh1du/ebfTo0cMoV66cER4ebtxyyy3G/PnzDcD4+uuvC3spjGHDhhmA8fPPPxdaZ+TIkQZgbNy40TAMwzhy5IgxdOhQo2rVqkZAQIBRqVIlo2vXrsbvv//uPCYjI8N4+umnjVq1ahmBgYFGVFSUcd111xlr1qxx1jl27JgxYMAAIyYmxggLCzO6d+9uJCcnG4AxevTofO09ePBgvtj27t1r3HLLLUZERIQRHh5udOrUydiyZYtRrVo1o2/fvi519+zZY/Tv39+IjY01AgICjMqVKxu33XabceDAgXzn7devn+Hv72/s3bu30NdFRER8p6A+0mazGS+++KJxxRVXGMHBwUa5cuWMunXrGgMHDjR27NhhGIZhrF271rj55puNatWqGUFBQUZUVJSRlJRkfPPNNy7nWrp0qdG4cWMjKCjIAJx9yowZMwzA2Llzp7NuYZ8h+vbta1SrVs2lbMuWLcb1119vBAcHG5GRkcY999xjzJo1ywCMzZs3F9rem266yQgMDDTS0tIKrdOrVy/D39/fSE1NNQzDvX7PnT49JSXF6NmzpxEZGWlUqFDB6NOnj7Fhw4Z8n4MK+9xiGIaxbds2o3379kZ4eLgRERFh3Hrrrcbu3bvz9fl5dW+99VYjKirKCAwMNKpWrWr069fPyMzMzHfeNm3aGJGRkUZ6enqhr4uIiJQNuo93lXftwn7yYvr111+N7t27GxUqVDACAwONK664wqX9hmEYL730ktGqVSvjkksucfa999xzj5GcnGwYhmFkZmYa999/v9GwYUOjfPnyRkhIiFGnTh1j9OjRxqlTp4qMU0REJM/Fdh9/5rUL+8mLafXq1cZ1111nhIWFGSEhIcZVV11lzJs3z+VcI0eONJo1a2ZEREQYQUFBRvXq1Y2HH37Y+OeffwzDMIwDBw4Y/fr1M+rWrWuEhYUZ5cqVMxo2bGhMmTLFyMnJKTJOkdLIYhiGUbLpdhEpCyZMmMBTTz3F7t27qVKliq/DMY3s7GwSEhK45ppr+PTTT30djoiIlHH33Xcfc+bM4dChQ1qN5DykpaVRrVo1HnroISZNmuTrcERERDxC9/EiIiKln+7jRUqelkkXkXxee+01IHcpUpvNxvLly3nllVfo06ePbqDddPDgQbZv386MGTM4cOCAlo8RERGPGzduHJUrV6Z69eqcPHmSb7/9lnfffVfbcpyHvXv38vfff/PCCy9gtVoZOnSor0MSEREpFt3Hi4iIlH66jxfxDSXDRSSf0NBQpkyZQnJyMllZWVStWpXHH3+cp556ytehmcZ3333H3XffTVxcHG+88QZNmjTxdUgiIlLGBAQE8MILL7B3715ycnKoVasWkydPVkL3PLz77ruMGzeOhIQEPvroIy699FJfhyQiIlIsuo8XEREp/XQfL+IbWiZdRERERERERERERERERETKHKuvAxAREREREREREREREREREfE0JcNFRERERERERERERERERKTMUTJcRERERERERERERERERETKHH9fB1AaOBwO9u/fT3h4OBaLxdfhiIiIuDAMgxMnTlC5cmWsVo1jA/XdIiJSuqnvzk99t4iIlGbqu/NT3y0iIqWdu/23kuHA/v37iY+P93UYIiIiRdqzZw9VqlTxdRilgvpuERExA/Xd/1LfLSIiZmCGvnvixIk88cQTDB06lKlTpxZYZ+7cubz55pv8/PPPZGVlUb9+fcaMGUPHjh3dvo76bhERMYtz9d9KhgPh4eFA7otVvnz5CzqXzWZj8eLFdOjQgYCAAE+E53Vmb4Pi9z2zt8Hs8YP522D2+MGzbTh+/Djx8fHO/krUd5/N7G0we/xg/jaYPX4wfxsUv++p7y5Z6rtdmb0NZo8fzN8Gs8cP5m+D2eMH87fhYuy7169fz9tvv03Dhg2LrLdq1Srat2/PhAkTqFixIjNmzKB79+78+OOPNG7c2K1rqe92ZfY2mD1+MH8bzB4/mL8NZo8fzN8GT8fvbv+tZDg4l3kpX768Rzr20NBQypcvb8o3Ipi/DYrf98zeBrPHD+Zvg9njh5Jpg5Yl+5f6bldmb4PZ4wfzt8Hs8YP526D4fU99d8lS3+3K7G0we/xg/jaYPX4wfxvMHj+Yvw0XW9998uRJ7rjjDt555x3Gjx9fZN2zZ4xPmDCBr7/+mnnz5rmdDFff7crsbTB7/GD+Npg9fjB/G8weP5i/DSUV/7n6b22AIiIiIiIiInKRmjhxIhaLhWHDhhVaZ+7cubRv357o6GjKly9Py5YtWbRokfeCFBEREQYPHkzXrl25/vrrz/tYh8PBiRMniIyMLIHIRERESjfNDBcRERERERG5CHlzqVUREREpvo8//phNmzaxfv36Yh3/0ksvcerUKW677bZC62RlZZGVleV8fPz4cSB3Fp/NZivWdfPkHX+h5/Els7fB7PGD+dtg9vjB/G0we/xg/jZ4On53z+PTZPiqVat44YUX2LhxIykpKXz55ZfcdNNNzucNw2Ds2LG8/fbbHDlyhBYtWvD6669Tv359Z52srCweffRR5syZQ0ZGBu3ateONN94ocqN0ERERERERkYuZt5daFRERkeLZs2cPQ4cOZfHixQQHB5/38XPmzGHMmDF8/fXXVKpUqdB6EydOZOzYsfnKFy9eTGho6HlftyBLlizxyHl8yextMHv8YP42mD1+MH8bzB4/mL8Nnoo/PT3drXo+TYafOnWKK664grvvvptbbrkl3/OTJk1i8uTJzJw5k9q1azN+/Hjat2/P9u3bnZuhDxs2jHnz5vHxxx8TFRXFI488Qrdu3di4cSN+fn7ebpKIiIiIiIhIqXfmUqvnSoafzZ2lVjW7rGhmb4PZ4wfzt8Hs8UPJtMFut5OTk4NhGB47Z2FycnLw9/fn5MmT+Pubc/FNs7fhfOK3WCwEBARgtRa8a2hp/n9p48aNpKWl0bRpU2eZ3W5n1apVvPbaa2RlZRX6Pfgnn3zCPffcw2effXbO5dVHjRrF8OHDnY+PHz9OfHw8HTp08Mie4UuWLKF9+/am3OMWzN8Gs8cP5m+D2eMH77ahJPr0nJwc1qxZQ6tWrUzZ74H523A+8VssFvz9/YvM9ebdZ56LT1+pzp0707lz5wKfMwyDqVOn8uSTT9KjRw8AZs2aRUxMDLNnz2bgwIEcO3aM6dOn88EHHzg78w8//JD4+HiWLl1Kx44dvdYWERERERERETPwxlKrml3mHrO3wezxg/nbYPb4wXNtCA8PJzw8vNBkZ0mIjY3l77//9tr1SoLZ23A+8dtsNg4ePIjD4cj3nLszy3yhXbt2/Prrry5ld999N3Xr1uXxxx8vNEkwZ84c+vfvz5w5c+jates5rxMUFERQUFC+8oCAAI8lvTx5Ll8xexvMHj+Yvw1mjx9Ktg2GYZCamsrRo0dL5NyxsbGkpKRgsVg8fn5vMHsbihN/xYoViY2NLbC+u+/DUjtsYOfOnaSmptKhQwdnWVBQEElJSaxZs4aBAweyceNGbDabS53KlSvToEED1qxZo2S4iIiIiIiIyBm8tdSqZpcVzextMHv8YP42mD1+8GwbDhw4wPHjx4mOjiY0NNQrXw4bhsGpU6cICwsz5ZfRYP42nE/8DoeDlJQUYmJiuPTSS/PVd3dmmS+Eh4fToEEDl7KwsDCioqKc5aNGjWLfvn28//77QG5/fdddd/Hyyy9z1VVXkZqaCkBISAgVKlTwbgNERM5DXiK8UqVKHu/THQ4HJ0+epFy5cl4dPOdJZm/D+cRvGAbp6emkpaUBEBcXV+zrltpkeF4HHRMT41IeExPDrl27nHUCAwOJiIjIVyfv+IJoubaimb0Nit/3zN4Gs8cP5m+D2eMHz7bBzK+DiIiIlC7eWmpVs8vcY/Y2mD1+MH8bzB4/XHgb7HY7J06cICYmhqioKA9GVjSHw4HNZiMkJMSUX0aD+dtwvvFXqlSJ/fv3O5dMP5PZ/z9KSUlh9+7dzsdvvfUWOTk5DB48mMGDBzvL+/bty8yZM30QoYjIudntdmcivCT6dIfDQXZ2NsHBwabs98D8bTjf+ENCQgBIS0ujUqVKxd4eu9Qmw/OcPerDMIxzjgQ5Vx0t1+Yes7dB8fue2dtg9vjB/G0we/zgmTaU5uXaRERExFy8tdSqiIi35A0e9tR3elJ2BQYGArnJFrMnv1esWOHy+OwE99nPi4iYgfp0KUje+8Fms5W9ZHhsbCyQO/v7zKnvaWlpztnisbGxZGdnc+TIEZfZ4WlpabRq1arQc2u5tqKZvQ2K3/dKextOZuYw7OONrP7rGPl3icrjAMw3ssqV2dtg7vgtQJifg3taV+e+pJoE+he/LaV5uTYRETEhhx0Orob0fZB1EIKiIfRSiG4NVr+C62akQEhcwXUKu0bqYtj6Ihz/HezZuccFloeYdtB4MvgHunduhx0OLIOdH0DOSbjkGqg5CI78eP5xSZlYajUj286nf1v4dOYGqkeX44ku9QgJ1O9f5GJnxmW+xbv0HvEhRzbVs7/BumkRVKid+1nOP9DXUYlIKaW/13ImT7wfSm0yPDExkdjYWJYsWULjxo0ByM7OZuXKlTz//PMANG3alICAAJYsWcJtt90G5C4Js2XLFiZNmlToubVcm3vM3gbF73ulsQ03vLqaX/a5k1g0bxL2X2Zvg7njN4CTdisvr0jm5RXJDLw2kVFd6hXrXKXt/yMREa/KS6r+NhlsRyGiWW4iNTDk3+cPLMO6YzpJp9bit/AJiGgE1ftBpSQ4tCY3YRpcCXIy4PepkLEH/GLg5J+Qk3L6Qv4QVhMu7QJZqWC35T6fdRwcGeAfBmFVodaDsP87OLgKrAFQsSkc2Qzpf4FfMFS5OTe+Iz9C6nI4+TdkHgR7BviFQHAMhFSGE9shJx1spyDrMH7pO+mGDctnZzbeD/wq5nYqjpOAI/caYYkQEAkZqZC5H8gGgsEaAjlpgN0rv5qzIqUbnBV/AbIPwMkd8Ne04l9s71fw86PFP74QfljoRDDW9bdDs9f+fY9dhErzUqv3vr+eJdvSAD84cJj/++swH/ywm/b1KvHOXVd6NRYRERFxw08j8P/tRS7HgL9Ol216BC57BBoX/h2+iIiIp/g0GX7y5En+/PNP5+OdO3fy888/ExkZSdWqVRk2bBgTJkygVq1a1KpViwkTJhAaGkrv3r0BqFChAvfccw+PPPIIUVFRREZG8uijj3L55Zefc/8yEbn4JL2wnF2HMnwdhlyk3lq1E6DYCXERkYvSnrnwf3eAI/PfssMbCkyk+gEVAU4AJ7bB7tnnOPkfZz3OgVO/wx+/F1w9Czi1A9KWuZYf3/rvv+0n4e93cn/OU8FDwOxgP3RWkQ2O/1JA3UxwHD3v63qKuYew5bJiEEQGJL+X+3PpjZD0la/D8gqzLLX6byI8vyXb0rj3/fVKiIvIRa1NmzY0atSIqVOnulU/OTmZGjVqsHHjRpo0aVKywcnF6acR8NsLBTzh+LdcCXERkfMye/ZsnnjiCY4ePerrUEzDp99ZbNiwgcaNGztnfg8fPpzGjRvz9NNPAzBixAiGDRvGoEGDaNasGfv27WPx4sWEh4c7zzFlyhRuuukmbrvtNq6++mpCQ0OZN29esdeNF5Gy6asNe5QIF597Z/VOsnMKX5xfRKRMctjhwApInpP7X0chs5bz6u38KHcW+KpbYPUtrolwEW/a9zWsvMnXUchpGdn2QhPheZZsSyMj2/srI4hI2WB3GKz96xBf/7yPtX8dwu4wSuxaFoulyJ9+/foV67xz587lmWeecbt+fHw8v//+e77tMzwtOTkZi8XCzz//XKLXkVImJxt+ewnI3UquQL+9lFtPRMTT3P0uwgNKql8HSEhIyDfI7eabb+b33wsZyO9BM2fOpGLFiiV+HW/w6czwNm3aYBiFf7C0WCyMGTOGMWPGFFonODiYV199lVdffbUEIhQRT8vOcfDWqh28u/JPjmX5OhoR73IY8MHaZO5pXd3XoYiIXDiHHdJWwP7FsG8hnPgDOCNxbQkBv1DIOQJoIJCY1L6vITvjol4yvbSYMH+b2/WeuenyEo5GRMqahVtSGDtvGynH/v0sE1chmNHd69GpQZzHr5eSkuL89yeffMLTTz/N9u3bnWUhIa79js1mc2v7rMjIyPOKw8/Pj5iYGPz9S+1OmmJmO17j3PcBjtx6lw33RkQicrHYMxc2DoX0vf+WhVaBxlOggudXlT7ffv1ChYSEUL58eY+es6wrC6vZiUgpdDIb2r/0PQkjv3P5qf3UAl5arES4XLx2HU73dQgiIud25gjq1GWwey58kQizLf/+fOwPy6+H3yfBiV9wSYQDGBmQcwglwsX0Nj/m6wgESD7k3mcod+uJiORZuCWFBz7c5JIIB0g9lskDH25i4ZaUQo4svtjYWOdPhQoVsFgszseZmZlUrFiRTz/9lDZt2hAcHMyHH37IoUOHuP3226lSpQqhoaFcfvnlzJkzx+W8bdq0YdiwYc7HCQkJTJgwgf79+xMeHk7VqlV5++23nc8nJycTERHhnLG9YsUKLBYLy5Yto1mzZoSGhtKqVSuXL/QBxo8fT6VKlQgPD2fAgAGMHDmSRo0aFfv1yMrKYsiQIVSqVIng4GCuueYa1q9f73z+yJEj3HHHHURHRxMSEkKtWrWYMWMGANnZ2Tz00EPExcURHBxMQkICEydOLHYs4kFpqz1bT0TEHXvmwuqerolwgPR9WP7vNgJS53n8kkX167GxsaxatYqmTZsSHBxM9erVGTt2LDk5Oc7jx4wZQ9WqVQkKCqJy5coMGTIEyO3Xd+3axcMPP+ycZQ65y6SfOQBuzJgxNGrUiA8++ICEhAQqVKhAr169OHHihLPOiRMnuOOOOwgLCyMuLo4pU6bk+9xwvnbv3s2NN95IuXLlKF++PLfddhsHDhxwPr9582batm1LeHg45cuXp2nTpmzYsAGAXbt20b17dyIiIggLC6N+/frMnz+/2LGci5LhIuJxLZ9fwZMb/Ug+avN1KCKlTrXIUF+HICJSMEc21m2T4JOo3ET3srawpnduwvt/t0BWsq8jFPGNEzt8HYEAVSPdm03hbj0RKfvSs3MK/cm05S6VancYjJ23jYLWrcwrGzNvm8uS6WeeJyPb7vy3pz3++OMMGTKE3377jY4dO5KZmUnTpk359ttv2bJlC/fddx933nknP/74Y5Hneemll2jWrBk//fQTgwYN4oEHHjjn0qpPPvkkL730Ehs2bMDf35/+/fs7n/voo4949tlnef7559m4cSNVq1blzTffvKC2jhgxgi+++IJZs2axadMmatasSceOHTl8+DAA//3vf9m2bRsLFizgt99+48033+SSSy4B4K233mLevHl8+umnbN++nQ8//JCEhIQLikc8JCD83HXOp56IXNxyThX+Yz89oM1hz50RXkTPHrJtpOuS6QWdz4MWLVpEnz59GDJkCNu2beOtt95i5syZPPvsswB8/vnnTJkyhbfeeosdO3bw1VdfcfnluStdzZ07lypVqjBu3DhSUlJcZqCf7a+//uKrr77i22+/5dtvv2XlypU899xzzueHDx/O//3f//HNN9+wZMkSVq9ezaZNm4rdLsMwuOmmmzh8+DArV65kyZIl/PXXX/znP/9x1rnjjjuoUqUK69evZ+PGjYwcOdK50s2DDz5IVlYWq1at4tdff+X555+nXLlyxY7nXLQGjoh41JXjl/DPSe31I1IQqwXubJng6zBERPKxbn6cGzKmYNnq60hESqHwWr6OQIAO9WL56Mc9btUTEQGo9/SiQp9rWyeaGXc3Z93Ow/lmhJ/JIHeG+Lqdh2lZIwqAa57/nsOn8n/vkfxc1wuO+UzDhg2jR48eLmWPPvqo898PPfQQCxcu5LPPPqNFixaFnqdLly4MGjQIyE2wT5kyhRUrVlC3bt1Cj3n22WdJSkoCYOTIkXTt2pXMzEzndpX33HMPd999NwBPP/00ixcv5uTJk8Vq56lTp3jzzTeZOXMmnTt3BuCdd95hyZIlTJ8+nccee4zdu3fTuHFjmjVrBuBMdjscDvbu3UutWrW45pprsFgsVKtWrVhxSAlIuBOSP3CvnojIuXxaRKK0chdo8x0cXJ1/RvgZLBhYMvfjOLga4q7LLfw6AbL+ca3Yu/Dtnc/Xs88+y8iRI+nbty8A1atX55lnnmHEiBGMHj2a3bt3Exsby/XXX09AQABVq1alefPmQO72J35+foSHhxMbm3uf43AUvPqew+Fg5syZhIfnDjC68847WbZsGc8++ywnTpxg1qxZzJ49m3bt2gEwY8YMKleuXOx2LV26lF9++YWdO3cSHx8PwAcffED9+vVZv349V155Jbt37+axxx5zfuaoVasWDoeD48ePs2fPHm655RZn4r969ZLdVlQzw0XEI05m5tD7jRUcdCbCLT6NR6Q0urd1IoH+6npFpJRZeRPWP6ao5xYpzBUv+DoCAY5muLfqlLv1REQA0k4UnggvTj1Pykv85rHb7Tz77LM0bNiQqKgoypUrx+LFi9m9e3eR52nYsKHz33nLtqalpbl9TFxc7p7pecds377d+SV9nrMfn4+//voLm83G1Vdf7SwLCAigefPm/PbbbwA88MADfPzxxzRq1IgRI0awZs0aZ93evXvz888/U6dOHYYMGcLixYuLHYt4WOx14H+OWX7+4bn1REQ8IcPNrU0yPb8FSmE2btzIuHHjKFeunPPn3nvvJSUlhfT0dG699VYyMjKoXr069957L19++aXLEuruSkhIcCbCIbf/zuu7//77b2w2m0t/XaFCBerUqVPsdv3222/Ex8c7E+EA9erVo2LFis7+e/jw4QwYMIDrr7+e5557jr/++stZ98EHH2T8+PFcffXVjB49ml9++aXYsbhDM8NF5IJ1e3U1W/Yd93UYIqXawGsTGdWlnq/DEBFxtesT2Pe1r6MQKb0uvRECtex2aVApPNij9USk7Ns2rmOhz1lP77lZnL8t/3u8LZA7A+vE8ROElw/HavX8oOewsDCXxy+99BJTpkxh6tSpXH755YSFhTFs2DCys4tenS9vOdI8Foul0FllBR2Ttz/pmcfkleUxjOLPoMs7tqBz5pV17tyZXbt28d1337F06VLatWvH4MGDmTRpEldccQV//fUXixYtYunSpdx2221cf/31fP7558WOSTzE6ge1HoDfXsCgkGkzte7PrScici63FbECieX035GQOPfOFXxGvRuTix2SOxwOB2PHjs232gtAcHAw8fHxbN++nSVLlrB06VIGDRrECy+8wMqVK/P14UUpqr8vqq8trjP76cLKx4wZQ+/evfnuu+9YsGABo0ePds5OHzBgAJ07d+a7775j8eLFTJw4kZdeeomHHnqo2DEVRclwESnSsXQbfd7+H7+mpvs6FC9zYP7FM8zeBnPHbwHC/Bzc26YWD7StrRnhIlK6OOyw/1v4v16A1nMRKdClN0LSV76OQk5rnhhJXIVgUo9lFrgDoAWIrRBM88RIb4cmIqVUaOC5v/Yszt+WvPM6HA5yAv0IDfQvkWT42VavXs2NN95Inz59nNffsWMHl112WYlf+0x16tRh3bp13Hnnv0tbb9iwodjnq1mzJoGBgfzvf/+jd+/eANhsNjZs2MCwYcOc9aKjo+nXrx/9+vWjdevWPPbYY0yaNAmA8uXL85///If//Oc/9OzZk06dOnH48GEiI9Un+JTDDrvmAEXcb+z6GK6YqIS4iJybf9i560S3htAqkL6PgvYNN7BgBFfOrXc+570ATZo0Yfv27dSsWbPQOiEhIdxwww3ccMMNDB48mLp16/Lrr7/SpEkTAgMDsdvthR7rjho1ahAQEMC6deucM7mPHz/Ojh07nNuinK969eqxe/du9uzZ4zzntm3bOHbsmMtnk9q1a1O7dm0efvhhbr/9dmbOnOlcqj0+Pp7777+f+++/n1GjRvHOO+8oGS4i3nUs3caV4xeTXfRg4TLlt3GdCAn0w2azMX/+fLp06XReo69KE7O3wezxwxltaFODACXCRcSbMo/Bsg5wbAO5A4v8wRoGfgEQXAlCL4UDy04/J+IZDnDO+Mnf6/mf/rEA2cCF3ciXFAcWbATjn3A7fs1e04zwUsbPamF093o88OGmQuuM7l4PP6uG94iI+87822LB9WvzvL8mpeVvS82aNfniiy9Ys2YNERERTJ48mdTUVK8nwx966CHuvfdemjVrRqtWrfjkk0/45Zdf3Nrrc/v27fnK6tWrxwMPPMBjjz1GZGQkVatWZdKkSaSnp3PPPfcAufuSN23alPr165OVlcW3337rbPcbb7xBQkICTZo0wWq18tlnnxEbG0vFihU92m4phnPs3QtA+p7cejFtvBKSiJRxVj9o+jKs7gmF9OwZ9SYS4sUBOE8//TTdunUjPj6eW2+9FavVyi+//MKvv/7K+PHjmTlzJna7nRYtWhAaGsoHH3xASEgI1apVA3KXP1+1ahW9evUiKCioWAO9wsPD6du3r7OvrVSpEqNHj8ZqtRY4u/tMdrudn3/+2aUsMDCQ66+/noYNG3LHHXcwdepUcnJyGDRoEElJSTRr1oyMjAwee+wxevbsSWJiInv37mX9+vXOGfIPP/wwXbp0oXbt2hw5coTly5eX6GcaJcNFJJ+kScvZdTjD12F4Vft6lQgJ1ChUERExsayT8GUlcJzdh+eA41huttL2D5zY5ovoSlzRydizne7zLQEQ0Qiu/gRW3wZHNwJn783lBwQANvIncf34dyWTMCDjdL3z5Y8jJI69mdWo3PoJ/Cu1gR2vwp8zIWNvbqMCKoK1AmTsAuPE6eMCc/dhtJQD+6HcawdFQ9M3oErXwmfYOOy5XzpmpOQuIxfduuC6BdVz2OHPN+DEXxBeA2oOAv9A7M6BbF2wmnQgm91mY+H8+XS5sgt+Jm1DWdepQRz3XZvIO6t34jjjey2rBe5tnUinBm4uiygicoZODeJ4s08Txs7bRsqxf/cGj60QzOju9UrN35b//ve/7Ny5k44dOxIaGsp9993HTTfdxLFjx7waxx133MHff//No48+SmZmJrfddhv9+vVj3bp15zy2V69e+cp27tzJc889h8Ph4M477+TEiRM0a9aMRYsWERERAeR+6T5q1CiSk5MJCQmhdevWfPzxx0DucvIvvPACO3bswM/PjyuvvJL58+d7Zba+nIO7e/e6W09ExB3xPaD157BxqOuAnNAqGI0nY6twPd4c9tyxY0e+/fZbxo0bx6RJkwgICKBu3boMGDAAgIoVK/Lcc88xfPhw7HY7l19+OfPmzSMqKgqAcePGMXDgQGrUqEFWVlaxZ4lPnjyZ+++/n27dulG+fHlGjBjBnj17CA4uesuYkydP0rhxY5eyatWqkZyczFdffcVDDz3Etddei9VqpVOnTrz66qsA+Pn5cejQIe666y4OHDjAJZdcQo8ePRgzZgzZ2dnY7XYGDx7M3r17KV++PJ06dWLKlCnFaps7lAwX8bBj6TbufOf/+CXl1AWcxcrQtYs9FpMUrX29Srxz15W+DkNERMyioAQhgCMb6/apcGgNBIRDwp0Qe537S/65m6AsyMLmcHh9sZpjShHNoMkLLq+V3e64sGRslx88H+d5sNts/DR/PnEx10NAANQfkftTEqx+7s2+Kaie1Q/qDiuBoETObeGWFN5etTPfgoeGAW+v2knjqhGlJmklIubSqUEc7evFsm7nYdJOZFIpPHdpdG/MCM9b+jtPQkJCgXt4RkZG8tVXXxV5rhUrVrg8Tk5OzlfnzNldCQkJHDlyhPLlywPQpk2bfNdu1KhRvrL//ve//Pe//3U+bt++fZHLvxbWpjO98sorvPLKKwU+99RTT/HUU0/lK3c4HPTt25eHHnpIye/SyN29e92tJyLirvgeudte5fvuxgLHj5fopc/u1yE3Id6xY8cC6990003cdNNNhZ7vqquuYvPmzc7HDoeD3r17c//99zvLxowZw5gxY1yOGzZsmMt2I+Hh4Xz00UfOx6dOnWLs2LHcd99959WWM1WtWpWvv/66wOcCAwOZM2dOvnKHw0F2djavvPKKV/tuJcNF3JSRbee/X29m3k8pZJX4qqL6AF/SQgIsdL/8Usbe1EAzwkVEyiqHPXc58D+nQ+r/gW0/Be0Z5Ql+wA2A5ZczCpM/KJFrXbSsIVCpbe4s7qBy+Z+3a9l3kbLM7jAYO29bgX/F81aFGDtvG+3rxZaK5YxFxHz8rBZa1ojydRilXnp6OtOmTaNjx474+fkxZ84cli5dypIlS3wdmpQ259i7Fyy5z5+5d6+IiKcUNLjbcfF+b/DTTz/x+++/07x5c44dO8a4ceMAuPHGG30cmXcoGS7ihnvfX8+SbWm+DqNMCfK30LJ6FK/1bkq5YPf/FNkdhk9GaouIiMnsmQv/dyc40r1yOQ1jC4CgSIhOgojGcGQTpK2F7H/I/eIrIPdFsvpBhXqQtACCK/g4ZhExk3U7D7ssX3w2A0g5lsm6nYeVzBIRKUEWi4X58+czfvx4srKyqFOnDl988QXXX3+9r0OT0uaMvXsNLFgK2LuXplPdXw1LREQuyIsvvsj27dsJDAykadOmrF69mksuucTXYXmFkuEi56BEuOc0rFKebx68sNGeGqktIiLntGcurL7F11GUPeeamS0iUoLSThSeCC9OPRERKZ6QkBCWLl3q6zDELPL27t0wBDL2/VseWiU3ER7fw2ehiYhcTBo3bszGjRt9HYbPKBkuF5WCZhVD7iyD/UfSWbvzIEt+TeFYto8DLYM8kQgXEZGLRHYGbH4Mjv8BfqEQXhsy94HdBif/hOwTuRvE+geCA8jJhKxUIMPXkZdNjV6Aeo/6OgoRuchVCg/2aD0RERHxkvge5MR04eQXDYgw/oJ6T0DDcZoRLiIiXqNkuJRp2TkO3lq1g3dX/smxLF9Hc7HJXQApqfYl570UuoiIXMRW3AD75/k6CnGyQO0hvg5CRITmiZHEVQgm9VhmYbuOElvh3wHPIiIiUopY/Mi2VgA7UL6WEuEiIuJVyk5JmfX8wu28+3+7fB3GRattrIO3B3cmICDA16GIiIhJWL+7DNL/8nUYcqbLHs2dgS8i4mN+Vguju9fjgQ83YYGCdh1ldPd6+FktBRwtIhcDh8Ph6xCklDOMgoZTiVcYdvyN9Nx/H90KDrsS4iJSKPXpciZPvB+UDJdS6WRmDg9+uI7Vfx7BXqwzWAElwr3JAkSGBXD31Ync3bIqSxcv9HVIIiJiIvXSp2M1lAgvVS57DBpP8nUUIiJOnRrE8WafJoz5Ziupx/9d+iu2QjCju9ejU4M4H0YnIr4SGBiI1Wpl//79REdHExgYiMVS8gNjHA4H2dnZZGZmYrVaS/x6JcHsbTif+A3D4ODBg1gsFk3c8LY9c/HfMIQox+k9w39/EXZ/DE1f1p7hIuKipPt0s/d7YP42nG/fnZ2dzcGDB7FarQQGFn+yhpLh4jUZ2Xb++/Vm5v2UQlaJD+wx3x8BM6hUzp9Ol19KtchQ7myZQKB/wa+zzWbzcmQiImJqjmxqGPPQXD7fMQAHFizhdbEm9oO6wzQjXERKpU4N4mhTK4quLy7irxNW7r46gae6aka4yMXMarWSmJhISkoK+/fv99p1DcMgIyODkJAQryTfS4LZ23C+8VssFqpUqYKfn2Yke82eubC6J5y9yUn6vtzy1p8rIS4iTiXdp5u93wPzt6E48YeGhlK1atULSv4rGS5ece/761myLc3XYcgFaFilPN882NrXYYiIiDflZMMfr8DerwALVO4OEQ3hn/8DRw5kHwWrFcrVgLBE2DwGTm4Hw55bn7zBURb+/dhpJXejuBznc34YGsZWYgKgXC1ITwFHOpB1xnN+EHQJ1B5KTu0hzF+4lC6dumDVTBkRKeX8rBYqnB6vUzUyVIlwESEwMJCqVauSk5OD3V68NQbPl81mY9WqVVx77bWmnWls9jacb/wBAQFKhHuTww4bhwJGAQOfDcACG4fBpTdqyXQRcSrJPt3s/R6Yvw3nG7+fnx/+/v4XnPhXMlzcZncYLP81lce/+pnDGdqzoWSd/kDoY0H+FlpWj+K13k0pF6w/FyIippOdARsfguQvwDhGvtH4AFjBUg78gnKT2H7+uQlSwwEnf3et+s//ihmIwb+J8fzPKRFeQsKqw41uLj2vVV1ExGTaX+rg4RuaUzO2vK9DEZFSIm/5a299Mezn50dOTg7BwcGm/DIazN8Gs8df5h1cDel7i6hgQPqe3HoxbbwVlYiYQEn16WWh3zB7G3wVv7Jb4paFW1J44MNNBX6FLvn5W6F+5fK83/8qKoSe3//QNpuNud/MZ8mJWPYczaJqRAhT/tP4vJLRdofBup2HSTuRSaXwYJonRmq2hIhIWeOwQ9oKOLACq8NOVE4wGB2B0/3Oyptg39funAiM4/9O1LYD2VrN5Xw5+Hcom0+T+5YAqNgE2i6C4Aq+jEREpERVDoOrqkea8gsgERGRi0JGimfriYiIFJOS4XJOC7ekcP+Hm3wdRqk18NpERnWp59FzBvvDm3c0KfYXO35WCy1rRHk0JhERKUX2zIUf74PsQwD4AdcAxuejfRqWaVz9MVT7j0dPabfZmD9/Pl26aJlxEZGSZncY7DhmYd4vKcRVDNPgXxERkdIoJM6z9URERIpJyXBxysi289+vN/PNphSyDStD1y72dUilXkkkwkVEzC4hIYFdu3blKx80aBCvv/46hmEwduxY3n77bY4cOUKLFi14/fXXqV+/vg+iLUXOmOkNQKU2uUvFnb132p65sPqWAk+hNMA5WPzgmk8hvoevIxERkWJauCWFMd9sJfW4H2z7FYC4CsGM7l6PTg30ZbqIiEipEd0aQqtA+j4K3rLLkvt8dGtvRyYiIhcZJcMFgAGz1rP0tzOXRNXunQWxAJFhAdx9dSL3XVuDQH+9TiIiZ1u/fj12u935eMuWLbRv355bb70VgEmTJjF58mRmzpxJ7dq1GT9+PO3bt2f79u2Eh4f7Kuzz47Dn7muWkZI7ij26df6kdUGyTsLqXnBwKRjZ5H4UK2Sv5q3jPRnxRcwPwhKgyVS4tLN7vycRESmVCtu+K/VYJg98uIk3+zRRQlxERKS0sPpB05dhdU8MLFhcevDTQ7mbTtU9moiIlDglw4UbXlvNL3uP+zqMUiMkwMoVVSryQJsaXFMrWsvtiYicp+joaJfHzz33HDVq1CApKQnDMJg6dSpPPvkkPXrkzs6dNWsWMTExzJ49m4EDB/oi5POz63PYMAiyDrqWW8tBYCT4h4E1KDfZbbFCxYaQeCdsHAYnt591skIS4VIEK7kLwxvkbnAO4A9BkVCpLdToD7HX6QsVEZEyxu4wGDtvW4Hzygxyv1IfO28b7evF6h5ORESktIjvAa0/hw1DIGPfv+WhVXIT4Vq1S0REvEDJ8IvcN5v2XnSJ8GpRIax87DpfhyEiclHIzs7mww8/ZPjw4VgsFv7++29SU1Pp0KGDs05QUBBJSUmsWbOm0GR4VlYWWVlZzsfHj+f2XTabDZvtwhLKece7cx7r5hFY/5ha8HLkjpOQeTJ/+fEtsHv2BcV4sctLfOT0OAZ+Iec+wO7I/fGi83kflUZmjx/M3wbF73uebIOZX4fSat3Ow6Qcyyz0eQNIOZbJup2HaVkjynuBiYiISNHie5AT04WDc9tS2fEDJPSBq2ZqALOIiHiNkuEml53j4K1VO3hnxZ8cz/Z1NKVbgBVe6NGQm5rF+zoUEZGLxldffcXRo0fp168fAKmpqQDExMS41IuJiSlwn/E8EydOZOzYsfnKFy9eTGhoqEdiXbJkScFPGHaisjdxZc6L+JOlfbm9LC8RnmJtzvpF3/s0FncU+j4yCbPHD+Zvg+L3PU+0IT093QORyJnSThSeCC9OPREREfEiix8Z1kvAAYTGKxEuIiJepWS4iY39Zisz1iT7OowSYwGiwwMZf+PltKsX47LUXXaOgw/WJvPXPydJO5YJFgvhQf7c1OhSHA47c5atp3qNGkSVC+GS8CBiywfTPDFSy+WJiHjZ9OnT6dy5M5UrV3Ypt1hc/x4bhpGv7EyjRo1i+PDhzsfHjx8nPj6eDh06UL58+QuK0WazsWTJEtq3b09AQIBrnHu/xO+HO7AYORd0DbkwjrjuRF/zBV18HUgRinofmYHZ4wfzt0Hx+54n25C3gol4TqXwYI/WExERMauJEyfyxBNPMHToUKZOnVpgnZSUFB555BE2btzIjh07GDJkSKF1vcXAevpf3l3FS0RERMlwk7py/BIOnjT3VHCrBd64owmdGsSd97GB/lbuaV29wOdsNhvHdxh0aV/btF/EiYiUBbt27WLp0qXMnTvXWRYbGwvkzhCPi/v3739aWlq+2eJnCgoKIigoKF95QECAx/7W5zvXnrmw9j8eObcUg184VOuFpcnL+AWGYJZ5A558T/qC2eMH87dB8fueJ9pg9tegNGqeGElchWBSj2UWuG+4BYitkDsIWkREpKxav349b7/9Ng0bNiyyXlZWFtHR0Tz55JNMmTLFS9EVzbCcToYbdt8GIiIiFx0lw02o2ysrTZsIt1qgSkQIT3erT9u6lTRTW0SkDJsxYwaVKlWia9euzrLExERiY2NZsmQJjRs3BnL3FV+5ciXPP/+8r0J1lZ0B6x+AXbN8HYkJBUBQZfDzA0c6BMZCZGM4sR0yj4A9C+zHwRoIIVVy62HFXr4uf+4+Qs3adfCLbQcxbbRsnoiIuPCzWhjdvR4PfLgJC7gkxPPuKkd3r6d7TBERKbNOnjzJHXfcwTvvvMP48eOLrJuQkMDLL78MwHvvveeN8Nxwuo82NDNcRES8S8nwUuJkZg4PfriO1X8ewWxj4wZem8ioLvV8HYaIiJQiDoeDGTNm0LdvX/z9//24YbFYGDZsGBMmTKBWrVrUqlWLCRMmEBoaSu/evX0Y8Wkrb4J9X/s6ipIVGAPxN8MlV0HmAdi3EI5uhpxMclMLNiBvWXgLWIOhUlu4+hMIKlciITlsNn5Pm0/1Bl3w02xKEREpRKcGcbzZpwljvtlK6vEsZ3lshWBGd69XrFXHREREzGLw4MF07dqV66+//pzJ8OLIysoiK+vf/jVv2xebzYbNZrugc9tsNvb6tabqFTdgrVgPLvB8vpD3Glzoa+ErZo8fzN8Gs8cP5m+D2eMH87fB0/G7ex4lw33E7jBYsfUAT8/7hX3HzfmmBSXCRUSkYEuXLmX37t30798/33MjRowgIyODQYMGceTIEVq0aMHixYsJDw/3QaT/sv7vFkiZ59MYSlYAXP0BVDtr6ff6I3wTjoiISDF0ahBHm1pRvPbJQqrXb0RcxTCaJ0ZqRriIiJRpH3/8MZs2bWL9+vUldo2JEycyduzYfOWLFy8mNDT0wi/gV52FvwGknP4xpyVLlvg6hAti9vjB/G0we/xg/jaYPX4wfxs8FX96erpb9ZQM97KMbDv9Z/7A2r+P+jqUYrEAkWEB3H11IvddW4NAf6uvQxIRkVKoQ4cOGEZBO3rmzg4fM2YMY8aM8W5QRbFnYjV5ItwADP8orIm9IKIpHF4PGfshoDwk3Amx12npcRERKRP8rBZqVTDo0jBO+7OLiEiZt2fPHoYOHcrixYsJDg4useuMGjWK4cOHOx8fP36c+Ph4OnToQPny5S/o3DabjSVLltC+fXvT9t1mb4PZ4wfzt8Hs8YP522D2+MH8bfB0/HmrmJyLkuFedO/761myLc3XYZwXqwV+f6azkt4iIlKmXW6bgZnnkxnAn9YbSLj5c6zOD5J3+zIkERGRErX/FMz9aR81KpWnWUKkr8MREREpMRs3biQtLY2mTZs6y+x2O6tWreK1114jKysLP78LH/gcFBREUFBQvvKAgACPJCzKOfYSeOBb/CvUgsjGF3w+X/HU6+ErZo8fzN8Gs8cP5m+D2eMH87fBU/G7ew4lw73EfIlwA7Bwb+tEJcJFRKTMq+j403cXD6oELWZC5Q4Fz9zOyYYdr8GBVZBzCoIuAQzIPgT+YRDdmpwa97Nt4VISvBy6iIiIr2w5YuG7uVv5T7N4JcNFRKRMa9euHb/++qtL2d13303dunV5/PHHPZII94bKOf+H/9o5UPN+aP6mr8MREZGLiJLhXpCRbTdZIjzXgKuraT9wERG5KAQaJy/wDHmj5w0gB3Cc+xBrMLR6H6reWnQ9/0C4bHjuT2FsNjfjFBERKRssp5d0cRSyLYuIiEhZER4eToMGDVzKwsLCiIqKcpaPGjWKffv28f777zvr/PzzzwCcPHmSgwcP8vPPPxMYGEi9er76vvd0523YfXR9ERG5WCkZ7gX3zVrv6xDckrcf+F1XVaXKye3c0KmOr0MSEREpeYadIA6f/3HlLoNOGyEwJP9zDjtsGQu/vQT29DOesMIlLaHBaO3hLSIicgHy1i9zKBcuIiJCSkoKu3fvdilr3Pjfpcg3btzI7NmzqVatGsnJyV6OLpfxb+/tk+uLiMjFS8nwEmZ3GKz5+5BXr/nH+Avb49tmszF//nYPRiQiIlJ6WQ7+D3/Oc2Z1xabQZUPhz1v9oOG43KT3wdWQkQIhcRDdWglwERERD8ibGW5oZriIiFyEVqxY4fJ45syZ+eqUtj7SmQw3lAwXERHvUjK8hK3beRi7Fz93DLxWe3yLiIicl8yU86sfeSV0WudeXasfxLQ575BERESkaKdz4VomXURExDTyRrIpGS4iIt6lZHgJSz2e6bVrDbw2UXt8i4iInK+gGPfqhdWATj9DULkSDUdERETOLW9muDcHn4uIiEjxGZzutE/8BQdWaOU0ERHxGiXDS9hzC7aW2Lnz9vi+++pE7ru2hmaEi4iIFIub36K3eEuJcBERkVLi3z3DlQ0XEREp7Sx7v6S27fPcB//8D5a1hdAq0PRliO/h2+BERKTMUzK8BH2zaS8Hjp/fHqSXhAXw3C1X0LZuJfyslnMfICIiIhcmK829eplu1hMREZESV7eiwdTbGhIfpYFqIiIipdqeufit7YXf2QPR0/fB6p7Q+nMlxEVEpEQpGV5C7A6DIZ9udrt+jUtCWDy8rRLgIiIi3hYc5169EDfriYiISImrFAJdLo8lICDA16GIiIhIYRx22DgUMMj/rbcBWGDjMLj0Ri2ZLiIiJUbrapeQV5btOK/6DS6tqES4iIiIDxjR15BFeNGLpQdF5e5nJiIiIiIiIiLuObga0vcWkAjPY0D6ntx6IiIiJUTJ8BLgMOCt1cnndUzPJvElE4yIiIi44Rz7jWo7UhERkVLlUCYs2JLK+uTDvg5FRERECpOR4tl6IiIixaBkeAn485gF+3l8aR7gZ6FVrUtKLiAREREplOXg/wjiZBEj1YHsQxqpLiIiUopsP2ZhyCe/MPqbraz96xB2h0auiYiIlDrubjembclERKQEKRleAv44dn7LnU++rZGWSBcREfGVTI1UFxERMZNFWw8wb1fu1xnb9h/n9nd+4Jrnl7Nwi/pqERGRUiW6NYRWKXDH8FwWCI3XtmQiIlKilAwvAUey3a9bMzqU7ldULrlgREREpGjBGqkuIiJiFgu3pPDQx5tJt7uWpx7L5IEPNykhLiIiUppY/aDpy0BBu4+dTpA3nZpbT0REpIQoGV4CKga4vzzb/KFJJRiJiIiInIsRfQ0ZliiNVBcRESnl7A6DsfO2nf4y3bXfzrsLHztvm5ZMFxERKU3ie2Bv+TE2yrmWh1aB1p9DfA/fxCUiIhcNJcNLQJi/e/VaJEYS6K9fgYiIiE9Z/Pg1cEDeg7OfzP2PRqqLiIj43Lqdh0k5llno8waQciyTdTsPey8oEREROSejys38Gtg/90GFy6Hd93DDTiXCRUTEK5SJLQE7T7q3/3fTahElHImIiIi4I8W/JfaWH0PIWVuXhF6qkeoiIiKlRNqJwhPhxaknIiIi3mNYTg8wD4mBmDYacC4iIl6jZLiH2R0G24+5lwy3uldNREREvMVwnPVYy6yKiIiUFpXCgz1aT0RERLzpdCrCsPs2DBERuegoGe5hG3YdIcvhXpa7ZfVLSjgaERERcUdczlr81vaCzBTXJzL2w+qesGeubwITEREpYRMnTsRisTBs2LAi661cuZKmTZsSHBxM9erVmTZtmncCPEPzxEjiKgTn29QkjwWIqxBM88RIb4YlIiIibjhqrY690UtQ+0FfhyIiIhcZJcM9LO1Ellv1QgP9uKpGVAlHIyIiIudk2Lk8+11ydxrN92TufzYOA4dGr4uISNmyfv163n77bRo2bFhkvZ07d9KlSxdat27NTz/9xBNPPMGQIUP44osvvBRpLj+rhdHd651+5Npv5yXIR3evh5+WYRMRESl1TlkvxVHrIW1DJiIiXleqk+E5OTk89dRTJCYmEhISQvXq1Rk3bhwOx79LmBqGwZgxY6hcuTIhISG0adOGrVu3+izmSuFBbtUbeG0N3aCLiIiUApaD/yPEOFToLDMwIH0PHFztxahERERK1smTJ7njjjt45513iIiIKLLutGnTqFq1KlOnTuWyyy5jwIAB9O/fnxdffNFL0f6rU4M4Xu11BRUDXctjKwTzZp8mdGoQ5/WYREREREREpPTy93UARXn++eeZNm0as2bNon79+mzYsIG7776bChUqMHToUAAmTZrE5MmTmTlzJrVr12b8+PG0b9+e7du3Ex4e7vWYm1WLINTfID2n8K/UwwL9ePC6ml6MSkRERAp19tLohclws56IiIgJDB48mK5du3L99dczfvz4IuuuXbuWDh06uJR17NiR6dOnY7PZCAgIyHdMVlYWWVn/rpx2/PhxAGw2Gzab7YJiv652JI9cbudwhdrYDLgqMZJm1SLws1ou+NzekhenWeI9m9njB/O3wezxg/nbYPb4wfxt8GT8Zn0NzCTAOIHl4P8guCJENvZ1OCIichEp1cnwtWvXcuONN9K1a1cAEhISmDNnDhs2bAByZ4VPnTqVJ598kh49cpdXmTVrFjExMcyePZuBAwf6LPaiBPiX6gn5IiIiF5egGDfrVSrZOERERLzk448/ZtOmTaxfv96t+qmpqcTEuPaXMTEx5OTk8M8//xAXl3829sSJExk7dmy+8sWLFxMaGlq8wM+w95SFtzb+TXyYQa3MP1j02wWf0ieWLFni6xAuiNnjB/O3wezxg/nbYPb4wfxt8ET86enpHohEihJp/w3/FXdCVAvo+IOvwxERkYtIqU6GX3PNNUybNo0//viD2rVrs3nzZv73v/8xdepUIHffstTUVJcR6kFBQSQlJbFmzZpCk+ElOUL9h78OFjkrHOBouo21f6bRIjHygq5VUjQq1LfMHj+Yvw1mjx/M3wazxw8aoW4uBe0VXgDtbiIiImXAnj17GDp0KIsXLyY4ONjt4ywW147QMIwCy/OMGjWK4cOHOx8fP36c+Ph4OnToQPny5YsR+b9sNhu/fbYUgHLh5enSpeUFnc8XbDYbS5YsoX379gXOrC/tzB4/mL8NZo8fzN8Gs8cP5m+DJ+PP+35YStLpCWKG3bdhiIjIRadUJ8Mff/xxjh07Rt26dfHz88Nut/Pss89y++23A7mj04ECR6jv2rWr0POW5Aj1jf9YAL9z1lu8+kcO/ebml+8+olGhvmX2+MH8bTB7/GD+Npg9ftAIdVPISnOvXqab9UREREqxjRs3kpaWRtOmTZ1ldrudVatW8dprr5GVlYWfn+s9bWxsrPP+O09aWhr+/v5ERUUVeJ2goCCCgoLylQcEBHgk4ZK33ppx+pxm5anXw1fMHj+Yvw1mjx/M3wazxw/mb4Mn4jdz+83CyBthbjh8G4iIiFx0SnUy/JNPPuHDDz9k9uzZ1K9fn59//plhw4ZRuXJl+vbt66xX0Aj1wkanQ8mOUK+wI433d/x8znodWrco1TPDNSrUd8weP5i/DWaPH8zfBrPHDxqhbirB+Zd2LVCIm/VERERKsXbt2vHrr7+6lN19993UrVuXxx9/PF8iHKBly5bMmzfPpWzx4sU0a9bMZ5/V8m75jdI9xlxEREROM5xD2ZQMFxER7yrVyfDHHnuMkSNH0qtXLwAuv/xydu3axcSJE+nbty+xsbFA7gzxM/coS0tLyzdb/EwlOUL9qhrRVAw0OJZtKXDRVQsQWyGYljUr4Wct3eutalSob5k9fjB/G8weP5i/DWaPHzRC3QyMS1riwIoFR+EroVv8IKqVN8MSEREpEeHh4TRo0MClLCwsjKioKGf5qFGj2LdvH++//z4A999/P6+99hrDhw/n3nvvZe3atUyfPp05c+Z4Pf48eX22Q9lwERERc7BomXQREfEN67mr+E56ejpWq2uIfn5+OBy5o8cSExOJjY11WYI2OzublStX0qqVb76w9rNa6JGQG9/ZX6jnPR7dvV6pT4SLiIhcLCz/rMVaVCIccm/WD63xVkgiIiI+lZKSwu7du52PExMTmT9/PitWrKBRo0Y888wzvPLKK9xyyy0+i9FiyU2CKxkuIiJiDlomXUREfKVUzwzv3r07zz77LFWrVqV+/fr89NNPTJ48mf79+wO5y6MPGzaMCRMmUKtWLWrVqsWECRMIDQ2ld+/ePov7iiiDV3tdwbMLtpNyLNNZHlshmNHd69GpgZZZFRERKS0s++eduxJARkrJBiIiIuIjK1ascHk8c+bMfHWSkpLYtGmTdwJyw78zw30ahoiIiLhJyXAREfGVUj0z/NVXX6Vnz54MGjSIyy67jEcffZSBAwfyzDPPOOuMGDGCYcOGMWjQIJo1a8a+fftYvHgx4eHhPowcOtaPYeVjbcmbAD7kupqsfKytEuEiInJR2LdvH3369CEqKorQ0FAaNWrExo0bnc8bhsGYMWOoXLkyISEhtGnThq1bt3o/UIcd667Z7tXVnuEiIiKlxiXB8HTXugxpV9PXoYiIiIgb0i2x2Bs8A3Uf9nUoIiJykSnVM8PDw8OZOnUqU6dOLbSOxWJhzJgxjBkzxmtxuWPR1gOMX7DdOUr9leV/8tnGvZoZLiIiZd6RI0e4+uqradu2LQsWLKBSpUr89ddfVKxY0Vln0qRJTJ48mZkzZ1K7dm3Gjx9P+/bt2b59u3cHtB1cjSX7n3PXC4qG6NYlH4+IiIi4pUIgdLmqKgEBAb4ORURERNyQYY3GcVlf/NR3i4iIl5XqZLhZbT5kYcbazZy9WlvqsUwe+HATb/ZpooS4iIiUWc8//zzx8fHMmDHDWZaQkOD8t2EYTJ06lSeffJIePXoAMGvWLGJiYpg9ezYDBw70XrDuLn2ecAdY/Uo2FhERERERERERERHxKCXDPczuMJibbM2XCAcwyN3XbOy8bbSvF4tf3hrqIiIiZcg333xDx44dufXWW1m5ciWXXnopgwYN4t577wVg586dpKam0qFDB+cxQUFBJCUlsWbNmgKT4VlZWWRlZTkfHz9+HACbzYbNZit2rJaAaLc+DOXEdsW4gOuUtLzX4EJeC18ye/xg/jaYPX4wfxsUv+95sg1mfh3MIssO65IPExwYSNNqEb4OR0RERM7Bz8iEIz9BQDBENPR1OCIichFRMtzDNuw6wtHswpPcBpByLJN1Ow/TskaU9wITERHxkr///ps333yT4cOH88QTT7Bu3TqGDBlCUFAQd911F6mpqQDExMS4HBcTE8OuXbsKPOfEiRMZO3ZsvvLFixcTGhpa/GANOx0sUQQbhyio9zaADMslLFl/HCzzi38dL1myZImvQ7ggZo8fzN8Gs8cP5m+D4vc9T7QhPT3dA5FIUQ5mwojpG4gtH8wPT7TzdTgiIiJyDuGOvQQs7QWhVeGmgu/9RURESoKS4R6WdiLr3JWAtBOZJRyJiIiIbzgcDpo1a8aECRMAaNy4MVu3buXNN9/krrvuctazWFzTz4Zh5CvLM2rUKIYPH+58fPz4ceLj4+nQoQPly5e/sHh3vQrrejtXcHHGc/q/gVe9Tpcq3S/oGiXNZrOxZMkS2rdvb8q9U80eP5i/DWaPH8zfBsXve55sQ94KJlJy8vpsh1HQumwiIiJS2hj/9t4+jUNERC4+SoZ7WKXwIDfrBZdwJCIiIr4RFxdHvXr1XMouu+wyvvjiCwBiY2MBSE1NJS4uzlknLS0t32zxPEFBQQQF5e9jAwICLjhhkeNX8F7gebfp/v7+YJLEjideD18ye/xg/jaYPX4wfxsUv+95og1mfw3MQMlwERG5WE2cOJEnnniCoUOHMnXq1ELrrVy5kuHDh7N161YqV67MiBEjuP/++70X6FmcyXDD7rMYRETk4mT1dQBlTbNqEVQMNApcahVyb9jjKgTTPDHSm2GJiIh4zdVXX8327dtdyv744w+qVasGQGJiIrGxsS7L0GZnZ7Ny5UpatWrl1Vhx2PH7OXfGecF9twU2DgOHbtZFRERKE+vpjtuhXLiIiFxE1q9fz9tvv03DhkXvub1z5066dOlC69at+emnn3jiiScYMmSIc5C6Lzi77JxTcGCF7rNFRMRrlAz3MD+rhR4JBS/1kvcl++ju9fCzFr6vuIiIiJk9/PDD/PDDD0yYMIE///yT2bNn8/bbbzN48GAgd3n0YcOGMWHCBL788ku2bNlCv379CA0NpXfv3t4N9uBqLBn7Ch3EBgak74GDq70YlIiIiJxL3hfqmTY7a/86hF1ZcRERKeNOnjzJHXfcwTvvvENERESRdadNm0bVqlWZOnUql112GQMGDKB///68+OKLXorWlWXvl7TKHJf7wHYclrWFbxJgz1yfxCMiIhcXLZNeAq6IMrjn6mpMX7OLM1dss1jg3taJdGoQV/jBIiIiJnfllVfy5ZdfMmrUKMaNG0diYiJTp07ljjvucNYZMWIEGRkZDBo0iCNHjtCiRQsWL15MeHi4d4PNSPFsPRERESlxi7Ye4PVtuducpGfbuf2dH4irEMzo7vV0vy0iImXW4MGD6dq1K9dffz3jx48vsu7atWvp0KGDS1nHjh2ZPn06NputwC1dsrKyyMrKcj4+fvw4ADabDZvNVuy4LXu/xG9tL/xwHbhmpO+D1T2xt/wYo8rNxT6/t+S9BhfyWviS2eMH87fB7PGD+dtg9vjB/G3wdPzunkfJ8BKw+ZCFGX/s4uxx6Q4D3l61k8ZVI3SDLiIiZVq3bt3o1q1boc9bLBbGjBnDmDFjvBdUQULc7I/drSciIiIlauGWFB76eHO+++3UY5k88OEm3uzTRPfbIiJS5nz88cds2rSJ9evXu1U/NTWVmJgYl7KYmBhycnL4559/iIvL31dOnDiRsWPH5itfvHgxoaGhxQvcsNMhYxB+5N9W1IKBAWT/MJglIf5g8SveNbzszC3fzMjs8YP522D2+MH8bTB7/GD+Nngq/vT0dLfqKRnuYXaHwdxka74b8zONnbeN9vVitVS6iIiIr0W3xgi5FApdKt0CoVUgurWXAxMREZGz2R0GY+dtO32/7dpzG6dLdL8tIiJlzZ49exg6dCiLFy8mODjY7eMslrP6ytNLmJ5dnmfUqFEMHz7c+fj48ePEx8fToUMHypcvX4zIwZK2Ev+Vhwp/Hgg1/qHrleUxKiUV6xreYrPZWLJkCe3bty9wZn1pZ/b4wfxtMHv8YP42mD1+MH8bPB1/3iom56JkuIdt2HWEo9lF7jxKyrFM1u08TMsaUd4LTERERPKz+mFvNBm/tf9xfonuyoCmU8FqjhHqIiIiZdm6nYdJOZZZ6PO63xYRkbJo48aNpKWl0bRpU2eZ3W5n1apVvPbaa2RlZeHn53rPGhsbS2pqqktZWloa/v7+REUV3EcGBQURFBSUrzwgIKD4CQvbQbeq+dsOgkmSOhf0epQCZo8fzN8Gs8cP5m+D2eMH87fBU/G7ew4lwz0s7UTWuSsBaScKv4EXEREREREREVfu3kfrfltERMqSdu3a8euvv7qU3X333dStW5fHH388XyIcoGXLlsybN8+lbPHixTRr1sy7yRNtTSYiIqWA1dcBlDWVwvOPniu4nvtL2oiIiEgJcdjx+zl3GbhCl0nfOAwcdi8GJSIiIgVx9z5a99siIlKWhIeH06BBA5efsLAwoqKiaNCgAZC7xPldd93lPOb+++9n165dDB8+nN9++4333nuP6dOn8+ijj3o3+OjWEFqlgB3D81ggNF5bk4mISIlSMtzDGsdXxFLkjuFgtUDTahFeikhEREQKdXA1lkL3CwcwIH0PHFztxaBERESkIM0TI4mrEFzU1+nEVQimeWKkN8MSERHxuZSUFHbv3u18nJiYyPz581mxYgWNGjXimWee4ZVXXuGWW27xbmBWP2j6MkAB35if7tG1NZmIiJQwLZPuYW+t2lnESLdcDgM27jqiPcxERER8LSPFs/VERESkxPhZLYzuXo8HPtxE7lfq/9575/1rdPd6+FmLvicXERExuxUrVrg8njlzZr46SUlJbNq0yTsBFSW+B/aWH5Oz9n6COfJveWiV3ER4fA+fhSYiIhcHzQz3ILvDYNYPu9yqqz3MRERESgHtXyYiImIqnRrE8WqvKyh/1nansRWCebNPEzo1UJ8tIiJS2hhVbmZF8Iv/FrT7Hm7YqUS4iIh4hWaGe9C6nYc5lpHjVl3tYSYiIlIKRLfGCLkUCl0q3ZI7Wl37l4mIiJQaHevHcHSHnac25n6l8dGAFlxVPUozwkVEREox48yl0CslgUX9toiIeIdmhnuQu7O9K4YGaA8zERGR0sDqh73RZKCg/ctOl2r/MhERkVLH74xvM5onRioRLiIiUsoZZ6YiDIfvAhERkYuOkuEe5O5s77tbJepGXURERERERKSYzvwyw2EUPKRNREREShOX3ttnUYiIyMVHyXAPap4YSWz5IAqbWwa5s8IfvK6m94ISERGRwjns+P08HKDwZdI3DgOH3YtBiYiIyLmcubKqcuEiIiKln3HmXbdmhouIiBdpz3AP8rNaeKpLXR78+OdC6zzX43LNChcRESktDq7GkrGviAoGpO+Bg6shpo23ohIREZFz8LPA4DbV8ffzw6o9R0VEREo9BwHYaw3Fz88PLJqjJyIi3qNkuIiIiFy8MlI8W09ERES8wt8Kw9rVJCAgwNehiIiIiBsclkAcjV7AT323iIh4mYZgeZDdYTB+/u+FPm8Bxs7bht2hNdxERERKhZA4z9YTERERERERERERkVJDyXAPWrfzMKnHsyhs11EDSDmWybqdh70al4iIiBQiujVGyKUUPkzNAqHxEN3ai0GJiIjIuRgG/Jl2kj8OnCDHrn1HRURESj3DgPS9cGo3OOy+jkZERC4iSoZ7UNqJTI/WExERkRJm9cPeaDJAAQnx04Pbmk4Fq58XgxIRERF3dH51DR2mrOJYhs3XoYiIiMg5OQj4rjp8XQ1sx3wdjIiIXESUDPegSuHBHq0nIiIiJc+ocjPrgx6HgAjXJ0IvhdafQ3wP3wQmIiIihbJYcn8AtBOZiIiIGZyRijC0qouIiHiPkuEe1DwxkoohARQ0tyxPRGgAzRMjvReUiIiIuMcwin4sIiIipYr1dDbcUJ8tIiJS+lnO3FpUyXAREfEeJcM9ruibcN2ii4iIlC6WvV9yZdbzkHPU9YmM/bC6J+yZ65O4REREpGjW09+p25UMFxERMQUjLx1haM9wERHxHiXDPWjdzsMczcjBucdoAY6m21i387D3ghIREZHCOez4/TwcKKj3Pv3F+sZh4NCNuoiISGljOT3DTMuki4iImITFL/e/WiZdRES8SMlwD0o7kenReiIiIlLCDq7GkrGviGFsBqTvgYOrvRiUiIiInIvD+Hd59I3Jh7ErIy4iIlL6WfJmhisZLiIi3qNkuAdVCg/2aD0REREpYRkpnq0nIiIiJW7R1gOM3eSHzZ6bAB/y8c9c8/xyFm5Rfy0iIlJqGXacK7D9839agU1ERLxGyXAPap4YSWz5IArbGdwCxFUIpnlipFfjEhERkUKExHm2noiIiJSohVtSeOjjzRzNdi1PPZbJAx9uUkJcRESkFLLs/ZIOGfdhcZzuwP/vdvgmAfbM9WlcIiJycVAy3IP8rBae6lK30OcNYHT3evhZC1+MVURERLwoujVGyKWFDGMDsEBoPES39mJQIiIiUhC7w2DsvG2n+23X++q8vnzsvG1aMl1ERKQ02TMXv7W9CDYOuZan74PVPZUQFxGREqdkuIiIiFy8rH7YG00GClrX5fSX7E2ngtXPi0GJiIhIQdbtPEzKscxCnzeAlGOZrNt52HtBiYiISOEcdtg4FDDIPz3s9F34xmFaMl1EREqUkuEeZHcYjJ//e6HPW9AodRERkdLGqHIz64MeB/9yrk+EVoHWn0N8D98EJiIiIi7SThSeCC9OPRERESlhB1dD+t4CEuF5DEjfk1tPRESkhCgZ7kHrdh4m9XgWZy/Xlkej1EVEREqnFP+WOKrekfugys3Q7nu4YacS4SIiIqVIpfBgj9YTERGREpaR4tl6IiIixaBkuAdplLqIiIhJGXYs6Xty/x1QMXePcC2NLiIiUqo0T4wkrkJwobPLLEBchWCaJ0Z6MywREREpTEicZ+uJiIgUg5LhHqRR6iIiIuZj2fslHTLuw5o6P7dg5wz4JgH2zPVpXCIiIuLKz2phdPd6px+5bj+WlyAf3b0eftbCF2MVERERL4puDaFVCtwxPJcFQuNz64mIiJQQJcM9qHliJLHlgzj7pjyPRqmLiIiUMnvm4re2F8HGIdfy9H2wuqcS4iIiIqVMpwZxvNrrCioGupbHVgjmzT5N6NRAM8tERERKDasfNH0ZKOgb89MJ8qZTtTKbiIiUKCXDPcjPauGpLnULfE6j1EVE5GIxZswYLBaLy09sbKzzecMwGDNmDJUrVyYkJIQ2bdqwdetW7wfqsMPGoVDgGPXTt+kbh+XWExERkVKjY/0YRjexc0lYAADP3NSA/z1+nRLhIiIipVF8D+wtPybTEuVaHloFWn8O8T18E5eIiFw0lAz3sI71Y7guzpHvS3WLBe67NlE35yIiclGoX78+KSkpzp9ff/3V+dykSZOYPHkyr732GuvXryc2Npb27dtz4sQJ7wZ5cDWk7y10sTYwIH1Pbj0REREpVawWCAn0B6B+5fIadC4iIlKKGVVuZnHI2xhBMbkFzd6AG3YqES4iIl6hZLiHLdp6gOUp1nzLvjgMeHvVThZuSfFJXCIiIt7k7+9PbGys8yc6OhrInRU+depUnnzySXr06EGDBg2YNWsW6enpzJ4927tBZrjZJ7tbT0RERLzKaslNgBtGwVuViYiISCli8QP/0Nx/RzbW0ugiIuI1/r4OoCyxOwzGz/+9yDpj522jfb1YjVoXEZEybceOHVSuXJmgoCBatGjBhAkTqF69Ojt37iQ1NZUOHTo46wYFBZGUlMSaNWsYOHBggefLysoiKyvL+fj48eMA2Gw2bDZbsWK0BES79UEoJyAao5jX8Ja816C4r4WvmT1+MH8bzB4/mL8Nit/3PNkGM78OZtK9YSzHMu1cUi7I16GIiIiIGxyX3ohf9iEIjDp3ZREREQ9RMtyD1u08TOrxLChkwVUDSDmWybqdh2lZQx2+iIiUTS1atOD999+ndu3aHDhwgPHjx9OqVSu2bt1KamoqADExMS7HxMTEsGvXrkLPOXHiRMaOHZuvfPHixYSGhhYvUMNOB0sUwcahAntuA8iwXMKS9cfBMr941/CyJUuW+DqEC2L2+MH8bTB7/GD+Nih+3/NEG9LT0z0QiZzL0HY1CQgI8HUYIiIi4ibHFZPwU98tIiJepmS4B6WdyPRoPRERETPq3Lmz89+XX345LVu2pEaNGsyaNYurrroKAIvFNf1sGEa+sjONGjWK4cOHOx8fP36c+Ph4OnToQPny5Ysdq2XvG7C2FwaGS0I871HgVa/TpUr3Yp/fW2w2G0uWLKF9+/amTAqYPX4wfxvMHj+Yvw2K3/c82Ya8FUxERERERERExLeUDPegSuHBHq0nIiJSFoSFhXH55ZezY8cObrrpJgBSU1OJi4tz1klLS8s3W/xMQUFBBAXlXwI1ICDgwhIWibeRA9h+GESIcchZbAmtAk2n4h/fo/jn9oELfj18zOzxg/nbYPb4wfxtUPy+54k2mP01MIvjGTaMLAfhwf4E+WvfURERKZvefPNN3nzzTZKTkwGoX78+Tz/9tMtA9LO9/vrrvPbaayQnJ1O1alWefPJJ7rrrLi9FXASHDXJywBqoPcNFRMRrrL4OoCxpnhhJbPkgchdWzc8CxFUIpnlipFfjEhER8aWsrCx+++034uLiSExMJDY21mUJ2uzsbFauXEmrVq18Ep9R5WYWh7yNo2KT3IJ6T8ANO/+fvfsOj6rM2zj+nZl0UhCEJJAIUZoBQbpRURQTiiKKdbHA6tqwseyKIrtAfGmyiqDuZnWXpqhYECsiUZeiiBRBKYqNbkLoCaRNZs77x5CBkEJCpp3k/lxXLpgzz8zcz1hOzvk9BUxWCBcREalvbpu1lm4TPmP1toP+jiIiIuI1CQkJTJkyhbVr17J27VquvPJKBg0axObNmytsn5GRwejRoxk/fjybN28mPT2dBx98kA8//NDHycsL+qwnvBUBOUv9HUVEROoRFcM9yGa18LcB7Sp8rnTp1XEDk7FZK18GVkRExOz++te/smzZMrZt28Y333zDjTfeSG5uLkOHDsVisTBixAgmTZrEwoUL2bRpE8OGDSMiIoIhQ4b4L7TFBiENXX9v2F4j1EVEREyg9NLaWfF4dBERkTph4MCBDBgwgDZt2tCmTRsmTpxIZGQkq1atqrD9q6++yn333cctt9zCueeey6233srdd9/N008/7ePkFTlejjCc/o0hIiL1ipZJ97C+7WO5q42Tt3aEcLTI4T4eFxPGuIHJ9OsQX8WrRUREzG/37t384Q9/YP/+/TRp0oSLLrqIVatW0aJFCwBGjRpFQUEBw4cP59ChQ/Ts2ZMlS5YQFRXl3+Dui3ENWhMRETEDq8V1znYaqoaLiEj94HA4ePvttzl27BgpKSkVtikqKiIsrOw2neHh4axevRq73e7f7VwsKoaLiIjvqRjuBRc0MsgJPZuPNu6lU0IMo/q146JzG2tGuIiI1Avz58+v8nmLxcL48eMZP368bwJV2/Eb6RYtnCMiImIG7pnhmhouIiJ13MaNG0lJSaGwsJDIyEgWLlxIcnJyhW379u3Lf//7X6677jq6dOnCunXrmDVrFna7nf379xMfX/FkraKiIoqKityPc3NzAbDb7djt9lrlL329Ews2oKSkGKOW7+lrpX2o7XfhL2bPD+bvg9nzg/n7YPb8YP4+eDp/dd9HxXAP+3TzXtK/tXG4eC8A3+0+wl/f/k6zwkVERAJe6Y10DV4TEREJdE4Djh1fje2HrFx6t22qAegiIlJntW3blg0bNnD48GEWLFjA0KFDWbZsWYUF8b///e9kZ2dz0UUXYRgGsbGxDBs2jKlTp2KzVb4l2OTJk0lPTy93fMmSJURERHikH7m5RzkLWLvmG/YGmXN2eGZmpr8j1IrZ84P5+2D2/GD+Ppg9P5i/D57Kn5+fX612KoZ70OJNWTw8/ztOHZOefaSQB+Z9S8btXVQQFxERCVSly7RZdCNdRETqroyMDDIyMti+fTsA7du3Z+zYsfTv37/S17z22mtMnTqVn3/+mZiYGPr168czzzxD48aNfZS6rBOD0I8B8MySn3jtm50ahC4iInVWSEgIrVq1AqBbt26sWbOGGTNm8NJLL5VrGx4ezqxZs3jppZfYu3cv8fHxvPzyy0RFRXH22WdX+hmjR49m5MiR7se5ubkkJiaSlpZGdHR0rfLb7XYyMzOJjjkLDkG3bl0wmg2o1Xv6WmkfUlNT/bvU/Bkye34wfx/Mnh/M3wez5wfz98HT+UtXMTmdgC+G79mzh8cff5xPPvmEgoIC2rRpw8yZM+natSsAhmGQnp7Oyy+/7N539J///Cft27f3aU6H0yD9wy3HC+Flb6Ibx4+kf7iF1OQ4jVYXEREJSKXD2bRMuoiI1F0JCQlMmTLFfUN97ty5DBo0iPXr11d4Hf3ll19y55138txzzzFw4ED27NnD/fffz5/+9CcWLlzo6/gahC4iIoLrnvjJS5pXJDg4mISEBMC1ndk111yD1Vr59W5oaCihoaEVvo+nCi4Wq2tmepDVCiYs4oBnvw9/MHt+MH8fzJ4fzN8Hs+cH8/fBU/mr+x4Bfbf30KFDXHLJJQQHB/PJJ5+wZcsWnn32WRo2bOhuM3XqVKZNm8aLL77ImjVriIuLIzU1lby8PJ9mXb3tIFlHCit93gCyjhSyettB34USERGR6jNK9wzXoDUREam7Bg4cyIABA2jTpg1t2rRh4sSJREZGsmrVqgrbr1q1ipYtW/LII4+QlJTEpZdeyn333cfatWt9nPz0g9DBNQjdoT3ERUSkDnnyySdZsWIF27dvZ+PGjYwZM4alS5dy2223Aa4Z3Xfeeae7/U8//cS8efP4+eefWb16NbfeeiubNm1i0qRJ/uqCm9GkFyQOhnANXBMREd8J6JnhTz/9NImJicyePdt9rGXLlu6/G4bB9OnTGTNmDIMHDwZco9pjY2N5/fXXue+++3yWNSev8kL4mbQTERERXyvdr0zFcBERqR8cDgdvv/02x44dIyUlpcI2F198MWPGjGHRokX079+fnJwc3nnnHa6++uoq37uoqKjMjLXS5evsdjt2u/2M8n5TzUHoX/+SQ8+kRmf0Gb5S+h2c6Xfhb2bPD+bvg9nzg/n7YPb8YP4+eDJ/IH8He/fu5Y477iArK4uYmBg6duzI4sWLSU1NBSArK4udO3e62zscDp599lm2bt1KcHAwV1xxBStXrixzX91fnBdMwGbimYwiImJOAV0M/+CDD+jbty833XQTy5Yto3nz5gwfPpx77rkHgG3btpGdnU1aWpr7NaGhoVx++eWsXLnSp8XwplFhHm0nIiIiPuaeGR7QC+eIiIjU2saNG0lJSaGwsJDIyEgWLlxIcnJyhW0vvvhiXnvtNW655RYKCwspKSnh2muv5YUXXqjyMyZPnkx6enq540uWLCEiIuKMcq/bbwFsp223ZMU3HPjBHLPDMzMz/R2hVsyeH8zfB7PnB/P3wez5wfx98ET+/Px8DyTxjpkzZ1b5/Jw5c8o8Pv/881m/fr0XE50hw4ElZxnY97lmhjfpBdbTn9dFRERqK6CL4b/99hsZGRmMHDmSJ598ktWrV/PII48QGhrKnXfeSXZ2NgCxsbFlXhcbG8uOHTsqfV9vjFDvnBBFXHQoe3OLyu1dBq45ZnExoXROiArokYagUaH+Zvb8YP4+mD0/mL8PZs8P9WeEet1SegbXzHAREanb2rZty4YNGzh8+DALFixg6NChLFu2rMKC+JYtW3jkkUcYO3Ysffv2JSsri8cee4z777+/ypvzo0ePZuTIke7Hubm5JCYmkpaWRnR09BnlbrztIK/8fPrl2dN69TTFzPDMzExSU1NNudef2fOD+ftg9vxg/j6YPT+Yvw+ezF96f1i8w7J7IWkFwwladuDEwYgE6DrDtWy6iIiIFwV0MdzpdNKtWzf3fiadO3dm8+bNZGRklNkHxXLK3p6GYZQ7djJvjFAHGBBnYVZu6Wyykz/fwAD6x+bz6eJPzvj9fU2jQv3L7PnB/H0we34wfx/Mnh/q/gj1OsU4vky6ZoaLiEgdFxISQqtWrQDo1q0ba9asYcaMGbz00kvl2k6ePJlLLrmExx57DICOHTvSoEEDevXqxYQJE4iPr3jPz9DQUEJDQ8sdDw4OPuOCRUqrpsTHhJF9pLCKQehhpLRqis1qjsFttfk+AoHZ84P5+2D2/GD+Ppg9P5i/D57Ib+b+B7xd72L7+lZsp5698/fAihuh1zsqiIuIiFcFdDE8Pj6+3Mj0888/nwULFgAQFxcHQHZ2dpkL8JycnHKzxU/mjRHqAAOATt//zrj3N3K4+KR+xIQxpn87+ravPFMg0ahQ/zJ7fjB/H8yeH8zfB7PnB41QNyfNDBcRkfrJMIwyq6edLD8/n6CgsrcObDab+3W+ZLNaGDcwmQfmfYvrvH3inF36t3EDk01TCBcREanznA5Y9yhgVHClffxcvm4ENB+kJdNFRMRralwMb9myJXfddRfDhg3jnHPO8UYmt0suuYStW7eWOfbTTz/RokULAJKSkoiLiyMzM5POnTsDUFxczLJly3j66acrfV9vjFAvNaBjM5y7NjDv97NZt/Mwd1/akicHmPNiXKNC/cvs+cH8fTB7fjB/H8yeHzRC3VxUDBcRkbrvySefpH///iQmJpKXl8f8+fNZunQpixcvBlyDx/fs2cMrr7wCwMCBA7nnnnvIyMhwL5M+YsQIevToQbNmzXyev1+HeF64tRN/e3dDmUHocTFhjBuYTL8OFc9UFxERET/YtwLyd1dxlW1A/i5Xu9jevsslIiL1So3XAf3LX/7C+++/z7nnnktqairz58+vdAR5bf35z39m1apVTJo0iV9++YXXX3+dl19+mQcffBBwLY8+YsQIJk2axMKFC9m0aRPDhg0jIiKCIUOGeCVTdVgt0DDCVbho1TTKlIVwERGR+saiZdJFRKQe2Lt3L3fccQdt27alT58+fPPNNyxevJjU1FQAsrKy2Llzp7v9sGHDmDZtGi+++CIdOnTgpptuom3btrz77rv+6gJ928cyrouDzokxANzTK4kvH79ShXAREZFAU5Dl2XYiIiJnoMYzwx9++GEefvhhvvvuO2bNmsUjjzzC8OHDGTJkCHfddRddunTxWLju3buzcOFCRo8ezVNPPUVSUhLTp0/ntttuc7cZNWoUBQUFDB8+nEOHDtGzZ0+WLFlCVFSUx3KcCefx5eJsVexdLiIiIoFEM8NFRKTumzlzZpXPz5kzp9yx0vsAgcRqgcYNQgBoeXYDDUIXEREJROHVHKhW3XYiIiJn4IynPnXq1IkZM2awZ88exo0bx3//+1+6d+9Op06dmDVrlsf2DrvmmmvYuHEjhYWF/PDDD9xzzz1lnrdYLIwfP56srCwKCwtZtmwZHTp08Mhn14azdHKZrsdFRETMofR3F528RURETMF6vADu9O3W5SIiIlJdTXpBREKFO4a7WCAi0dVORETES864GG6323nrrbe49tpr+ctf/kK3bt3473//y80338yYMWPKzN6uj9wzwzU6XURExCS0TLqIiIiZWI8PYPPUYHwRERHxMKsNus4ATqzFdsLx++Zdp7vaiYiIeEmNl0n/9ttvmT17Nm+88QY2m4077riD5557jnbt2rnbpKWlcdlll3k0qNmUjky3anaZiIiIORhaJl1ERMRMLkyMwWq1kNgowt9RREREpDKJg3GkzMe+ajjhxoETxyMSXIXwxMF+iyYiIvVDjYvh3bt3JzU1lYyMDK677jqCg4PLtUlOTubWW2/1SECzKp0Zrlq4iIiIWWhmuIiIiJncfUnLCu9JiIiISGAxEq5nSXgQV3ePJsi+z7VHeJNemhEuIiI+UeNi+G+//UaLFi2qbNOgQQNmz559xqHqAi2TLiIiYjKaGS4iIiIiIiLiHRYbRtPLQQPZRETEx2o89SknJ4dvvvmm3PFvvvmGtWvXeiRUXaBl0kVERMxGxXARERGzMQxDe4aLiIiIiIhIpWpcDH/wwQfZtWtXueN79uzhwQcf9EiousB5vBquieEiIiImYWiZdBERETN5YuEmkkYv4qXlv/k7ioiIiFSDdd2D8FYU/DjD31FERKQeqfHd3i1bttClS5dyxzt37syWLVs8EqouKF0mXTPDRUREzEIzw0VERMzCacC+vGIAtu8/hsOp2eEiIiKBzuIsgpKj4CzydxQREalHalwMDw0NZe/eveWOZ2VlERRU4y3I6ySnAUcK7AD8tDdPF+UiIiJmooFsIiIiAe3TzXtJ/9bG8p/3AzB/zS4uffoLFm/K8nMyERERqdrxckTpymwiIiI+UONieGpqKqNHj+bIkSPuY4cPH+bJJ58kNTXVo+HMqPSi/Lf9+QA8s+QnXZSLiIiYgftiXMuki4iIBKrFm7J4eP53HC4uezz7SCEPzPtW194iIiKBzL0tmYrhIiLiOzW+2/vss8+ya9cuWrRowRVXXMEVV1xBUlIS2dnZPPvss97IaBq6KBcRETGz4yu5aGa4iIhIQHI4DdI/3HL8jF32fF26Hlv6h1u0OpuIiEiAMiyaGS4iIr5X42J48+bN+f7775k6dSrJycl07dqVGTNmsHHjRhITE72R0RR0US4iImJihgNKXKu6cHA9OB3+zSMiIiLlrN52kKwjhZU+bwBZRwpZve2g70KJiIhIDZQWw3XNLSIivnNGm3w3aNCAe++919NZTK0mF+Up5zX2XTARERGpkmX3QtIKhmMxDrgOrB0OWyZB1xmQONi/4URERMQtJ6/ya+4zaSciIiI+ZrG5/tTMcBER8aEzKoYDbNmyhZ07d1JcXHZN8GuvvbbWocxIF+UiIiImtOtdbF/fio1TVm7J3wMrboRe76ggLiIiEiCaRoV5tJ2IiIj4WOR50KQXNDjH30lERKQeqXEx/LfffuP6669n48aNWCwWDOP4wuDH99d0OOrnEie6KBcRETEZpwPWPQoYlN8l3AAssG4ENB8EVpuv04mIiMgpeiQ1Ij4mjOwjhacOYwNcG5bFxYTRI6mRr6OJiIhINTjbPIKt/V/8HUNEROqZGu8Z/uijj5KUlMTevXuJiIhg8+bNLF++nG7durF06VIvRDSH0ovy8jfTXSxAvC7KRUREAse+FZC/u9JzNxiQv8vVTkREJADs2rWL3bt3ux+vXr2aESNG8PLLL/sxle/YrBbGDUw+/qhsObz0fD5uYDI2a+VndxEREfETw4ElZxlsfwP2LnUNUBcREfGBGhfDv/76a5566imaNGmC1WrFarVy6aWXMnnyZB555BFvZDQFXZSLiIiYTEGWZ9uJiIh42ZAhQ/jf//4HQHZ2NqmpqaxevZonn3ySp556ys/pfKNfh3heuLUTDUPKHo+LCSPj9i706xDvn2AiIiJSKcvuhaQV3EvQslRYOQQ+vwI+aAm73vV3NBERqQdqXAx3OBxERkYCcPbZZ/P7778D0KJFC7Zu3erZdCaji3IRERETCa/mebm67URERLxs06ZN9OjRA4C33nqLDh06sHLlSl5//XXmzJnj33A+1Ld9LOO6OLi5SzOCbRb6to/ly8ev1DW3iIhIINr1LravbyXMOFD2eP4eWHGjCuIiIuJ1NS6Gd+jQge+//x6Anj17MnXqVL766iueeuopzj33XI8HNJvSi/KzGwQDMOG6DrooFxGRemvy5MlYLBZGjBjhPmYYBuPHj6dZs2aEh4fTu3dvNm/e7PtwTXpBREKFe466RSS62omIiAQAu91OaGgoAJ999hnXXnstAO3atSMrq36tZGK1QLOzIrA7DBo1CNUqbCIiIoHI6YB1jwJGBVuUHb8aXzdCS6aLiIhX1bgY/re//Q2n0wnAhAkT2LFjB7169WLRokU8//zzHg9oRlYLBNlcX22nhIa6KBcRkXppzZo1vPzyy3Ts2LHM8alTpzJt2jRefPFF1qxZQ1xcHKmpqeTl5fk2oNUGLf4AnLrByUla3OpqJyIiEgDat2/Pv//9b1asWEFmZib9+vUD4Pfff6dx48Z+Tud7tuOX2o7j9yhEREQkwOxbAfm7KyiElzIgf5ernYiIiJfUuBjet29fBg8eDMC5557Lli1b2L9/Pzk5OVx55ZUeD2hWxvG76tYaf8MiIiLmd/ToUW677Tb+85//cNZZZ7mPG4bB9OnTGTNmDIMHD6ZDhw7MnTuX/Px8Xn/9dd+GdDpgxxsAlV+Y75ivEeoiIhIwnn76aV566SV69+7NH/7wBzp16gTABx984F4+vT6xHa+GlzirXOdFRERE/KWgmivXVLediIjIGahRqbakpISgoCA2bdpU5nijRo2wWDT7+WTO49Vwq74XERGphx588EGuvvpqrrrqqjLHt23bRnZ2Nmlpae5joaGhXH755axcudK3IU87Qh2NUBcRkYDSu3dv9u/fz/79+5k1a5b7+L333su///1vPybzD9vx622niuEiIiKBKbyaW4dWt52IiMgZCKpR46AgWrRogcOhGVKn41AxXERE6qn58+fz7bffsmbNmnLPZWdnAxAbG1vmeGxsLDt27Kj0PYuKiigqKnI/zs3NBVx7p9rt9jPKaTm6q1q/CJUc3YXR6Mw+w1dKv4Mz/S78zez5wfx9MHt+MH8flN//PNkHb30PBQUFGIbhXnVlx44dLFy4kPPPP5++fft65TMDWemWZJoZLiIiEqCa9IKIBIz8PVgq3KDMAhEJrnYiIiJeUqNiOLj2DB89ejTz5s2jUaNG3shUJ5Quk27TMukiIlKP7Nq1i0cffZQlS5YQFhZWabtTV5QxDKPKVWYmT55Menp6ueNLliwhIiLijLI2duzg0mq0W/XdDg5sWnRGn+FrmZmZ/o5QK2bPD+bvg9nzg/n7oPz+54k+5OfneyBJeYMGDWLw4MHcf//9HD58mJ49exIcHMz+/fuZNm0aDzzwgFc+N1Adr4Wz62A+X/96gB5JjdwFchEREQkAVht0nQErbsTg1C3Kjj/qOt3VTkRExEtqXAx//vnn+eWXX2jWrBktWrSgQYMGZZ7/9ttvPRbOzEqXSdfy8SIiUp+sW7eOnJwcunbt6j7mcDhYvnw5L774Ilu3bgVcM8Tj408sg5aTk1NutvjJRo8ezciRI92Pc3NzSUxMJC0tjejo6DMLa/TF+PjfUPB7hSPUDSwQ3pyeV/8VLIF9YW6328nMzCQ1NZXg4GB/x6kxs+cH8/fB7PnB/H1Qfv/zZB9KVzDxtG+//ZbnnnsOgHfeeYfY2FjWr1/PggULGDt2bL0qhn93wMK73/3m+vvuI/zhP6uIjwlj3MBk+nXQUqsiIiIBI3EwjpT52FcNJ9w4cOJ4RIKrEJ442G/RRESkfqhxMfy6667zQoy6x+F0/all0kVEpD7p06cPGzduLHPsj3/8I+3atePxxx/n3HPPJS4ujszMTDp37gxAcXExy5Yt4+mnn670fUNDQwkNDS13PDg4uBYFi2Do9jxGJSPULQDdZhAcUvkM90BTu+/D/8yeH8zfB7PnB/P3Qfn9zxN98NZ3kJ+fT1RUFOBaHWXw4MFYrVYuuuiiKrcbqWs+3byXWT9ZgbLL0WcfKeSBed+ScXsXFcRFRKTOyMjIICMjg+3btwPQvn17xo4dS//+/St9zWuvvcbUqVP5+eefiYmJoV+/fjzzzDM0btzYR6nLMhKuZ0l4EFd3jybIvs+1R3iTXpoRLiIiPlHjYvi4ceO8kaPOMY7PDLepGC4iIvVIVFQUHTp0KHOsQYMGNG7c2H18xIgRTJo0idatW9O6dWsmTZpEREQEQ4YM8X3g4yPUS76+nzAOnTiuEeoiIhKAWrVqxXvvvcf111/Pp59+yp///GfAtcLKGa+UYjIOp8GERT9W+Fzp4Lb0D7eQmhynJdNFRKROSEhIYMqUKbRq1QqAuXPnMmjQINavX0/79u3Ltf/yyy+58847ee655xg4cCB79uzh/vvv509/+hMLFy70dfwTLDaMppeDyQdOioiI+dS4GC6n5zSg+PjU8A27DtH8rHBdhIuIiBw3atQoCgoKGD58OIcOHaJnz54sWbLEPdPN14yE61kRdpDUwgfAGgpXLNYIdRERCUhjx45lyJAh/PnPf+bKK68kJSUFcM0SL11xpa5bve0g2blFnLqmSykDyDpSyOptB0k5zz+z30RERDxp4MCBZR5PnDiRjIwMVq1aVWExfNWqVbRs2ZJHHnkEgKSkJO677z6mTp3qk7wiIiKBpsbFcKvVWuU+2A6Ho1aBzO7TzXtJ/9aG3eGaGf7I/A1M/uRH7VsmIiL11tKlS8s8tlgsjB8/nvHjx/slT4VKf7WxhkBsb38mERERqdSNN97IpZdeSlZWFp06dXIf79OnD9dff70fk/lOTl6hR9uJiIiYicPh4O233+bYsWPuQXGnuvjiixkzZgyLFi2if//+5OTk8M4773D11VdX+d5FRUUUFRW5H+fm5gJgt9ux2+2VvaxaSl/v/GUmxk9TcTa7BueFz9bqPX2ttA+1/S78xez5wfx9MHt+MH8fzJ4fzN8HT+ev7vvUuBh+6lIqdrud9evXM3fuXNLT02v6dnXK4k1ZPDz/O4xTjmvfMhERkcBmKT17W6z+DSIiInIacXFxxMXFsXv3biwWC82bN6dHjx7+juUzTaPCPNpORETEDDZu3EhKSgqFhYVERkaycOFCkpOTK2x78cUX89prr3HLLbdQWFhISUkJ1157LS+88EKVnzF58uQK7+8vWbKEiIgIj/Tjp81ruaB4G1m/bWDd74s88p6+lpmZ6e8ItWL2/GD+Ppg9P5i/D2bPD+bvg6fy5+fnV6tdjYvhgwYNKnfsxhtvpH379rz55pvcfffdNX3LOsHhNEj/cMvxW+llZ85r3zIREZHA5i6GV7LkqoiISCBwOp1MmDCBZ599lqNHjwIQFRXFX/7yF8aMGYPVWvcHdfVIakRcdCjZuYVUdN62AHExYfRIauTzbCIiIt7Stm1bNmzYwOHDh1mwYAFDhw5l2bJlFRbEt2zZwiOPPMLYsWPp27cvWVlZPPbYY9x///3MnDmz0s8YPXo0I0eOdD/Ozc0lMTGRtLQ0oqOja5XfbreTmZlJ23bJ8D00i48lNmVArd7T10r7kJqaSrAJ9z03e34wfx/Mnh/M3wez5wfz98HT+UtXMTkdj+0Z3rNnT+655x5PvZ3prN52kKwjlS/Dpn3LREREAplmhouISOAbM2YMM2fOZMqUKVxyySUYhsFXX33F+PHjKSwsZOLEif6O6HU2q4W/DWjHQ/M3lHuutDQ+bmCyBqGLiEidEhISQqtWrQDo1q0ba9asYcaMGbz00kvl2k6ePJlLLrmExx57DICOHTvSoEEDevXqxYQJE4iPr3jl0tDQUEJDQ8sdDw4O9ljBxWZzvY/VAlYTFnHAs9+HP5g9P5i/D2bPD+bvg9nzg/n74Kn81X0PjxTDCwoKeOGFF0hISPDE25mS9i0TERExLy2TLiIiZjB37lz++9//cu2117qPderUiebNmzN8+PB6UQwH6Ns+lrvaOFm4O4xD+Sf2iIuLCWPcwGRtTyYiInWeYRhl9vc+WX5+PkFBZW/722w29+v8yn3N7fRrDBERqV9qXAw/66yzsFhOjLA2DIO8vDwiIiKYN2+eR8OZifYtExERMbPjF+IqhouISAA7ePAg7dq1K3e8Xbt2HDx40A+J/KdTY4NLel7A3a9+S7OG4Tx7Uyd6JDXSjHAREalznnzySfr3709iYiJ5eXnMnz+fpUuXsnjxYsC1vPmePXt45ZVXABg4cCD33HMPGRkZ7mXSR4wYQY8ePWjWrJk/u+LeoIxjO2HvUmjSC6w2PyYSEZH6oMbF8Oeee65MMdxqtdKkSRN69uzJWWed5dFwZtIjqRHxMWFkHymkovF12rdMREQkcJ3YM1zFcBERCVydOnXixRdf5Pnnny9z/MUXX6Rjx45+SuU/IUGu83ZkqE3bkYmISJ21d+9e7rjjDrKysoiJiaFjx44sXryY1NRUALKysti5c6e7/bBhw8jLy+PFF1/kL3/5Cw0bNuTKK6/k6aef9lcXAIgv+RrbxuPLuh9cC59fAREJ0HUGJA72azYREanbalwMHzZsmBdimJ/NamHcwGQemPctrjFuJwYMaN8yERGRAGeUzgzXeVpERALX1KlTufrqq/nss89ISUnBYrGwcuVKdu3axaJFi/wdz+eiwoK4oHkM5zSO8HcUERERr5k5c2aVz8+ZM6fcsYcffpiHH37YS4lqzrJ7Id2LKijG5++BFTdCr3dUEBcREa+p8fSn2bNn8/bbb5c7/vbbbzN37lyPhDKrfh3ieeHWTsSElD0eFxNGxu1dtG+ZiIhIgLIYDtdfSgpdS7U5HX7NIyIiUpHLL7+cn376ieuvv57Dhw9z8OBBBg8ezObNm5k9e7a/4/lc+2bRfPjwpfxzSBd/RxEREZHKOB3YNowETp4+Vur4Km3rRug6XEREvKbGM8OnTJnCv//973LHmzZtyr333svQoUM9Esys+raPJe9XB6PXuL7auXd159JWTTQjXEREJEBZdi/koqIJrgf2Q1qqTUREAlqzZs2YOHFimWPfffcdc+fOZdasWX5KJSIiIlKJfSuwFOypooEB+btg3wqI7e2rVCIiUo/UeGb4jh07SEpKKne8RYsWZfYmEZeUc89WIVxERCRQ7XoX29e3EsqRssdLl2rb9a5/comIiIiIiIjUBQVZnm0nIiJSQzUuhjdt2pTvv/++3PHvvvuOxo0beySU2RnGib+rEC4iIhKgnA5Y9yhgaKk2ERERE9qanUe3CZn0evoLvv71AA6ncfoXiYiIiG+FV3Pr0Oq2ExERqaEaF8NvvfVWHnnkEf73v//hcDhwOBx88cUXPProo9x6663eyGg6J19+qxYuIiISoPatgPzdFRTCS520VJuIiIgElO8OWBg2dx37jxaz61ABf/jPKi59+gsWb9KsMhERkYDSpBdGeHMqH7JmgYhEaNLLh6FERKQ+qfGe4RMmTGDHjh306dOHoCDXy51OJ3feeSeTJk3yeEAzKnGe+Puq3w7SI6mRZoiLiIgEGi3VJiIiJjF48OAqnz98+LBvggSITzfvZdZPVqC4zPHsI4U8MO9bMm7vQr8Oml0mIiISEKw2HBdOw/b1LRhwyoD044+6TgerzefRRESkfqhxMTwkJIQ333yTCRMmsGHDBsLDw7ngggto0aKFN/KZzqeb9/LsxhMn7j/8ZxXxMWGMG5isi3EREZFAoqXaRETEJGJiYk77/J133umjNP7lcBpMWPRjhc+V3mBP/3ALqclxGpQuIiISIIyE61kT+jjdjf9C8YETT0QkuArhiVUP/BMREamNGhfDS7Vu3ZrWrVt7MovpLd6UxcPzvyu35ItGp4uIiASgJr0gIgEjfw+WChdss7guzLVUm4iI+Nns2bP9HSFgrN52kOzcIk6dV1bKALKOFLJ620FSzmvs02wiIiJSuaygFBwduxD09S0Q1Rp6vOy63taMcBER8bIa7xl+4403MmXKlHLH//GPf3DTTTd5JJQZOZwG6R9uOX4rvexFeent9fQPt+BwVr47ioiIiPiQ1QZdZwBUUArXUm0iIiKBKCev0KPtRERExIeswa4/QxpBbG9db4uIiE/UuBi+bNkyrr766nLH+/Xrx/Llyz0SyoxWbztI1pHKL7ZPHp0uIiIiASJxMI6U+RQTXfZ4RAL0ekdLtYmIiASYplFhHm0nIiIiPmQ5Xo4wHP7NISIi9UqNi+FHjx4lJCSk3PHg4GByc3M9EsqMNDpdRETEnIyE61kX+mfXg4hE6PM/uHabCuEiIiIBqEdSI+KiQ6loXRdwre0SHxNGj6RGPs0lIiIi1WCLcA0+D4v1dxIREalHalwM79ChA2+++Wa54/Pnzyc5OdkjocxIo9NFRERMzHJ8WXQt1SYiIhLQbFYLfxvQDii/a3jp43EDk7FZK95TXERERPzHaNobrtsFvT/ydxQREalHgmr6gr///e/ccMMN/Prrr1x55ZUAfP7557z++uu88847Hg9oFj2SGhEfE0b2kcIKx6dbgDiNThcREQlMxvGzt6XG4wRFRETEx/q2j+WuNk4WZUeQnVvkPh4XE8a4gcn06xDvx3QiIiIiIiISSGpcDL/22mt57733mDRpEu+88w7h4eF06tSJL774gujo6NO/QR1ls1oYNzCZB+Z9i2u5thOj0DU6XUREJLBZcB7/i4rhIiIiZtCpscGo2y5j/e48cvIKaRrlGnyua24RERERERE52Rnd8b366qv56quvOHbsGL/88guDBw9mxIgRdO3a1dP5TKVfh3heuLUTUcFlj8fFhJFxexeNThcREQlQFve6LiqGi4iImIXTMBj7/iZmfP4zHZpHqxAuIiIS6A6th097wpe3+DuJiIjUIzWeGV7qiy++YNasWbz77ru0aNGCG264gZkzZ3oymyn1bR/L9s0OntkYRFRYEC/f0U2j00VERAJe6cxwna9FRETMwmqx8HPOUQBKHBVtWCYiIiKBxGI/AgdWQ/4u2LsUmvQCq83fsUREpI6rUTF89+7dzJkzh1mzZnHs2DFuvvlm7HY7CxYsIDk52VsZTaf0PnpEiI2U8xr7N4yIiIic1okSuGaGi4iImIVhnCiAf7PtAKnJcRqILiIiEqDiS77GtmqW60FBFnx+BUQkQNcZkDjYv+FERKROq/Yd3wEDBpCcnMyWLVt44YUX+P3333nhhRe8mc20Sq/HbZpdJiIiYhLaM1xEROqHjIwMOnbsSHR0NNHR0aSkpPDJJ59U+ZqioiLGjBlDixYtCA0N5bzzzmPWrFk+Slyx7w5YuGLaCvfj++d9y6VPf8HiTVl+TCUiIiIVsexeSPeip6FoX9kn8vfAihth17v+CSYiIvVCte/4LlmyhD/96U+kp6dz9dVXY7P5fvmSyZMnY7FYGDFihPuYYRiMHz+eZs2aER4eTu/evdm8ebPPs53MebwYblExXERExBTce4arGC4iInVcQkICU6ZMYe3ataxdu5Yrr7ySQYMGVXkdffPNN/P5558zc+ZMtm7dyhtvvEG7du18mLqsTzfvZdZPVrJzi8oczz5SyAPzvlVBXEREJJA4Hdg2jAROXpWt1PFr8XUjwOnwYSgREalPqn3Hd8WKFeTl5dGtWzd69uzJiy++yL59+07/Qg9Zs2YNL7/8Mh07dixzfOrUqUybNo0XX3yRNWvWEBcXR2pqKnl5eT7LdqrShdq0PJuIiIhZqBguIiL1w8CBAxkwYABt2rShTZs2TJw4kcjISFatWlVh+8WLF7Ns2TIWLVrEVVddRcuWLenRowcXX3yxj5O7OJwGExb9WOFzpdfi6R9uweHUHuIiIiIBYd8KLAV7KiiElzJce4jvW1FpCxERkdqo9p7hKSkppKSkMGPGDObPn8+sWbMYOXIkTqeTzMxMEhMTiYqK8krIo0ePctttt/Gf//yHCRMmuI8bhsH06dMZM2YMgwe79hWZO3cusbGxvP7669x3331eyXM6pZfcqoWLiIiYg6V0mfQqLs9FRETqGofDwdtvv82xY8dISUmpsM0HH3xAt27dmDp1Kq+++ioNGjTg2muv5f/+7/8IDw+v9L2LioooKjoxczs3NxcAu92O3W4/48zfbDt4fEZ4xedsA8g6UsjXv+TQM6nRGX+Ot5V+B7X5LvzJ7PnB/H0we34wfx/Mnh/M3wdP5jfrd2AKBdVcsaW67URERGqo2sXwUhEREdx1113cddddbN26lZkzZzJlyhSeeOIJUlNT+eCDDzwe8sEHH+Tqq6/mqquuKlMM37ZtG9nZ2aSlpbmPhYaGcvnll7Ny5cpKi+HeuigvfQ/3MumY8xcp/SLsX2bPD+bvg9nzg/n7YPb8oItys9Ey6SIiUp9s3LiRlJQUCgsLiYyMZOHChSQnJ1fY9rfffuPLL78kLCyMhQsXsn//foYPH87Bgwer3Dd88uTJpKenlzu+ZMkSIiIizjj7uv0W4PTbti1Z8Q0Hfgj82eGZmZn+jlArZs8P5u+D2fOD+ftg9vxg/j54In9+fr4HkkiFwuM9205ERKSGalwMP1nbtm2ZOnUqkydP5sMPP6zyQvhMzZ8/n2+//ZY1a9aUey47OxuA2NjYMsdjY2PZsWNHpe/prYvyUsbx6+38/GMsWrSo1u/nL/pF2L/Mnh/M3wez5wfz98Hs+UEX5ebhXtfFrylERER8oW3btmzYsIHDhw+zYMEChg4dyrJlyyosiDudTiwWC6+99hoxMTEATJs2jRtvvJF//vOflc4OHz16NCNHjnQ/zs3NJTExkbS0NKKjo884e+NtB3nl57WnbZfWq2fAzwzPzMwkNTWV4OBgf8epMbPnB/P3wez5wfx9MHt+MH8fPJm/dLKUeEGTXhjhzaHSpdItEJEATXr5OJiIiNQXtSqGl7LZbFx33XVcd911nng7t127dvHoo4+yZMkSwsLCKm1nsZQ9jRqGUe7Yybx1UQ6uX8J+euczAKKjIhkw4JJavZ8/6Bdh/zJ7fjB/H8yeH8zfB7Pnh/p9UZ6RkUFGRgbbt28HoH379owdO5b+/fsDrvN0eno6L7/8MocOHaJnz57885//pH379n7LrJnhIiJSn4SEhNCqVSsAunXrxpo1a5gxYwYvvfRSubbx8fE0b97cXQgHOP/88zEMg927d9O6desKPyM0NJTQ0NByx4ODg2v1u1FKq6bERYeSnVtIRUulW4C4mDBSWjXFZoK9y2r7ffib2fOD+ftg9vxg/j6YPT+Yvw+eyG/m/gc8qw3HhdOwfX0LBpYT19+A+1zedTpYT7/yi4iIyJnwSDHcW9atW0dOTg5du3Z1H3M4HCxfvpwXX3yRrVu3Aq4Z4vHxJ5ZRycnJKTdb/GTeuigvVToz3Ga1mvoXKf0i7F9mzw/m74PZ84P5+2D2/FA/L8oTEhKYMmWK+yb73LlzGTRoEOvXr6d9+/ZMnTqVadOmMWfOHNq0acOECRNITU1l69atREVF+Se0cXzPcBXDRUSkHjIMo8xWYie75JJLePvttzl69CiRkZEA/PTTT1itVhISEnwZEwCb1cLfBrTjofkbsEBFt9MZNzDZFIVwERGR+sJIuJ41oY/T3ToPCvaceCIiwVUITxzst2wiIlL3BfQd3z59+rBx40Y2bNjg/unWrRu33XYbGzZs4NxzzyUuLq7MErTFxcUsW7aMiy++2G+53QutVjE7XUREpK4aOHAgAwYMoE2bNrRp04aJEycSGRnJqlWrMAyD6dOnM2bMGAYPHkyHDh2YO3cu+fn5vP76637LbNEy6SIiUk88+eSTrFixgu3bt7Nx40bGjBnD0qVLue222wDXSmp33nmnu/2QIUNo3Lgxf/zjH9myZQvLly/nscce46677qp0iXRv69s+lrvaOGkaFVLmeGx0KBm3d6FfB+05KiIiEmiyglIoueJ/cFZnOKsL9PkfXLtNhXAREfG6gJ4ZHhUVRYcOHcoca9CgAY0bN3YfHzFiBJMmTaJ169a0bt2aSZMmERERwZAhQ/wRGQDn8fvpVt1PFxGRes7hcPD2229z7NgxUlJS2LZtG9nZ2aSlpbnbhIaGcvnll7Ny5Uruu+8+v+Q8sUy6BrKJiEjdtnfvXu644w6ysrKIiYmhY8eOLF68mNTUVACysrLYuXOnu31kZCSZmZk8/PDDdOvWjcaNG3PzzTczYcIEf3XBrfz2aDqPi4iIBDSnHQ6th+AYiO3t7zQiIlJPBHQxvDpGjRpFQUEBw4cPd+87umTJEv8ts8qJmeE23VAXEZF6auPGjaSkpFBYWEhkZCQLFy4kOTmZlStXApTbziQ2NpYdO3ZU+n5FRUVllm8t3Ufdbrdjt9trldX1etfZ22mAo5bv5w+l30Ftvwt/MXt+MH8fzJ4fzN8H5fc/T/YhkL+HmTNnVvn8nDlzyh1r165dmRXZ/O3TzXuZ9ZMVKLu0+97cQh6Y961mh4uIiASq0q3JDId/c4iISL1iumL40qVLyzy2WCyMHz+e8ePH+yVPRQz35DIVw0VEpH5q27YtGzZs4PDhwyxYsIChQ4eybNky9/OnniMNw6jyvDl58mTS09PLHV+yZAkRERG1znvO8WL43pz9rF60qNbv5y+BVKg4E2bPD+bvg9nzg/n7oPz+54k+5OfneyCJVMThNJiw6McKnzNwzQ1P/3ALqclx2jdcREQk0Fhsrj8Np39ziIhIvWK6YrgZlJ7Kdd0tIiL1VUhICK1atQKgW7durFmzhhkzZvD4448DkJ2dTXz8iRlbOTk55WaLn2z06NGMHDnS/Tg3N5fExETS0tKIjo6uVVa73c5Piz4FIDY2jgGXDKjV+/mD3W4nMzOT1NRUgoOD/R2nxsyeH8zfB7PnB/P3Qfn9z5N9KF3BRDxv9baDZOcWUdmS6AaQdaSQ1dsOknJeY59mExERkdMonRmOiuEiIuI7KoZ7QenMcI1CFxERcTEMg6KiIpKSkoiLiyMzM5POnTsDUFxczLJly3j66acrfX1oaCihoaHljgcHB3uk6FK6Z7jVFoTVpEUc8Nz34S9mzw/m74PZ84P5+6D8/ueJPpj9OwhkOXmFHm0nIiIS6DIyMsjIyGD79u0AtG/fnrFjx9K/f/8K2w8bNoy5c+eWO56cnMzmzZu9GbUaSpdJVzFcRER8R8VwLyjdM1zLpIuISH305JNP0r9/fxITE8nLy2P+/PksXbqUxYsXY7FYGDFiBJMmTaJ169a0bt2aSZMmERERwZAhQ/yYuvRCXOduERGRQNY0Ksyj7URERAJdQkICU6ZMca++NnfuXAYNGsT69etp3759ufYzZsxgypQp7sclJSV06tSJm266yWeZK3f8zrmzBPYuhSa9wGrzayIREan7VAz3Aufxc7omhouISH20d+9e7rjjDrKysoiJiaFjx44sXryY1NRUAEaNGkVBQQHDhw/n0KFD9OzZkyVLlhAVFeW3zO5TtnvJNhEREQlEPZIaERcdSnZuIRUNYrMAcTFh9Ehq5PNsIiIi3jBw4MAyjydOnEhGRgarVq2qsBgeExNDTEyM+/F7773HoUOH+OMf/+j1rFWJL/maoM8fOP7ICZ9fAREJ0HUGJA72azYREanbVAz3AsfxYvjBY8V8/esBeiQ10pLpIiJSb8ycObPK5y0WC+PHj2f8+PG+CVQtpTPDVQwXEREJZDarhb8NaMdD8zdU+LwBjBuYrGtwERGpkxwOB2+//TbHjh0jJSWlWq+ZOXMmV111FS1atPByuspZdi+ke1EFW6Pl74EVN0Kvd1QQFxERr1Ex3MM+3byXd7e5bqT/tPcof/jPKuJjwhg3MJl+HeL9nE5EREQqUrpnuGaGi4iIiIiISKDZuHEjKSkpFBYWEhkZycKFC0lOTj7t67Kysvjkk094/fXXT9u2qKiIoqIi9+Pc3FwA7HY7drv9zMMbDmzrRwIVreliYGCBtY9SEjsALIG7ZHrpd1Cr78KPzJ4fzN8Hs+cH8/fB7PnB/H3wdP7qvo+K4R60eFMWD8//zr1neKnsI4U8MO9bMm7vooK4iIhIIDIcrj+P7dS+ZSIiIgHM4TSYsOjHSp+3AOkfbiE1OU6zw0VEpM5o27YtGzZs4PDhwyxYsIChQ4eybNmy0xbE58yZQ8OGDbnuuutO+xmTJ08mPT293PElS5YQERFxptFp7NjIpYV7Kn3eggEFu/nmo2c4YLvgjD/HVzIzM/0doVbMnh/M3wez5wfz98Hs+cH8ffBU/vz8/Gq1UzHcQxxOg/QPtxwvhJe94DbQBbmIiEigsuxeSFv7W64H+7/SvmUiIiIBbPW2g2TnFlHR3DJwXX9nHSlk9baDpJzX2KfZREREvCUkJIRWrVoB0K1bN9asWcOMGTN46aWXKn2NYRjMmjWLO+64g5CQkNN+xujRoxk5cqT7cW5uLomJiaSlpREdHX3G2S07c+Gb07e7qFMLjHMGnPHneJvdbiczM5PU1FSCg4P9HafGzJ4fzN8Hs+cH8/fB7PnB/H3wdP7SVUxOR8VwD1m97SBZRworfV4X5CIiIgFo17vYvr4V26nrumjfMhERkYCUk1f5dfeZtBMRETEjwzDKLGlekWXLlvHLL79w9913V+s9Q0NDCQ0NLXc8ODi4dgWLyMRqNQuKTAQTFHZq/X34mdnzg/n7YPb8YP4+mD0/mL8Pnspf3fdQMdxDdEEuIiJiMk4HrHsUXDuUneL4ui7rRkDzQVoyXUREJEA0jQrzaDsREZFA9+STT9K/f38SExPJy8tj/vz5LF26lMWLFwOuGd179uzhlVdeKfO6mTNn0rNnTzp06OCP2Cc06YUR3hwK9lSyrovFtTpbk14+DiYiIvWF1d8B6gpdkIuIiJjMvhWQv7uSi3EAA/J3udqJiIhIQOiR1Ii46FA4dVWXkzSMCKZHUiPfhRIREfGivXv3cscdd9C2bVv69OnDN998w+LFi0lNTQUgKyuLnTt3lnnNkSNHWLBgQbVnhXuV1YbjwmlARUPRjz/uOl2D0EVExGs0M9xDeiQ1Ij4mjOwjhRVekluAuJgwXZCLiIgEioIsz7YTERERr7NZLfxtQDsemr+h0jaH8+1kbsmmX4d43wUTERHxkpkzZ1b5/Jw5c8odi4mJIT8/30uJas5IuJ41oY/T3ToPCvaceCIiwVUI1/ZkIiLiRZoZ7iE2q4VxA5OPPypbDi8d7zZuYDI2a+Xzz0RERMSHwqt5g7y67URERMQnrjq/KRFVDO23AOkfbsHhrHz2uIiIiPhWVlAKJVf/AtbjK6de/Bpcu02FcBER8ToVwz2oX4d4Xri1E2GnrOgSFxNGxu1dNCpdREQkkDTpBREJFe4Y7mKBiETtWyYiIhJg1u44RH5JlRudkHWkkNXbDvoulIiIiJyexQa248XwRl21NLqIiPiEiuEe1rd9LH2bOwHo1qIhb9xzEV8+fqUK4SIiIoHGaoOuM4CKdh3VvmUiIiKBKievqJrtCr2cRERERGrEcOC+At+3EpwOv8YREZH6QcVwbzh+//ycRg1IOa+xlkYXEREJVImDcaTMp4TwsscjEqDXO1quTUREJAA1jQqtZrswLycRERGR6oov+Zqgj1uB/YjrwDd3wQctYde7fs0lIiJ1n4rhXmAcH9xmsagILiIiEuiMhOvZFtTf9SC+H/T5n/YtExERCWDdWpxFw5Cq9wOPjwmjR1IjHyUSERGRqlh2L6R70dNQsKfsE/l7YMWNKoiLiIhXqRjuBc7jf9r07YqIiJiCxXL8hnpMe4jtraXRRUREApjNaqFLY2eVba7tFK9V2kRERAKB04Ftw0jAvaDqSY5fi68boSXTRUTEa1Su9YLSmeFWzQwXERExBYtx/Ia6Rb8aiYiIBDqH0+DbA1Wfsz/4LguHs+rZ4yIiIuID+1ZgKdhTQSG8lAH5u2DfCh+GEhGR+kR3fL2g9HJby6SLiIiYRekeJ5oRLiIiEujW7jjE4eKqr7ezjhSyettBHyUSERGRShVkebadiIhIDakY7gWlg8+1TLqIiIg5WNDMcBEREbPIySuqZrtCLycRERGR0wqP92w7ERGRGtIdXy8wDNcIdS2TLiIiYg4niuGaGS4iIhLomkaFVrNdmJeTiIiIyGk16YUR3pzKNy+xQEQiNOnlw1AiIlKfqBjuBaUndhXDRUREzMFdDNevRiIiIgGvW4uziAiqej/whhHB9Ehq5KNEIiIiUimrDceF0wAqKYgb0HU6WDU4XUREvEN3fL3AfTtdxXARERGT0J7hIiIidYmuxkVERERERARUDPcK4/j9dKuuvkVERExBe4aLiIiYx9odh8gvqfqC+1C+ndXbDvookYiIiFTK6cC2YSRQ2WA1C6wbAU6HD0OJiEh9oju+XlBaDLepGi4iImIK2jNcRETEPHLyiqrZrtDLSUREROS09q3AUrCnilVbDMjfBftW+DCUiIjUJyqGe0HpMukWLZMuIiIS+AwHYc4Drr8f3abR6CIiIgGuaVRoNduFeTmJiIiInFZBlmfbiYiI1JCK4R7mcBocOj74POtwAQ6n4d9AIiIiUrld7xL0cStinRtcj399GT5oCbve9WcqERERqUK3FmcREVT1tXbDiGB6JDXyUSIRERGpVHi8Z9uJiIjUkIrhHrR4Uxa9n13Od4dcX+v73/3OpU9/weJNGtUmIiIScHa9CytuhII9ZY/n73EdV0FcRETEtLROm4iISIBo0gsjvDlVDmOLSIQmvXyVSERE6hkVwz1k8aYsHpj3Ldm5Zfcuyz5SyAPzvlVBXEREJJA4HbDuUcCo4Gb58Uv0dSO0ZLqIiEgAWrvjEPklVZe7D+XbWb3toI8SiYiISKWsNhwXTgOovCDe4law2nwWSURE6hcVwz3A4TRI/3BLhSfz0mPpH27RkukiIiKBYt8KyN9dRQMD8ne52omIiEhAyckrOn0jICev0MtJREREpDqMhOv5Jei6yhv88IxWZxMREa9RMdwDVm87SNaRyi+yDSDrSKFGpYuIiASKgmqu2FLddiIiIuIzTaNCq9kuzMtJREREpFoMBwmO0ww21+psIiLiJSqGe0B1R5trVLqIiEiACI/3bDsRERHxmW4tzqJhSEVbnbhYgPiYMHokNfJlLBEREamEZd+XhBsHKj13a3U2ERHxJhXDPaC6o801Kl1ERCRANOkFEQlQ1W30iERXOxEREQkoNquFwS2dle47agDjBiZjs1a9r7iIiIj4SKFWZxMREf9RMdwDeiQ1Ij4mTKPSRUREzMJqg64zACg/r+z4467TXe1ERERERERE5MyFaXU2ERHxHxXDPcBmtTBuYDJQfn5Z6WONShcRkfpi8uTJdO/enaioKJo2bcp1113H1q1by7QxDIPx48fTrFkzwsPD6d27N5s3b/Zt0MTB0OsdCG9W9nhEgut44mDf5hEREZFqcTgN3t1e+e0MC5D+4RYczsrmjouIiIgvGU0upcDSuNJVXQCtziYiIl6jYriH9OsQT8btXYiNDi1zPC4mjIzbu9Cvg0a1iYhI/bBs2TIefPBBVq1aRWZmJiUlJaSlpXHs2DF3m6lTpzJt2jRefPFF1qxZQ1xcHKmpqeTl5fk2bOJgSq7+hQPWdq7H7UbCtdtUCBcREQlga3cc4nBxlbuOknWkkNXbDvoulIiIiFTOYmO37TSF7ha3anU2ERHxChXDPahfh3iW/uUy2sY4AbitZyJfPn6lCuEiIlKvLF68mGHDhtG+fXs6derE7Nmz2blzJ+vWrQNcs8KnT5/OmDFjGDx4MB06dGDu3Lnk5+fz+uuv+z6wxUaxJdr19+h2uvgWEREJcDl5RdVsV+jlJCIiIlIthoMEx4qq2+yYD06Hb/KIiEi9omK4h9msFqKCXX9v2ThSS6OLiEi9d+TIEQAaNWoEwLZt28jOziYtLc3dJjQ0lMsvv5yVK1f6JaOIiIiYR9Oo0NM3AppGhXk5iYiIiFSHZd+XhBsHym0xWkb+Lth3moK5iIjIGQjyd4C6qHTvE4vq4CIiUs8ZhsHIkSO59NJL6dChAwDZ2dkAxMbGlmkbGxvLjh07KnyfoqIiiopOzALLzc0FwG63Y7fba5XR9XrX2bvE4cSo5fv5Q+l3UNvvwl/Mnh/M3wez5wfz90H5/c+TfTDz92AG3VqcRcMQgyPFlkr3Hm0YEUyPpEY+zSUiIiKVKMyqXruCarYTERGpARXDvciiariIiNRzDz30EN9//z1ffvlluedOPU8ahlHpuXPy5Mmkp6eXO75kyRIiIiJqnbPn8VvpGzduZOePi2r9fv6SmZnp7wi1Yvb8YP4+mD0/mL8Pyu9/nuhDfn6+B5JIZWxWC4NbOpn1U+VbmxzOt5O5JVvblomIiASCsGqej8N13hYREc9TMdwLjOND01UKFxGR+uzhhx/mgw8+YPny5SQkJLiPx8XFAa4Z4vHxJy50c3Jyys0WLzV69GhGjhzpfpybm0tiYiJpaWlER0fXKqfdbif3wwkAXNCxEx2SBtTq/fzBbreTmZlJamoqwcHB/o5TY2bPD+bvg9nzg/n7oPz+58k+lK5gIt5zQSODhuHBHC6oeBa+BUj/cAupyXHavkxERMTPjCaXUkQkIRyt/J55SGNo0suXsUREpJ5QMdwLtEy6iIjUZ4Zh8PDDD7Nw4UKWLl1KUlJSmeeTkpKIi4sjMzOTzp07A1BcXMyyZct4+umnK3zP0NBQQkPL7w8aHBzs0aJLkM0GJi3igOe/D18ze34wfx/Mnh/M3wfl9z9P9MHs34EZ/JprqbQQDq7r8qwjhazedpCU8xr7LpiIiIhU4jQ3y3UvXUREvMTq7wB1mc7fIiJSHz344IPMmzeP119/naioKLKzs8nOzqagoABwLY8+YsQIJk2axMKFC9m0aRPDhg0jIiKCIUOG+Cm1eyibnz5fREREaiK3mtuy5+QVejeIiIiIl2VkZNCxY0eio6OJjo4mJSWFTz75pMrXFBUVMWbMGFq0aEFoaCjnnXces2bN8lHi8iz7viSUvKqvuIsOwL4VvookIiL1iGaGe0HpMulWLcUmIiL1UEZGBgC9e/cuc3z27NkMGzYMgFGjRlFQUMDw4cM5dOgQPXv2ZMmSJURFRfk4rYultBiuZV1ERERMIbqak++bRoV5N4iIiIiXJSQkMGXKFFq1agXA3LlzGTRoEOvXr6d9+/YVvubmm29m7969zJw5k1atWpGTk0NJSYkvY5dVmFW9dgXVbCciIlIDKoZ7geaWiYhIfWaUjgqrgsViYfz48YwfP977gWpEZ28REREzOC/aoGF4EIcLKr+x3zAimB5JjXyYSkRExPMGDhxY5vHEiRPJyMhg1apVFRbDFy9ezLJly/jtt99o1Mh1HmzZsqUvolYuLL567cKr2U5ERKQGtEy6F7hLAJpdJiIiIiIiIuIlVV9z64pcRETqGofDwfz58zl27BgpKSkVtvnggw/o1q0bU6dOpXnz5rRp04a//vWv7q3L/MFocilFRFLl0PmQxtCkl68iiYhIPaKZ4V7gXiZdV94iIiImoXVdREREzOTXXAuHC6reOPxQvp3V2w6Scl5jH6USERHxjo0bN5KSkkJhYSGRkZEsXLiQ5OTkCtv+9ttvfPnll4SFhbFw4UL279/P8OHDOXjwYJX7hhcVFVFUVOR+nJubC4Ddbsdur/qcezp2ux1bFdfbpVfkJSV2sDhr9VneUvod1Pa78Bez5wfz98Hs+cH8fTB7fjB/Hzydv7rvo2K4F1l0Q11ERMQUtGe4iIiIueRW895JTl6hd4OIiIj4QNu2bdmwYQOHDx9mwYIFDB06lGXLllVYEHc6nVgsFl577TViYmIAmDZtGjfeeCP//Oc/CQ8Pr/AzJk+eTHp6ernjS5YsISIiolb5Gzs2cil5lT5vASg+wDcfPcMB2wW1+ixvy8zM9HeEWjF7fjB/H8yeH8zfB7PnB/P3wVP58/Pzq9VOxXAvcM8t0/10ERERczA0M1xERMRMIqt5N+PsBqHeDSIiIuIDISEhtGrVCoBu3bqxZs0aZsyYwUsvvVSubXx8PM2bN3cXwgHOP/98DMNg9+7dtG7dusLPGD16NCNHjnQ/zs3NJTExkbS0NKKjo2uV37ntEKw9fbuLOrXAOGdArT7LW+x2O5mZmaSmphIcHOzvODVm9vxg/j6YPT+Yvw9mzw/m74On85euYnI6KoZ7kW6ni4iIiIiIiHheta+3dWEuIiJ1kGEYZZY0P9kll1zC22+/zdGjR4mMjATgp59+wmq1kpCQUOl7hoaGEhpafhBZcHBwrQsWJQ0q/9yTBUUmQoAXdzzxffiT2fOD+ftg9vxg/j6YPT+Yvw+eyl/d97DW+pOkHKd7z3BddYuIiJiLzt0iIiJmkFdSvXb7j1ZcKBARETGLJ598khUrVrB9+3Y2btzImDFjWLp0KbfddhvgmtF95513utsPGTKExo0b88c//pEtW7awfPlyHnvsMe66665Kl0j3NqPJpRQR6V5RtUIhjaFJL19FEhGRekTFcG/S/XQRERFT0J7hIiJSX2RkZNCxY0eio6OJjo4mJSWFTz75pFqv/eqrrwgKCuLCCy/0bshq0DLpIiJSX+zdu5c77riDtm3b0qdPH7755hsWL15MamoqAFlZWezcudPdPjIykszMTA4fPky3bt247bbbGDhwIM8//7y/unDcaa63dTkuIiJeEtDLpE+ePJl3332XH3/8kfDwcC6++GKefvpp2rZt625jGAbp6em8/PLLHDp0iJ49e/LPf/6T9u3b+y23dh0VERExG529RUSkfkhISGDKlCnufUfnzp3LoEGDWL9+fZXX0UeOHOHOO++kT58+7N2711dxK6Vl0kVEpL6YOXNmlc/PmTOn3LF27dqRmZnppUQ1Z9n3JaHkVd2o6ADsWwGxvX2SSURE6o+Anhm+bNkyHnzwQVatWkVmZiYlJSWkpaVx7Ngxd5upU6cybdo0XnzxRdasWUNcXBypqank5Z3m5OpFhpZJFxERERERkQA0cOBABgwYQJs2bWjTpg0TJ04kMjKSVatWVfm6++67jyFDhpCSkuKjpFU7Yq9eu5zcQu8GERERkdMrzKpeu4JqthMREamBgC6GL168mGHDhtG+fXs6derE7Nmz2blzJ+vWrQNcs8KnT5/OmDFjGDx4MB06dGDu3Lnk5+fz+uuv+zm9VloVERExH528RUSk/nA4HMyfP59jx45VWeSePXs2v/76K+PGjfNhuqodrWYx/Ktf9ns3iIiIiJxeWHz12oVXs52IiEgNBPQy6ac6cuQIAI0aNQJg27ZtZGdnk5aW5m4TGhrK5ZdfzsqVK7nvvvsqfJ+ioiKKiorcj3NzcwGw2+3Y7dW8oq6E3W53L7TqdDhq/X7+UJrZjNlB+QOB2ftg9vxg/j6YPT94tg9m/h7MQ3uGi4hI/bFx40ZSUlIoLCwkMjKShQsXkpycXGHbn3/+mSeeeIIVK1YQFFT9Wwjevu6OCq5e289+yKGwqBibNbDO8Wb/fdfs+cH8fTB7fjB/H8yeH8zfB113m4fR5FIKLI0JMw5icd89P0VIY2jSy7fBRESkXjBNMdwwDEaOHMmll15Khw4dAMjOzgYgNja2TNvY2Fh27NhR6XtNnjyZ9PT0cseXLFlCRERE7bMen3D/3XffEfz7hlq/n78E0r4yZ0L5/c/sfTB7fjB/H8yeHzzTh/z8fA8kkapUejEuIiJSB7Vt25YNGzZw+PBhFixYwNChQ1m2bFm5grjD4WDIkCGkp6fTpk2bGn2Gt6+7Y0KqV9w+XGDnxTcX0zomMM/1Zv991+z5wfx9MHt+MH8fzJ4fzN8HXXebgMXGxpA/0b3o6crbFB+APe9D4mDf5RIRkXrBNMXwhx56iO+//54vv/yy3HOWU2ZxGYZR7tjJRo8ezciRI92Pc3NzSUxMJC0tjejo6FrltNvtvLj5cwA6d76QAR3Nt7SL3W4nMzOT1NRUgoOrOdw+gCi//5m9D2bPD+bvg9nzg2f7UDqTSnwhsGaNiYiIeENISAitWrUCoFu3bqxZs4YZM2bw0ksvlWmXl5fH2rVrWb9+PQ899BAATqcTwzAICgpiyZIlXHnllRV+hrevu51LMokKs5FX6Dht+5bJnRjQqVmtPtPTzP77rtnzg/n7YPb8YP4+mD0/mL8Puu42lyxbDwhpBMUHK2lhgXUjoPkgsNp8GU1EROo4UxTDH374YT744AOWL19OQkKC+3hcXBzgmiEeH3+i6JyTk1NutvjJQkNDCQ0NLXc8ODjYo7/4BQUFmfIXyVKe/j58Tfn9z+x9MHt+MH8fzJ4fPNMHs38HpqJl0kVEpB4yDKPMkualoqOj2bhxY5lj//rXv/jiiy945513SEpKqvQ9vX3dbbXAVW2bsvC7rNO2PZRfErC/T5n9912z5wfz98Hs+cH8fTB7fjB/H3TdbQ6NnVuwVFoIBzAgfxfsWwGxvX0VS0RE6oGALoYbhsHDDz/MwoULWbp0abkL7aSkJOLi4sjMzKRz584AFBcXs2zZMp5+uoolV7zMOD6rTLfTRUREzKJ06VSdvUVEpG578skn6d+/P4mJieTl5TF//nyWLl3K4sWLAdeM7j179vDKK69gtVrd25SVatq0KWFhYeWO+0OT6JBqtTuYX+zlJCIiInI6Ycah6jUsOP1ANxERkZoI6GL4gw8+yOuvv877779PVFSUe4/wmJgYwsPDsVgsjBgxgkmTJtG6dWtat27NpEmTiIiIYMiQIX7J7HAa5Ntdf/8l5ygOp4HNqhvrIiIigUx7houISH2xd+9e7rjjDrKysoiJiaFjx44sXryY1NRUALKysti5c6efU1ZP9pHCarVbsjmbx/ud7+U0IiIiUpVCy1nVaxhuvm1HRUQksAV0MTwjIwOA3r17lzk+e/Zshg0bBsCoUaMoKChg+PDhHDp0iJ49e7JkyRKioqJ8nBYWb8pi/AebyS5wFb9nfP4zb63dxbiByfTroJO4iIhI4NMANhERqdtmzpxZ5fNz5syp8vnx48czfvx4zwWqBaOa5+1f9+VTXOIkJMjq5UQiIiJSmQOWthhYseCsvJHFBo0v9l0oERGpFwL6StAwjAp/SgvhABaLhfHjx5OVlUVhYSHLli3zy3Jtizdl8cC8b8nOLbvPWvaRQh6Y9y2LN2l5FxERkYCnPcNFRERMo3nD8Gq3nbtymxeTiIiIyOk0NrZWXQgHMBxwYKVvAomISL0R0MVws3A4DdI/3FLhAqulx9I/3ILDqSVYRUREApP2DBcRETGblHMbVbvtvFU7vJhERERETkd7houIiL+oGO4Bq7cdJKuKvcoMIOtIIau3HfRdKBEREak27RkuIiJiPj2TGmGt5ji2HQcLKC45zWw0ERER8RrtGS4iIv6iYrgH5ORVXgg/k3YiIiLiL5oZLiIiYhY2q4UOzaKr3X72V795MY2IiIhUpXTP8Cppz3AREfECFcM9oGlUmEfbiYiIiJ9oz3ARERFTGdipebXbfrop24tJREREpCraM1xERPxFxXAP6JHUiPiYsErnklmA+JgweiRVfz8zERER8SXtGS4iImJGQy9uWe22Ow7mey+IiIiIVEl7houIiL+oGO4BNquFcQOTgfK30EsfjxuYjK26m5mJiIiIT2nPcBEREXMKCbLSNCq0Wm0PF9hxOHXOFxER8YdCYqrXMKypd4OIiEi9o2K4h/TrEE/G7V2IjS57ER4XE0bG7V3o1yHeT8lERESk+jRwTURExGz+dGlStdo5nLDq1wNeTiMiIiK1onFrIiLiYSqGe1C/DvEs/ctlnB3mOmM/3q8tXz5+pQrhIiIiZqE9w0VERExn2CXVK4YDzFn5mxeTiIiISGWqvUx6YbZ3g4iISL2jYriH2awWQo9/q8nNYrQ0uoiIiCloz3ARERGzCgmy0iy6ekulZ/6wj/fW7vZyIhERETlVqJFbvYbZn3s3iIiI1DsqhnuBbqeLiIiYi/YMFxERMbf4s8Kr3XbEO9/R/u+LOFpY4sVEIiIicrIiS3T1Gu56G5wO74YREZF6JcjfAeqi0tvpVi21KiIiYjI6d4uIiJhR4lkRrNtxuNrtj9kNOoz/1P3YaoGkxhG8ff8lNIoM8UJCERGR+q3Q2rh6DUuOQs5SiOvj1TwiIlJ/qBjuDcer4aqFi4iImIVO3iIiImZ2Q5cE3tvw+xm/3mnAr/vz6TIhs9xzFqBJVAgTBl1An+RYbYcmIiJyBg5YkzEsEViM/NM3/vFFFcNFRMRjtEy6F2iZdBERqc+WL1/OwIEDadasGRaLhffee6/M84ZhMH78eJo1a0Z4eDi9e/dm8+bN/glbjs7eIiIiZnRxq7O9doPDAHLyirl33jpaPbmIxZuyvPRJIiIidZjFhtEgqXptf38Pdrzj1TgiIlJ/qBjuBe5dR3U/XURE6qFjx47RqVMnXnzxxQqfnzp1KtOmTePFF19kzZo1xMXFkZqaSl5eno+Tnkx7houIiJiZzWph2o0dvf45BnD/vG9VEBcRETkTNalGfHUT7HrXa1FERKT+UDHci7RnuIiI1Ef9+/dnwoQJDB48uNxzhmEwffp0xowZw+DBg+nQoQNz584lPz+f119/3Q9pT6Vzt4iIiFld1y2RZtG+2Q1u+LxvOZJv98lniYiI1BlB0TVrv+JmKC7wThYREak3tGe4Fxil2476N4aIiEjA2bZtG9nZ2aSlpbmPhYaGcvnll7Ny5Uruu+8+v+SyaM9wERGROmHlk30594mPcXr5c5xAp6eWnNFrw4MtDOgQz0XBns0kIiIS6IyEQXDw6xq8wgHvRFTyXDCEnAWR7cCRB8W5YBRCWCw07gGdp0FIuCdii4iIyakY7gXuPcN1Q11ERKSM7OxsAGJjY8scj42NZceOHZW+rqioiKKiIvfj3NxcAOx2O3Z77WZlnfz6kpISjFq+nz+U9qG234W/mD0/mL8PZs8P5u+D8vufJ/tg5u+hLvhtytWc98THOPwdpBIFdoMF639nATYW565n5rAe/o4kIiLiE87WD2H7/gkPvZsdinPgYE7Zw4V74PC38Ou/ocH5cPUGCArx0GeKiIgZqRjuBaXFcKtq4SIiIhU6dcCYYRhVDiKbPHky6enp5Y4vWbKEiIjKRolXX+/jy7qsXrOGfTbzFjAyMzP9HaFWzJ4fzN8Hs+cH8/dB+f3PE33Iz8/3QBKpjV+nXE3XpzI5kF/s7yhV+vzHfdzzyhr+c2d3f0cRERHxPmsItH4Yfn7BN5937Ad4KxTOfww6T63Za0uK4Zd/Qd6vEHUeJN3jnYwiIuJ1KoZ7g1ZaFRERqVBcXBzgmiEeHx/vPp6Tk1NutvjJRo8ezciRI92Pc3NzSUxMJC0tjejoGu45dgq73Y79fdffe/ToiRHbp1bv5w92u53MzExSU1MJDjbfmqtmzw/m74PZ84P5+6D8/ufJPpSuYCL+tW5sKukfbGb2yu3+jlIJ102DzC05FBQ7CA+x+TmPiIiID3R/Hn57DRwHffeZP/zD9VMLQd/+mWSuAQZ4JpOIiPiMiuFeYLj/pmq4iIjIyZKSkoiLiyMzM5POnTsDUFxczLJly3j66acrfV1oaCihoaHljgcHB3uk6FJy/OwdFBQMJi3igOe+D38xe34wfx/Mnh/M3wfl9z9P9MHs30FdMu7a9owecD4j3/qWj77f6+84lTp/7OIyjyNDbQy4IJ70azuoSC4iInXPLQfgdSsn30kPdBagFR/hfD8Rbsz2dxwREakBFcO94MSe4X6NISIi4hdHjx7ll19+cT/etm0bGzZsoFGjRpxzzjmMGDGCSZMm0bp1a1q3bs2kSZOIiIhgyJAhfkwtIiIidVVIkJUXh3Rjxq0GSzfvZeyH37MnN7C3RTla5OCttbt5a+1uUpObahl1ERGpe248Bu/UftszX7MW74XXK7jxbw2HplfAJW9CaKTvg4mISKWs/g5Ql1lVDRcRkXpo7dq1dO7c2T3ze+TIkXTu3JmxY8cCMGrUKEaMGMHw4cPp1q0be/bsYcmSJURFRfkxtXsomx8ziIiIiDfZrBb6XBDHV0+msX3K1e6f78am0TG+gb/jVSpzSw73vLLG3zFERMRPMjIy6NixI9HR0URHR5OSksInn3xSafulS5disVjK/fz4448+TF0NIeEQe7W/U9SIhSruGjgLIHsRLIiCxT18mEpERE5HM8O9wFm6Z7h/Y4iIiPhF7969MYzKlzqzWCyMHz+e8ePH+y5UdWkgm4iISL0TExHMB4/2LnNs8qItvLR8m38CVSBzSw4tn/i42u2jw2zcc+l53Nf7PEKCNA9CRMTMEhISmDJlCq1atQJg7ty5DBo0iPXr19O+fftKX7d161aio6Pdj5s0aeL1rDXW5yN4uyHYj/g7iWcdXFPx7PFKWSAoChJvhq7PuwYKiIiIx6gY7kW6ny4iImIOFs0MFxERkZOMHpAMEFAF8ZrILXTw7Gc/8exnP3HfZUnu/oiIiPkMHDiwzOOJEyeSkZHBqlWrqiyGN23alIYNG3o5nQfcdBgWdYPD6/ydxI8MKMmFbf91/XiBDbgGsLx90kFLCDTtA5e+paXdRaROUzHcC0pvp2uZdBERERERERFzGj0gmb+kteOl5T+T8cUv5Jf4O9GZeWn5tiqL+pGhNgZcEE/6tR0ID7G5jzucBqt+PcCKX3LYuDuX8BArcdHhXJjYkP1HC1i3w8IPmT/RODKcs6NCiYsOo0dSI2xW3QsREfEWh8PB22+/zbFjx0hJSamybefOnSksLCQ5OZm//e1vXHHFFVW2LyoqoqioyP04NzcXALvdjt1ur1Xu0tdX+j6pX0PJUawftsRakqth6l5Q4ToxRjHs/QRjQRTOht1wpq70dawaOe2/RwHO7PnB/H0we34wfx88nb+676NiuDdUvjKsiIiIBCTNDBcREZHyQoKsPHxlWx6+si0Op8HSzXv5+wff8XueSSvjFTha5OCttbt5a+3uarWf983O43+zkfn79jLP2Szwj8EXMLj7OZ4NKSJSz23cuJGUlBQKCwuJjIxk4cKFJCdXvOpHfHw8L7/8Ml27dqWoqIhXX32VPn36sHTpUi677LJKP2Py5Mmkp6eXO75kyRIiIiI80o/MzMyqG4S+ArZ8uhf9gyasx4qriKsrde+zHl7LkXeSWRHxjL+jnNZp/z0KcGbPD+bvg9nzg/n74Kn8+fn51WqnYrgXuG+n6ywtIiJiLjp5i4iISCVsVgt9LoijzwVxNX5t5/GfcKjQ6YVUgcVhwMgFG5m0+EfW/j3N33FEROqMtm3bsmHDBg4fPsyCBQsYOnQoy5Ytq7Ag3rZtW9q2bet+nJKSwq5du3jmmWeqLIaPHj2akSNHuh/n5uaSmJhIWlpamb3Hz4TdbiczM5PU1FSCg4Or8YobAXACTmcx1p//BTnLsRTsAVsIEIZRdADLsd+AAkrvyJde0evKvmYsuL7Bs4xfGJB2GQQF5pLpNf/3KLCYPT+Yvw9mzw/m74On85euYnI6KoZ7gZZJFxERMRftGS4iIiLetH58f9qMWUSxo34sJbf/mJ2WT3xMiBVCgm3ERoWQeFYEuUUOrBZIS45j2CVJhARVuGiriIicIiQkhFatWgHQrVs31qxZw4wZM3jppZeq9fqLLrqIefPmVdkmNDSU0NDQcseDg4M9VnA5s/cKhg6PAY+dvumy62DP+2eQTErvhgSvvgsuX+jXLKfjyX8n/cHs+cH8fTB7fjB/HzyVv7rvoWK4F2hmuIiIiIiIiIic7KeJA7h6xgo2Z1Vv9kJdUOyE4iIHR4sK+HV/gfv42h2HmfTJj5W+rrJ9zEVExMUwjDL7e5/O+vXriY+P92KiAHH5e1BcAGsfhO3zcc0alxrZ8x68XtPChgWCYqDdXyH5MQgK8UYyEZEzpmK4FxjHq+EWzS4TEREJfIYDm1Ho+vvBDXD2xWDVTVcRERHxvI8f7cXRwhIefHUVy349hGsnVDnVyfuYpyY35T93dvd3JBERv3nyySfp378/iYmJ5OXlMX/+fJYuXcrixYsB1/Lme/bs4ZVXXgFg+vTptGzZkvbt21NcXMy8efNYsGABCxYs8Gc3fCckHC6e5fqpjV3vwoqbAYdHYtVtBpQchk1/c/2c/xh0nurvUCIibiqGe5FmhouIiAS4Xe8StPYRgjnoevztI/DjVOg6AxIH+zebiIiI1EmRYUH8d1hPFi1axIAB/dxL+y3elMX98771c7rAk7klh3teWaOCuIjUW3v37uWOO+4gKyuLmJgYOnbsyOLFi0lNTQUgKyuLnTt3utsXFxfz17/+lT179hAeHk779u35+OOPGTBggL+6YE6Jg+HWIvj9I/hmBM6i7Ri4lhPXULbT+OEfrj9VEBeRAKFiuBec2DPcrzFERESkKrvehRU3cuLMfVz+HtfxXu+oIC4iIiI+069DPL9OGkDnp5aQW1ji7zgBJXNLDi2f+LjMMQvQqEEwf7wkiXsvO6/G+48Xlzj5Yo+FD19fT1RoMIO7JHBxq7Ox6WaOiASYmTNnVvn8nDlzyjweNWoUo0aN8mKiesRqg4RBkDAIh91+fCDbAKyle9SuH3Wi8Ctl/fCPKr4bG9gioOllcPF8CI0EpwP2rYCCLAhr6nqcswyObXctxWuxYg1PpHFJOBh9gVP2CXY6YO/n8MtMyP4K7HuBEtdnEYrr3o/d1dYaCuHxrns+8WkQ29t1PHsJbPoHHFwPzmOuY5ZgMIKBo7hXCbCEQHQydJoEzdK0uqCICagY7gWG+566LqBEREQCktMB6x4FjArO1sfHeq8bAc0H6aJGREREfMZmtfD9+L4cPFrMDf9cxrZDxf6OFLAM4MAxO88s+Ylnlvx0hu9iA/YBsHDD756KVrMEFujQLJpX7r6ImIjg079AREQCR+epcMEE+OEZ2PwsOA/6O5FJOMCRB1kfw4Koar/KBlwKGO+Mq9lnkV/2kLMEjv3iWhnwx9PMXjfsFRwrhiMbYPkAV7H80vmaTCES4FQM9yItky7if4ZhUFJSgsNR/f197HY7QUFBFBYW1uh1gcTsfTB7fqhZH2w2G0FBQVh04vCdfSsgf3cVDQzI3+VqVzpCWET8zuFwYLdXcDMC8587lN//atqH4OBgbDYNmBLvaBQZwv8eT63RayYv2sJLy7d5KZF4i8OA7/bk0umpJZzTKIzlo/r4O5KIiNREUAhc8KTr50yUFPukmO7ENXWvLtx5Cqg+GHZYcQP0WqCCuEgAUzHcC04skx5Q/1sWqXeKi4vJysoiPz//9I1PYhgGcXFx7Nq1y7TFSbP3wez5oeZ9iIiIID4+npCQEB+kEwqyPNtORLzu6NGj7N69G8MwKnze7OcO5fe/mvbBYrGQkJBAZGSkD9KJnN7oAcn8Ja0dLy3/mf8u+4UjRf5OJDW182AhbUZ/zF/7tWP34QJaNIrgjpSWNV4CXkRETKQmxfSTlxIPj4cmvaq9mp3DbufAO92JM76rZWCp0IobKn/OGg5Nr4CUeb7LIyJlqBjuBaX358x5C0ikbnA6nWzbtg2bzUazZs0ICQmp9o1Zp9PJ0aNHiYyMxGo1500Hs/fB7Pmh+n0wDIPi4mL27dvHtm3baN26tWn7bCrh8Z5tJyJe5XA42L17NxERETRp0qTCc7rZzx3K73816YNhGOzbt4/du3fTunVrzRCXgBESZOXhK9vy8JVtT9u2uMRJxrKf+PcXv1JwRgs6OAFz/vceyIoNmPTJj+7H//fxD2WeDw2ykHJuY567qaOvo4mIiL9ZbbVave6b0NFcW3ir6ha+5iyA7EUELWzEQIC3bRDZBlK/hPBG/k4nUi+oGO4FpXNVTDohQqROKC4uxul0kpiYSERERI1e63Q6KS4uJiwszNQ3c83cB7Pnh5r1ITw8nODgYHbs2OF+jXhZk14QkQD5ezhx5j6ZxfV8k16+TiYiFbDb7RiGQZMmTQgPD6+wjdnPHcrvfzXtQ5MmTdi+fTt2u13FcDGlkCArj/Zpx6N92tX4tXa7nUWLFjFgQD9ueGkV3+/O9UJCqUhRicHSn/bTeeIXNAu1MGCAvxOJiIhp2MJwxg/ElvWhv5PUSyeWqXfA0R9gYWMIi4XB2f4NJlIPmPMuhUlYNMZKxO/MejNW6h/9u+pjVht0nQGAUe58ffxx1+nVXm5MRHzDrMtvS92kfx9FXD54qBebxvfl8vMa+jtKvfN7kZXWf1/CoBdXcCTf7u84IiJiAs5LF0DzQf6OIaUK98K7cf5OIVLnaWa4F2hmuIiIiAkkDoZe78DaR6Bgz4njEQmuQnjiYL9FExERETGTyLAg5t5zCeBafn32V7/x6aZsso7kc6zATq7qtF7iuvH03e5cOj21hHMahbF8VB8/ZxIRkYB3+XtQXABrH4Tt84ECfyeq3wr3QsFBLZku4kUqhnuYw2ngcLr+vn7XIZo1DMdmVVVcRPyrd+/eXHjhhUyfPr1a7bdv305SUhLr16/nwgsv9Go2Eb9KHExJ7ACs70Rjww4Xvwbn3KIZ4SJiOnPmzGHEiBEcPnzY31FEpJ4LCbJy3+WtuO/yVjV6Xcfxn5JbWOKlVPXDzoOFdJ+QyZq/pfo7ioiIBLqQcLh4luvnTCy7Dva879FI9drnl8M1G/2dQqTO0pqsHrR4Uxa9n12O8/jI3Efe2MClT3/B4k1Zfk4mIrXhcBp8/esB3t+wh69/PYDDWdH+wp5hsViq/Bk2bNgZve+7777L//3f/1W7fWJiIllZWXTo0OGMPu9MpKWlYbPZWLVqlc8+UwQAy0mF7yaXqhAuUpc5HbB3KWx/w/Wn0+G1j/LWOR2gZcuW5Qa43XLLLfz000+1C10DBQUFnHXWWTRq1IiCAs0kEZHa+358X85pFO7vGKa372gxLZ/4mJZPfEyrJz8+oyXUfXkNLCIiJnX5e1pu3ZNyN8HrFngjFD4fAEVH/Z1IpE7RzHAPWbwpiwfmfcuplwfZRwp5YN63ZNzehX4d4v2STUTO3OJNWaR/uIWsI4XuY/ExYYwbmOyV/6azsk4MnnnzzTcZO3YsW7dudR8LDy97c8hutxMcHHza923UqGbL7NhsNuLifLdfzc6dO/n666956KGHmDlzJj169PDZZ1ekut+r1BGGAwvHZyHtXw3hzVUQF6mLdr0L6x6F/N0njkUkQNcZXtkWoabn9NoKDw/3+HtWZcGCBXTo0AHDMHj33Xf5wx/+4LPPPpVhGDgcDoKCdHkrYnbLR13JkXw7t7/8JRuz8/0dx/RKnCeWUPeHICu0bxbNK3ddREyErq9EROocLbfueUYx7P0EFkR54c2tYA2H8HjXNXB8GsT21j0wqRc0M9wDHE6D9A+3lCuEw4n9w9M/3KKRtCIms3hTNg/M+7ZMIRxODHLxxqoPcXFx7p+YmBgsFov7cWFhIQ0bNuStt96id+/ehIWFMW/ePA4cOMAf/vAHEhISiIiI4IILLuCNN94o8769e/dmxIgR7sctW7Zk0qRJ3HXXXURFRXHOOefw8ssvu5/fvn07FouFDRs2ALB06VIsFguff/453bp1IyIigosvvrjMTX2ACRMm0LRpU6KiovjTn/7EE088Ua1l1mfPns0111zDAw88wJtvvsmxY8fKPH/48GHuvfdeYmNjCQsLo0OHDnz00Ufu57/66isuv/xyIiIiOOuss+jbty+HDh1y9/XU2XMXXngh48ePdz+2WCz8+9//ZtCgQTRo0IAJEybgcDi4++67SUpKIjw8nLZt2zJjxoxy2WfNmkX79u0JDQ0lPj6ehx56CIC7776bW265pUzbkpIS4uLimDXrDJegEs/b9S5BH7fCWnrG/uom+KClq2gmInXHrndhxY1lC+EA+Xtcx73w33xV5/S4uDiWL19O165dCQsL49xzz+Wpp56ipOTE8sDjx4/nnHPOITQ0lGbNmvHII48ArnP6jh07+POf/+yeZQ6uZdIbNmxY5vUXXnghr776Ki1btiQmJoZbb72VvLw8d5u8vDxuu+02GjRoQHx8PM8991y53xkqM3PmTG6//XZuv/12Zs6cWe75zZs3c/XVVxMdHU1UVBS9evXi119/dT9f2fnz1N9BwPV7gMViYenSpcCJ30s+/fRTunXrRmhoKCtWrODXX39l0KBBxMbGEhkZSffu3fnss8/K5CoqKmLUqFEkJiYSGhpK69atmTlzJoZh0KZNG1544YUy7Tdt2oTVai2TXUS8KyYimA9HXMH2KVdX+vPDU/24sWs8obW+q+X0RGSpxMnF+Mumfu7vOCIi4g2ly60PyYchRuU/NxdB52ch/lqI7AhE1vijnICDapy9LeEQlghRnaBBOwhpCiHxENEKoruBpWHVr7dGQEQbCD8PgmLBFgmhcZh/bqkTnMfg2C/w41T431UwP8g1I939EwSf9ITCI/4OK+JRZv+vNyCs3nawXLHsZAaQdaSQ1dsOknJeY98FE5EyDMOgwH765VCdTidHC0tI/6jyQS4WYPwHW7ik1dnYrJbTvmd4sM19s7q2Hn/8cZ599llmz55NaGgohYWFdO3alccff5zo6Gg+/vhjhg4dyuLFi7nyyisrfZ9nn32W//u//+PJJ5/knXfe4YEHHuCyyy6jXbt2lb5mzJgxPPvsszRp0oT777+fu+66i6+++gqA1157jYkTJ/Kvf/2LSy65hPnz5/Pss8+SlJRUZX8Mw2D27Nn885//pF27drRp04a33nqLG264AXD98+jfvz95eXnMmzeP8847jy1btmCzuUYtbtiwgT59+nDXXXfx/PPPExQUxP/+9z8cjpotfTtu3DgmT57Mc889h81mw+l0kpCQwFtvvcXZZ5/NypUruffee4mPj+fmm28GICMjg5EjRzJlyhT69+/PkSNH3N/H3XffTe/evcnKyqJ58+YALFq0iKNHj7pfL35WWhw79b/00uJYr3e8MltURDzAMMBxyoxBpxNKjkGJDawnVUecDlj7COX+W3e9EWCBtY9C7FXVGxFvi4BantM//fRTbr/9dp5//nl3kfjee++lqKiIiRMn8s477/Dcc88xf/582rdvT3Z2Nt999x3g2vqkU6dO3Hvvvdxzzz1Vfs6vv/7Ke++9x0cffcShQ4e4+eabmTJlChMnTgRg5MiRfPXVV3zwwQfExsYyduxYvv3229MOZPv111/5+uuveffddzEMgxEjRvDbb79x9tlnA7Bnzx4uu+wyevfuzRdffEF0dDRfffWVu9hf1fmzJkaNGsUzzzzDueeeS8OGDdm9ezcDBgxgwoQJhIWFMXfuXAYOHMjWrVs555xzALjzzjv5+uuvef755+nUqRPbtm1j//79WCwW/vjHPzJv3jzGjBnj/oxZs2bRq1cvzjvvvBrnExHvCQ+x8cxNXXjmpjN/D7vdzqJFixgwoJ9fVoW69sUVfL871+ef6y/az1xEpJ4LCoHzR7p+zpDDfe4egNVfKzr+7xrI+tg/n+0TDji0Gt5tCJHnwbW/+DuQiEeoGO4BOXmVF8LPpJ2IeEeB3UHy2E898l4GkJ1byAXjq7fc3Jan+hIR4pn/5Y4YMYLBg8sW6P7617+6//7www/zySef8P7771dZDB8wYADDhw8HXAX25557jqVLl1ZZDJ84cSKXX345AE888QRXX301hYWFhIWF8cILL3D33Xfzxz/+EYCxY8eyZMkSjh6teo+bzz77jPz8fPr27QvA7bffzuzZs93F8M8++4zVq1fzww8/0KZNGwDOPfdc9+unTp1Kt27d+Ne//uU+1r59+yo/syJDhgzhrrvuKnMsPT3d/fekpCRWrlzJW2+95S5mT5gwgb/85S88+uij7nbdu3cH4OKLL6Z169bMmzePxx9/HHDNgL/pppuIjKz56FfxMKfDtVwyBuVLWseLY+tGuPa/0nJRIoHHkQ9vlf1/qRVoeEZvZkDBbngnpnrNbz4KQQ3O6JNKTZw4kSeeeIKhQ4cCrvNaeno6jz/+OBMnTmTnzp3ExcVx1VVXERwczDnnnOPeQqRRo0bYbDaioqJOu6WJ0+lkzpw5REW5lti74447+Pzzz5k4cSJ5eXnMnTuX119/nT59+gCu81SzZs1Om3/WrFn079+fs846C4B+/foxe/ZsHnvsMQD++c9/EhMTw/z5890FptJzOFR9/qyJp556itTUE0WVxo0b06lTpzKfs3DhQj744AMeeughfvrpJ9566y0yMzO56qqrgLK/UwwbNoxx48axevVqLrroIux2O/PmzeMf//hHjbOJiJzOBw/14mhhCZdP/ZwD+SWnf0EdsO9oMQvW7eaGrgn+jiIiInJmrvjItTT8mgdgx+uA3d+JvOfor67Z4iexAdcAlrf9kqjWfJ8/CIIbwlkdod0oaFbJIHynA/atgIIs1zL2DbvB909A3s8Q1Ro6/cO1CoOcMRXDPeDsyFCPthMRqUq3bt3KPHY4HEyZMoU333yTPXv2UFRURFFREaGhVf8/p2PHju6/ly7dmpOTU+3XxMe79kzPycnhnHPOYevWre7ieqkePXrwxRdfVPmeM2fO5JZbbnHv8/mHP/yBxx57jJ9//pmuXbuyYcMGEhISytxEP9mGDRu46aZaTAk57tTvFeDf//43//3vf9mxYwcFBQUUFxe7Z8vl5OTw+++/uwsIFbnjjjuYM2cOjz/+ODk5OXz88cd8/rmWBwwI+1aUXy65DAPyd7naxfb2VSoRqSfWrVvHmjVr3DO0wXU+LywsJD8/n5tuuonp06dz7rnn0q9fPwYMGMDAgQNrvCd2y5Yt3YVwcJ27S8/1v/32G3a73V1kB4iJiaFt27ZVvqfD4WDu3Llltg65/fbb+fOf/8zIka5ZHhs2bKBXr14VzrSszvmzuk49dx87doz09HQ++ugjfv/9d0pKSigoKGDnzp3uXDabzT2w71Tx8fGkpaUxe/ZsLrroIj766CMKCws98nuGiEhFIsOCWDe2LwXFDv7+/nd8uD6Lojq+cvtf3v6Ov7z93UlHrDz6tX/2NPcMs+cHs/ch2GJleeEmJlzfkfAQDWQWER8ICYdL5rh+quJ0wO8fwfKbqCtFc7Pvu+z7/CVg3w85X7h+zkT2Evj5n+6HdWFAwgCCsS6/Ci59C0J9M2lMxXBPqO5W4NoyXMSvwoNtbHmq72nbOZ1Olm3ezYNv/3DatnP+2J0eSY2q9dme0qBB2dlozz77LM899xzTp0/nggsuoEGDBjz66KMUFxdX+T6n3qC2WCw4nVXfeTn5NaXLvp/8mlOXgjeMqv/Hd/DgQd577z3sdjsZGRnu4w6Hg3nz5tG1a1fCw6se9Xa6561Wa7kcdnv5X0BP/V7feust/vznP/Pss8+SkpJCVFQU//jHP/jmm2+q9bkAt956K+np6Xz99dd8/fXXtGzZkl69ep32deIDBVmebScivmWLcM3QPonT6SQ3N5fo6GisJy+TnrMclg44/Xv2XgRNL6veZ9eS0+kkPT29zEovTqeTo0ePEhYWRmJiIlu3biUzM5PPPvuM4cOH849//INly5bVaCnfqs71pefGmp67P/30U/bs2cMtt9xS5rjD4eCLL77ghhtuqPIcWZ3z9qk5KjpvQ/lz92OPPcann37KM888Q6tWrQgPD+fGG290/05UnXP3HXfcwQMPPMD06dOZPXs2t9xyCxERtf9nLiJSleos+/5/H21h5pfbfBfKZ3Rb3f/M3Qe7YWXB+t9ZsP53UpOb8p87a77ajIiIV1htkDAIBu+Ddxu6t94UqQ1zn7Vd+a3YYe8nsCAKGnWHfqt98rlSS7O++q1a7fYfK/JyEhGpisViISIkqFo/FyWdRVx0WKW/oFiA+JgwerVuUq3389R+4RVZsWIFgwYN4vbbb6dTp06ce+65/PKL7/dzadu2LatXlz1xrV27tsrXvPbaayQkJPDdd9+xYcMG90/pPqklJSV07NiR3bt389NPP1X4Hh07dqxytnWTJk3IyjpR0MzNzWXbttPfRFqxYgUXX3wxw4cPp3PnzrRq1Ypff/3V/XxUVBQtW7as8rMbNWrEoEGDmD17NrNnz3YvIS8BIHdr9dqFNvVuDhE5MxaLa6ny6vzEpUFEApXfdrBARKKrXXXezwPn9C5durB161ZatWpV5ufcc891F4PDw8O59tpref7551m6dClff/01GzduBCAkJASHw1GrDOeddx7BwcFlzt25ubn8/PPPVb5u5syZ3HrrrWXO2xs2bGDIkCHMmzcPcJ2bV6xYUWER+3TnzyZNmgCUOXdv2LChWn1asWIFw4YN4/rrr+eCCy4gLi6O7du3u5+/4IILXIMely2r9D3S0tJo0KABGRkZfPLJJ+W2UBER8Ze/X5PMTxP683CfJH9HEQlYmVtyuOeVNf6OISJSVliMa/9tNF9SpJyDa2Bxj9O3qyXNDK+l4hInn/+4r1pttUy6iHnYrBbGXnM+D76+Hgtlf1EpvQU+bmAyNqv/x/O1atWKBQsWsHLlSs466yymTZtGdnY2rVq18mmOhx9+mHvuuYdu3bpx8cUX8+abb/L999+X2YvzVDNnzuTGG2+kQ4cOZY4nJibyxBNP8PHHH3P99ddz2WWXccMNNzBt2jRatWrFjz/+iMVioV+/fowePZoLLriA4cOHc//99xMSEsL//vc/brrpJs4++2yuvPJK5syZw8CBAznrrLP4+9//js12+pn6rVq14pVXXuHTTz8lKSmJV199lTVr1pCUdOLm0/jx47n//vtp2rQp/fv3Jy8vj6+++oqHH37Y3ebuu+/m2muvxeFwuPeGFT9zOmDri9Vra9Su2CQiAcBqg64zYMWNUNlZvev0ivft8pKxY8dyzTXXkJiYyE033YTVamXDhg2sW7eOqVOnMmfOHBwOBz179iQiIoJXX32V8PBwWrRoAbiWP1++fDm33noroaGhnH322TXOEBUVxdChQ3nsscdo1KgRTZs2Zdy4cVit1koH8e3bt48PP/yQDz74oNy5+84772TgwIHs27ePhx56iBdeeIFbb72V0aNHExMTw6pVq+jRowdt27at8vwZHh7ORRddxJQpU2jZsiX79+/nb3/7W7X61KpVK959910GDhyIxWLh73//e5kVbFq2bMnQoUO56667eP755+nUqRM7duwgJyeHm2++GQCbzcbQoUMZPXo0rVq1IiUlpcbfrYiIt4QEWflLajI/Zh0jc0sOaI6ZSDmZW3IoKHZoyXQRCSzX/oLzvVZY8389fVuR+ubgGig66tUl0zUzvJZe/Xp79Rtr2I+IqfTrEEfG7V2IiwkrczwuJoyM27vQr0O8n5KV9fe//50uXbrQt29fevfuTVxcHIMGDfJ5jttuu43Ro0fz17/+lS5durBt2zaGDRtGWFhYhe3XrVvHd999xw033FDuuaioKK644gpmzZoFwIIFC+jevTt/+MMfSE5OZtSoUe4ZcW3atGHJkiV899139OjRg5SUFN5//333vqqjR4/msssu45prrmHAgAFcd911nHfeeaftz/3338/gwYO55ZZb6NmzJwcOHCi3J/rQoUOZPn06//rXv2jfvj3XXHNNuRl1V111FfHx8fTt25dmzZqd/osU79u3AuwHqt9WRMwvcTD0egcimpc9HpHgOp44uOLXeUnfvn356KOPyMzMpHv37lx00UVMnz6dxMREABo2bMh//vMfLrnkEvcKKB9++CGNGzcG4KmnnmL79u2cd9557pnUZ2LatGmkpKRwzTXXcNVVV3HJJZdw/vnnV3rufuWVV2jQoEGF+31fccUVREZGMm/ePBo3bswXX3zB0aNHufzyy+natSv/+c9/3Mu2n+78OWvWLOx2O926dePRRx9lwoQJ1erPc889x1lnncXFF1/MwIED6du3L126dCnTJiMjgxtvvJHhw4fTrl077rnnHo4dO1amzV133UVxcbFmhYtIwPrPnd3p0+7M//8vUtdNWrTF3xFERMpxXv0DH4TMw2EJ8XcUkcCz6g6vvr3FON2mcPVAbm4uMTExHDlyhOjo6Bq9duz7m3jl6x3Vajvj1gsZdGHz0zf0M7vdzqJFixgwYECN9iQMFMrvf4HQh8LCQrZt20ZSUlKlN3Qrc+qeow6nweptB8nJK6RpVBg9khoFxIzwqlS6b6qPpaamEhcXx6uvvlqj1wVK/too7UNQUBAJCQnMmjWrzN6wp6rq39nanKfqqlp9J9vfgJVDqte2/d+g0//VPKCPBcL/d2vD7PnB/H0I9PzVOa9X69zhyZZvQgAAIHtJREFUdLgGuRRkQXg8NOnl0xnhVQmEc9+xY8do3rw5zz77LHfffXeNXhsI+WurtA8bN27kyiuvZPfu3cTGxlbaXufumvHkdxLo/8+qDrP3wez5wfx9sNvtvPfhIr4ubsb7G3LQekYiJ/RqfTav3t2zRq/Rubs8nbvLMnsfzJ4fzN+HMvkd+fB5Ghzx/l7JIqYQcwFc/X2NX1bdc5WWSa+lFo0iqt22aVTNCnIiEhhsVgsp5zX2d4yAl5+fz7///W/69u2LzWbjjTfe4LPPPiMzM9Pf0fzC6XSSlZXFf/7zH2JiYrj22mv9HUlKhddgVYfY3l6LISJ+YLXpv+uTrF+/nh9//JEePXpw5MgRnnrqKQC/rDATCIqKivjtt98YN24cN998c5WFcBGRQBBig6dvuJBpt5YvCBzJt3P7y1+yMTvfD8lE/Ktl4+rfrxUR8YuwGLj6mxOPnQ74/SP4ZgQUbfdXKhH/iTz9Sq61UWeK4f/617/4xz/+QVZWFu3bt2f69On06tXL6597R0pLJnz8w2lXQI+Pcc0mFRGpqywWC4sWLWLChAkUFRXRtm1bFixYwFVXXeXvaH6xc+dOkpOTSUhIYM6cOe5l2yUANOkF4c2hYE/V7UIaQ9PePokkIuIvzzzzDFu3biUkJISuXbuyYsWKM9qDvC544403uOeee7jwwgtrvKqNmWRkZJCRkcH27dsBaN++PWPHjqV///4Vtn/33XfJyMhgw4YNFBUV0b59e8aPH0/fvn19mFpEaiomIpgPR1zh088sLnHS5m+f+PQzRSry5IBkf0cQEakZqw0SBrl+zkRJMfzyL8j7FaLOg1bDISjkxHM/vwh7voB9X4Fx2GOxRTzmIu9eg9eJO/NvvvkmI0aM4F//+heXXHIJL730Ev3792fLli2cc845Xv3skCAr916WxEvLt1XZbtzA5IBfVllEpDbCw8P57LPP/B0jYLRs2ZJDhw6ZernYOstqg27Pw4ry+9WX0fPlgFk+WUTEGzp37sy6dev8HSNgDBs2jMGDB9f5c3dCQgJTpkyhVatWAMydO5dBgwaxfv162rdvX6798uXLSU1NZdKkSTRs2JDZs2czcOBAvvnmGzp37uzr+CISwEKCrNxXjXtkIt6UmtyU8BBdx4lIPRMUAu1GVP7c+SNdPxVZPwp++IfXoomcVqPuEBrp1Y+oE1f406ZN4+677+ZPf/oT559/PtOnTycxMZGMjAyffP7oAcncd1kSFZW6G4Ta+PftXejXoQZLsoqIiNQD//rXv9z7rJbORvSZxMHQa4Fr9vepQhq5nkusfI93ERERsxo4cCADBgygTZs2tGnThokTJxIZGcmqVasqbD99+nRGjRpF9+7dad26NZMmTaJ169Z8+OGHPk4uImZQeo9MxB9Sk5vynzu7+zuGiIi5dJ4K5z9WraZOwHH8TxGPaNQd+q32+seYfmZ4cXEx69at44knnihzPC0tjZUrV/osx+gByfwlrR2zvvyFj7/ZyrnnNOPGrudwcauzNSNcRETkFP5c1cUtcTA0H0TJ75/x69czadWqFba4Pq79hDUjXERE6gGHw8Hbb7/NsWPHSElJqdZrnE4neXl5NGqkbcBEpGKl98heWv4z/132C0eKqmrtxNxzdcyeH8zeh2CLk2svTGDC9R01I1xE5Ex1ngoXTKh8qfXjHHY7ixYtYsCAAViDgz33+U4H7FsBBVkQHu/a4rD03pzTATlLYe9S1+Omvcvfuzv59WFNwQCKclx/dzpg7//gwBqcjgJ2Hw6i2SVPENQsrfx7ZC+BH6aB/TCc1Q06T4OQ8BPvn78HivZBaBOIaF4258lKiuGn52H7O3B4M3DUc18Vru5ZMOfZ2zWgIhhr7FXYLn3L6zPCS5m+GL5//34cDgexsbFljsfGxpKdnV3ha4qKiigqOvGbeG5uLgB2ux273X7GWSzAnT0SiD/yA6mp5xMcHIzTUYLTccZv6Rel30Ftvgt/Un7/C4Q+lJSUYBgGDocDp7NmY9UMw3D/WdPXBgqz98Hs+aHmfXA4HBiGQUlJSbn/dsz8/4PKnLyqC7hmnX366adkZGQwefJk3wWx2jBir+THkELO7TAAmyd/kRcRjyv9f6tIIDDzv48bN24kJSWFwsJCIiMjWbhwIcnJ1dtf9dlnn+XYsWPcfPPNVbbz1nV36Xuc/KcZmb0PZs8P5u9DoOe3APf3Opf7e51baRu73U5mZiapqX0INuHv4WbPD+bvw4n8bQmyOLHbz/z+QaD+tyQi4jNVLbXubVabq8Bd2XNxfVw/Z/J6gGZpgKuYv37RIuJjrypfxLbaoFl/109N3/9UQSGQ/FfXj4d5bUCCj7jzX+bb+7CmL4aXsljKzr42DKPcsVKTJ08mPT293PElS5YQERHhkTyZmZkeeR9/MnsflN///NkHi8VCfHw8Bw8eJCoq6ozeIy8vz8OpfM/sfTB7fqh+H/Ly8jh27BhffPFFuZvr+fn53ojmN2eyqotuqFfN7H0we34wfx8CPb9hGBiGQVFREaGhoZW2Kf3TjAOplN//atqHoqIi97+bZhvI1rZtWzZs2MDhw4dZsGABQ4cOZdmyZactiL/xxhuMHz+e999/n6ZNm1bZVtfd1WP2Ppg9P5i/D2bPD+bvg9nzg/n74In8de26W0RERE4wfTH87LPPxmazlZsFnpOTU262eKnRo0czcuRI9+Pc3FwSExNJS0sjOjq6VnlOjEhMNeWISjB/H5Tf/wKlD3v37iU3N5ewsDAiIiIqHSBzKsMwOHbsGA0aNKj2awKN2ftg9vxQ/T4YhkF+fj55eXnEx8dz4YUXlmtTWvitK85kVRfdUK8es/fB7PnB/H0I5PyNGjXC6XTSpEmTKv+/euDAAR+m8jzl97/q9MEwDPbt28fB/2/v/mOrqu8/jr9uS1tKUzqwlNsKIkEYk18ZrYMyRYeT0ImAMgXtapkOAxQGURN1jgFmycxc2EycDBJgGok1JEDI0JqyAf6CQfihCMhYrNRJC4L8KCBQ6Pv7B99evfRCi9zbcz6nz0fSpJxz7u3rfc699wX9tJevvtK+ffua7Pf7N9RTU1N10003SZIKCgq0ZcsWvfjii1q4cOFlb/PGG2/o0Ucf1fLly/XTn/602a/Bv7uvzPUZXM8vuT+D6/kl92dwPb/k/gzxzB+0f3cDAIBvOL8Ynpqaqvz8fFVWVuree++NbK+srNTYsWNj3iYtLS3mb5SkpKTE7S9+8bwvr7g+A/m95/UM119/vZKTk3X48OGrup2Z6euvv1Z6errTC7Euz+B6funqZ+jUqZPC4XDMY11/Lbicq3lXF76hfmWuz+B6fsn9GVzIX19fr+rq6ssuVJqZzpw5o/bt2zvZHeT33tXO0K5dOxUUFMR8zrj2DfXGd164nNdff12PPPKIXn/9dd19990tuk/+3d0yrs/gen7J/Rlczy+5P4Pr+SX3Z4hHfpfnBwAAV+b8YrgkPf744yopKVFBQYEKCwu1aNEiVVdXa8qUKV5HA+ChxrdKz8nJuaq3qqyvr9c777yj4cOHO/uPIddncD2/dHUzpKSkKDk5+YrHBMl3eVcXvqHeMq7P4Hp+yf0Z/Jw/JSVFffr00blz52Lud707yO+9q50hNTVVSUlJMff5+Rz85je/UVFRkbp37666ujqVl5dr/fr1qqiokHTxB9C++OILvfrqq5IuLoQ//PDDevHFFzV06NBIf6enpysrK8uzOQAAAAAAaIlALIZPmDBBR44c0XPPPaeamhr1799fb775pnr06OF1NAA+kJycfFULjcnJyTp//rzat2/v629kXonrM7ieXwrGDInyXd7VBQAkKSkpSe3bt4+5z/XXXfJ7LwgztMTBgwdVUlKimpoaZWVlaeDAgaqoqNBdd90lSaqpqVF1dXXk+IULF+r8+fMqKytTWVlZZHtpaan+/ve/t3Z8AAAAAACuSiAWwyVp2rRpmjZtmtcxAABAC/CuLgAAeGPx4sVX3H/pAvf69esTFwYAAAAAgAQLzGI4AABwB+/qAgAAAAAAAABINBbDAQCAJ3hXFwAAAAAAAABAIrEYLsnMJEknTpy45vuqr6/X6dOndeLECWf/nznXZyC/91yfwfX8kvszuJ5fiu8Mjf3U2Feguy/l+gyu55fcn8H1/JL7M5Dfe3R3YtHd0VyfwfX8kvszuJ5fcn8G1/NL7s9AdycW3R3N9Rlczy+5P4Pr+SX3Z3A9v+T+DPHO39L+ZjFcUl1dnSSpe/fuHicBAODy6urqlJWV5XUMX6C7AQAuoLu/QXcDAFxAd3+D7gYAuKK5/g4ZP+6mhoYGHThwQJmZmQqFQtd0XydOnFD37t31+eefq2PHjnFK2Lpcn4H83nN9BtfzS+7P4Hp+Kb4zmJnq6uqUl5enpKSkOCV0G90dzfUZXM8vuT+D6/kl92cgv/fo7sSiu6O5PoPr+SX3Z3A9v+T+DK7nl9yfge5OLLo7muszuJ5fcn8G1/NL7s/gen7J/Rninb+l/c1vhktKSkpSt27d4nqfHTt2dPKB+G2uz0B+77k+g+v5JfdncD2/FL8Z+Mn0aHR3bK7P4Hp+yf0ZXM8vuT8D+b1HdycG3R2b6zO4nl9yfwbX80vuz+B6fsn9GejuxKC7Y3N9BtfzS+7P4Hp+yf0ZXM8vuT9DPPO3pL/5MTcAAAAAAAAAAAAAQOCwGA4AAAAAAAAAAAAACBwWw+MsLS1Nc+bMUVpamtdRvjPXZyC/91yfwfX8kvszuJ5fCsYMbUUQrpXrM7ieX3J/BtfzS+7PQH7vBWGGtiII18r1GVzPL7k/g+v5JfdncD2/5P4MrudvS4JwrVyfwfX8kvszuJ5fcn8G1/NL7s/gVf6QmVmrfkUAAAAAAAAAAAAAABKM3wwHAAAAAAAAAAAAAAQOi+EAAAAAAAAAAAAAgMBhMRwAAAAAAAAAAAAAEDgshsfZyy+/rJ49e6p9+/bKz8/Xu+++63Uk/eEPf9Att9yizMxM5eTkaNy4cdq7d2/UMZMmTVIoFIr6GDp0aNQxZ8+e1YwZM5Sdna2MjAyNGTNG//vf/1plhrlz5zbJFw6HI/vNTHPnzlVeXp7S09N1xx13aNeuXb7Jf+ONNzbJHwqFVFZWJsmf5/+dd97RPffco7y8PIVCIa1atSpqf7zO+dGjR1VSUqKsrCxlZWWppKREx44dS2j++vp6PfXUUxowYIAyMjKUl5enhx9+WAcOHIi6jzvuuKPJdZk4cWKr5G9uBil+jxsvroGkmM+JUCikF154IXKMl9egJa+dfn8eoGXo7sSgu+nueOanu72/BhLd3RozoGXo7sRwvbsl9/qb7qa7E52f7k78DGgZP3a35H5/092tP4Pr3d3cDC70t+vd3ZIZ/NzfrnY3i+Fx9MYbb2jWrFl69tlntX37dt12220qKipSdXW1p7k2bNigsrIybdq0SZWVlTp//rxGjhypU6dORR03atQo1dTURD7efPPNqP2zZs3SypUrVV5ervfee08nT57U6NGjdeHChVaZo1+/flH5du7cGdn3xz/+UfPnz9dLL72kLVu2KBwO66677lJdXZ0v8m/ZsiUqe2VlpSTp/vvvjxzjt/N/6tQpDRo0SC+99FLM/fE65w899JB27NihiooKVVRUaMeOHSopKUlo/tOnT2vbtm2aPXu2tm3bphUrVug///mPxowZ0+TYyZMnR12XhQsXRu1PVP7mZmgUj8eNF9dAUlTumpoaLVmyRKFQSOPHj486zqtr0JLXTr8/D9A8ujux6G66O1756W7vr4FEd9Pd/kB3J5bL3S25199090V0d+Ly0910tx/4tbulYPQ33U13x3MGF/rb9e5uyQx+7m9nu9sQNz/60Y9sypQpUdv69u1rTz/9tEeJYjt06JBJsg0bNkS2lZaW2tixYy97m2PHjllKSoqVl5dHtn3xxReWlJRkFRUViYxrZmZz5syxQYMGxdzX0NBg4XDYnn/++ci2M2fOWFZWlv3tb38zM+/zX2rmzJnWq1cva2hoMDP/n39JtnLlysif43XOd+/ebZJs06ZNkWM2btxokuyTTz5JWP5YNm/ebJJs//79kW233367zZw587K3aa38ZrFniMfjxk/XYOzYsTZixIiobX66Bpe+drr2PEBsdHfi0N10dzzzx0J3e38N6G662wt0d+IErbvN3Opvujs2uvva8l+K7qa7veBKd5u51990N90d7xli8XN/u97dl5vhUn7ub1e6m98Mj5Nz585p69atGjlyZNT2kSNH6oMPPvAoVWzHjx+XJHXu3Dlq+/r165WTk6M+ffpo8uTJOnToUGTf1q1bVV9fHzVfXl6e+vfv32rz7du3T3l5eerZs6cmTpyoTz/9VJJUVVWl2traqGxpaWm6/fbbI9n8kL/RuXPn9Nprr+mRRx5RKBSKbPf7+f+2eJ3zjRs3KisrS0OGDIkcM3ToUGVlZbX6XMePH1coFNL3vve9qO3Lli1Tdna2+vXrpyeffDLqp5f8kP9aHzd+mEGSDh48qDVr1ujRRx9tss8v1+DS184gPg/aGro78ehuf+SXgvmaRXfT3c2hu4OH7k68oHS35H5/B/E1i+6mu5tDdwePS90tudnfdLd/Zgjqa5aL/R2U7pb839+udHe7qx8NsRw+fFgXLlxQ165do7Z37dpVtbW1HqVqysz0+OOP69Zbb1X//v0j24uKinT//ferR48eqqqq0uzZszVixAht3bpVaWlpqq2tVWpqqjp16hR1f60135AhQ/Tqq6+qT58+OnjwoH7/+99r2LBh2rVrV+Trxzr3+/fvlyTP83/bqlWrdOzYMU2aNCmyze/n/1LxOue1tbXKyclpcv85OTmtOteZM2f09NNP66GHHlLHjh0j24uLi9WzZ0+Fw2F9/PHHeuaZZ/Thhx9G3q7H6/zxeNx4PUOjV155RZmZmbrvvvuitvvlGsR67Qza86AtorsTi+72T34peK9ZdLf314Du9v4atEV0d2IFqbsl9/s7aK9ZdLf314Du9v4atEWudLfkZn/T3XR3ornY30Hqbsnf/e1Sd7MYHmff/okl6eKD4dJtXpo+fbo++ugjvffee1HbJ0yYEPm8f//+KigoUI8ePbRmzZomT7Jva635ioqKIp8PGDBAhYWF6tWrl1555RUNHTpU0nc7915cn8WLF6uoqEh5eXmRbX4//5cTj3Me6/jWnKu+vl4TJ05UQ0ODXn755ah9kydPjnzev39/9e7dWwUFBdq2bZsGDx4sydv88XrceH0NJGnJkiUqLi5W+/bto7b75Rpc7rUz1td38XnQ1tHdiUF3x+b14ysIr1l0t/fXQKK7/XAN2jK6OzGC1N1ScPo7CK9ZdLf310Ciu/1wDdoyv3e35GZ/0910dyK52t9B6m7J3/3tUnfzNulxkp2dreTk5CY/kXDo0KEmPwHhlRkzZmj16tVat26dunXrdsVjc3Nz1aNHD+3bt0+SFA6Hde7cOR09ejTqOK/my8jI0IABA7Rv3z6Fw2FJuuK590v+/fv3a+3atfrVr351xeP8fv7jdc7D4bAOHjzY5P6//PLLVpmrvr5eDzzwgKqqqlRZWRn1022xDB48WCkpKVHXxcv8l/oujxs/zPDuu+9q7969zT4vJG+uweVeO4PyPGjL6O7WRXdfRHdfG7rbHzPQ3d5fg7aK7m5drna3FIz+DsprFt3tjxnobu+vQVvlQndLwelvuvsbdPe1CVJ/u9rdkr/727XuZjE8TlJTU5Wfnx95C4JGlZWVGjZsmEepLjIzTZ8+XStWrNC//vUv9ezZs9nbHDlyRJ9//rlyc3MlSfn5+UpJSYmar6amRh9//LEn8509e1Z79uxRbm5u5K0gvp3t3Llz2rBhQySbX/IvXbpUOTk5uvvuu694nN/Pf7zOeWFhoY4fP67NmzdHjvn3v/+t48ePJ3yuxkLft2+f1q5dq+uuu67Z2+zatUv19fWR6+Jl/li+y+PGDzMsXrxY+fn5GjRoULPHtuY1aO61MwjPg7aO7m5ddDfdfa3obu+vQSO62/tr0FbR3a3L1e6WgtHfQXjNoru9vwaN6G7vr0Fb5efuloLX33T3RXT3tQlaf7va3ZI/+9vZ7jbETXl5uaWkpNjixYtt9+7dNmvWLMvIyLDPPvvM01xTp061rKwsW79+vdXU1EQ+Tp8+bWZmdXV19sQTT9gHH3xgVVVVtm7dOissLLTrr7/eTpw4EbmfKVOmWLdu3Wzt2rW2bds2GzFihA0aNMjOnz+f8BmeeOIJW79+vX366ae2adMmGz16tGVmZkbO7fPPP29ZWVm2YsUK27lzpz344IOWm5vrm/xmZhcuXLAbbrjBnnrqqajtfj3/dXV1tn37dtu+fbtJsvnz59v27dtt//79Zha/cz5q1CgbOHCgbdy40TZu3GgDBgyw0aNHJzR/fX29jRkzxrp162Y7duyIel6cPXvWzMz++9//2rx582zLli1WVVVla9assb59+9oPf/jDVsnf3AzxfNx4cQ0aHT9+3Dp06GALFixocnuvr0Fzr51m/n8eoHl0d+LQ3XR3PPPT3d5fg0Z0d2JnQPPo7sQJQnebudXfdDfdncj8jejuxM6A5vm1u83c72+6u/VncL27m5vBhf52vbubm6GRX/vb1e5mMTzO/vrXv1qPHj0sNTXVBg8ebBs2bPA6kkmK+bF06VIzMzt9+rSNHDnSunTpYikpKXbDDTdYaWmpVVdXR93P119/bdOnT7fOnTtbenq6jR49uskxiTJhwgTLzc21lJQUy8vLs/vuu8927doV2d/Q0GBz5syxcDhsaWlpNnz4cNu5c6dv8puZvf322ybJ9u7dG7Xdr+d/3bp1MR83paWlZha/c37kyBErLi62zMxMy8zMtOLiYjt69GhC81dVVV32ebFu3TozM6uurrbhw4db586dLTU11Xr16mW//vWv7ciRI62Sv7kZ4vm48eIaNFq4cKGlp6fbsWPHmtze62vQ3Gunmf+fB2gZujsx6G66O5756W7vr0EjujuxM6Bl6O7ECEJ3m7nV33Q33Z3I/I3o7sTOgJbxY3ebud/fdPdFdHf8ZnChv13v7uZmaOTX/na1u0P/Hx4AAAAAAAAAAAAAgMDg/wwHAAAAAAAAAAAAAAQOi+EAAAAAAAAAAAAAgMBhMRwAAAAAAAAAAAAAEDgshgMAAAAAAAAAAAAAAofFcAAAAAAAAAAAAABA4LAYDgAAAAAAAAAAAAAIHBbDAQAAAAAAAAAAAACBw2I4AAAAAAAAAAAAACBwWAwH4KlQKKRVq1Z5HQMAALQQ3Q0AgFvobgAA3EJ3A/HFYjjQhk2aNEmhUKjJx6hRo7yOBgAAYqC7AQBwC90NAIBb6G4geNp5HQCAt0aNGqWlS5dGbUtLS/MoDQAAaA7dDQCAW+huAADcQncDwcJvhgNtXFpamsLhcNRHp06dJF18O5YFCxaoqKhI6enp6tmzp5YvXx51+507d2rEiBFKT0/Xddddp8cee0wnT56MOmbJkiXq16+f0tLSlJubq+nTp0ftP3z4sO6991516NBBvXv31urVqyP7jh49quLiYnXp0kXp6enq3bt3k7+IAADQltDdAAC4he4GAMAtdDcQLCyGA7ii2bNna/z48frwww/1i1/8Qg8++KD27NkjSTp9+rRGjRqlTp06acuWLVq+fLnWrl0bVdwLFixQWVmZHnvsMe3cuVOrV6/WTTfdFPU15s2bpwceeEAfffSRfvazn6m4uFhfffVV5Ovv3r1bb731lvbs2aMFCxYoOzu79U4AAACOobsBAHAL3Q0AgFvobsAxBqDNKi0tteTkZMvIyIj6eO6558zMTJJNmTIl6jZDhgyxqVOnmpnZokWLrFOnTnby5MnI/jVr1lhSUpLV1taamVleXp49++yzl80gyX77299G/nzy5EkLhUL21ltvmZnZPffcY7/85S/jMzAAAI6juwEAcAvdDQCAW+huIHj4P8OBNu4nP/mJFixYELWtc+fOkc8LCwuj9hUWFmrHjh2SpD179mjQoEHKyMiI7P/xj3+shoYG7d27V6FQSAcOHNCdd955xQwDBw6MfJ6RkaHMzEwdOnRIkjR16lSNHz9e27Zt08iRIzVu3DgNGzbsO80KAEAQ0N0AALiF7gYAwC10NxAsLIYDbVxGRkaTt2BpTigUkiSZWeTzWMekp6e36P5SUlKa3LahoUGSVFRUpP3792vNmjVau3at7rzzTpWVlelPf/rTVWUGACAo6G4AANxCdwMA4Ba6GwgW/s9wAFe0adOmJn/u27evJOnmm2/Wjh07dOrUqcj+999/X0lJSerTp48yMzN144036p///Oc1ZejSpYsmTZqk1157TX/5y1+0aNGia7o/AACCjO4GAMAtdDcAAG6huwG38JvhQBt39uxZ1dbWRm1r166dsrOzJUnLly9XQUGBbr31Vi1btkybN2/W4sWLJUnFxcWaM2eOSktLNXfuXH355ZeaMWOGSkpK1LVrV0nS3LlzNWXKFOXk5KioqEh1dXV6//33NWPGjBbl+93vfqf8/Hz169dPZ8+e1T/+8Q/94Ac/iOMZAADALXQ3AABuobsBAHAL3Q0EC4vhQBtXUVGh3NzcqG3f//739cknn0iS5s2bp/Lyck2bNk3hcFjLli3TzTffLEnq0KGD3n77bc2cOVO33HKLOnTooPHjx2v+/PmR+yotLdWZM2f05z//WU8++aSys7P185//vMX5UlNT9cwzz+izzz5Tenq6brvtNpWXl8dhcgAA3ER3AwDgFrobAAC30N1AsITMzLwOAcCfQqGQVq5cqXHjxnkdBQAAtADdDQCAW+huAADcQncD7uH/DAcAAAAAAAAAAAAABA6L4QAAAAAAAAAAAACAwOFt0gEAAAAAAAAAAAAAgcNvhgMAAAAAAAAAAAAAAofFcAAAAAAAAAAAAABA4LAYDgAAAAAAAAAAAAAIHBbDAQAAAAAAAAAAAACBw2I4AAAAAAAAAAAAACBwWAwHAAAAAAAAAAAAAAQOi+EAAAAAAAAAAAAAgMBhMRwAAAAAAAAAAAAAEDgshgMAAAAAAAAAAAAAAuf/AC5cGuGQ4jScAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_performance(train_accuracies, test_accuracies, train_losses, test_losses, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a18d3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 260\n",
    "# Output size is 65, corresponding to the final prediction across 65 classes\n",
    "output_size = 65\n",
    "\n",
    "# Create an instance of the neural network\n",
    "# ensemble_net = SingleLayerNN(input_size, output_size)\n",
    "ensemble_net = MultiLayerNN(input_size, output_size)\n",
    "\n",
    "state_dict = torch.load('./records/smriNet_best_ensemble_model_78pc.pth')\n",
    "\n",
    "# num_input_features = best_mnet.classifier[1].in_features\n",
    "# best_mnet.classifier[1] = nn.Linear(num_input_features, 65)\n",
    "ensemble_net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee01b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82100a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
