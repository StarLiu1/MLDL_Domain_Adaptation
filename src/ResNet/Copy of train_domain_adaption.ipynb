{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ll5DkDbzgu3d","executionInfo":{"status":"ok","timestamp":1701906171781,"user_tz":300,"elapsed":18194,"user":{"displayName":"Star Liu","userId":"03747231952172118717"}},"outputId":"d259e9da-6433-4b02-faca-1dd4a0bfc9bd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"aJCjRceXXVrm","executionInfo":{"status":"ok","timestamp":1701906184865,"user_tz":300,"elapsed":8981,"user":{"displayName":"Star Liu","userId":"03747231952172118717"}}},"outputs":[],"source":["import math\n","import random\n","\n","import torch\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from torch import nn, optim\n","from torch.autograd import Variable\n","from torch.utils import model_zoo\n","from torch.utils.data import ConcatDataset, DataLoader, Subset\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import datasets, transforms\n","from torchvision.models.resnet import BasicBlock\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"lyhyGmo4XVrq","executionInfo":{"status":"ok","timestamp":1701906184865,"user_tz":300,"elapsed":3,"user":{"displayName":"Star Liu","userId":"03747231952172118717"}}},"outputs":[],"source":["\n","class AlexNet(nn.Module):\n","    def __init__(self, classes=100, dropout: float = 0.5) -> None:\n","        super().__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=dropout),\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=dropout),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, classes),\n","        )\n","\n","    def forward(self, x, gt=None, flag=None, epoch=None) -> torch.Tensor:\n","        x = self.features(x)\n","\n","        if flag:\n","            interval = 10\n","            if epoch % interval == 0:\n","                self.pecent = 3.0 / 10 + (epoch / interval) * 2.0 / 10\n","\n","            self.eval()\n","            x_new = x.clone().detach()\n","            x_new = Variable(x_new.data, requires_grad=True)\n","            x_new_view = self.avgpool(x_new)\n","            x_new_view = x_new_view.view(x_new_view.size(0), -1)\n","            output = self.classifier(x_new_view)\n","            class_num = output.shape[1]\n","            index = gt\n","            num_rois = x_new.shape[0]\n","            num_channel = x_new.shape[1]\n","            H = x_new.shape[2]\n","            HW = x_new.shape[2] * x_new.shape[3]\n","            one_hot = torch.zeros((1), dtype=torch.float32).cuda()\n","            one_hot = Variable(one_hot, requires_grad=False)\n","            sp_i = torch.ones([2, num_rois]).long()\n","            sp_i[0, :] = torch.arange(num_rois)\n","            sp_i[1, :] = index\n","            sp_v = torch.ones([num_rois])\n","            one_hot_sparse = torch.sparse.FloatTensor(sp_i, sp_v, torch.Size([num_rois, class_num])).to_dense().cuda()\n","            one_hot_sparse = Variable(one_hot_sparse, requires_grad=False)\n","            one_hot = torch.sum(output * one_hot_sparse)\n","            self.zero_grad()\n","            one_hot.backward()\n","            grads_val = x_new.grad.clone().detach()\n","            grad_channel_mean = torch.mean(grads_val.view(num_rois, num_channel, -1), dim=2)\n","            channel_mean = grad_channel_mean\n","            grad_channel_mean = grad_channel_mean.view(num_rois, num_channel, 1, 1)\n","            spatial_mean = torch.sum(x_new * grad_channel_mean, 1)\n","            spatial_mean = spatial_mean.view(num_rois, HW)\n","            self.zero_grad()\n","\n","            choose_one = random.randint(0, 9)\n","            if choose_one <= 4:\n","                # ---------------------------- spatial -----------------------\n","                mask_all = self.spatial_RCS(num_rois, H, HW, spatial_mean)\n","            else:\n","                # -------------------------- channel ----------------------------\n","                mask_all = self.channel_RCS(num_rois, num_channel, channel_mean)\n","\n","            # ----------------------------------- batch ----------------------------------------\n","            cls_prob_before = F.softmax(output, dim=1)\n","            x_new_view_after = x_new * mask_all\n","            x_new_view_after = self.avgpool(x_new_view_after)\n","            x_new_view_after = x_new_view_after.view(x_new_view_after.size(0), -1)\n","            x_new_view_after = self.classifier(x_new_view_after)\n","            cls_prob_after = F.softmax(x_new_view_after, dim=1)\n","\n","            sp_i = torch.ones([2, num_rois]).long()\n","            sp_i[0, :] = torch.arange(num_rois)\n","            sp_i[1, :] = index\n","            sp_v = torch.ones([num_rois])\n","            one_hot_sparse = torch.sparse.FloatTensor(sp_i, sp_v, torch.Size([num_rois, class_num])).to_dense().cuda()\n","            before_vector = torch.sum(one_hot_sparse * cls_prob_before, dim=1)\n","            after_vector = torch.sum(one_hot_sparse * cls_prob_after, dim=1)\n","            change_vector = before_vector - after_vector - 0.0001\n","            change_vector = torch.where(change_vector > 0, change_vector, torch.zeros(change_vector.shape).cuda())\n","            th_fg_value = torch.sort(change_vector, dim=0, descending=True)[0][int(round(float(num_rois) * self.pecent))]\n","            drop_index_fg = change_vector.gt(th_fg_value).long()\n","            ignore_index_fg = 1 - drop_index_fg\n","            not_01_ignore_index_fg = ignore_index_fg.nonzero()[:, 0]\n","            mask_all[not_01_ignore_index_fg.long(), :] = 1\n","\n","            self.train()\n","            mask_all = Variable(mask_all, requires_grad=True)\n","            x = x * mask_all\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def channel_RCS(self, num_rois, num_channel, channel_mean):\n","        vector_thresh_percent = math.ceil(num_channel * 1 / 3.2)\n","        vector_thresh_value = torch.sort(channel_mean, dim=1, descending=True)[0][:, vector_thresh_percent]\n","        vector_thresh_value = vector_thresh_value.view(num_rois, 1).expand(num_rois, num_channel)\n","        vector = torch.where(channel_mean > vector_thresh_value,\n","                                     torch.zeros(channel_mean.shape).cuda(),\n","                                     torch.ones(channel_mean.shape).cuda())\n","        mask_all = vector.view(num_rois, num_channel, 1, 1)\n","        return mask_all\n","\n","    def spatial_RCS(self, num_rois, H, HW, spatial_mean):\n","        spatial_drop_num = math.ceil(HW * 1 / 3.0)\n","        th18_mask_value = torch.sort(spatial_mean, dim=1, descending=True)[0][:, spatial_drop_num]\n","        th18_mask_value = th18_mask_value.view(num_rois, 1).expand(num_rois, 36)\n","        mask_all_cuda = torch.where(spatial_mean > th18_mask_value, torch.zeros(spatial_mean.shape).cuda(),\n","                                            torch.ones(spatial_mean.shape).cuda())\n","        mask_all = mask_all_cuda.reshape(num_rois, H, H).view(num_rois, 1, H, H)\n","        return mask_all\n","\n","def alex(pretrained=True, **kwargs):\n","    \"\"\"\n","    Constructs a custom alexnet model.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = AlexNet(**kwargs)\n","    if pretrained:\n","        # Load the original AlexNet\n","        alexnet_original = models.alexnet(pretrained=True)\n","\n","        # Copy weights for the common layers\n","        # Features\n","        for layer, param in model.features.named_parameters():\n","            param.data = alexnet_original.features.state_dict()[layer].data\n","\n","        # Classifier (except for the final layer)\n","        for layer, param in model.classifier.named_parameters():\n","            if '6' not in layer:  # Skip the final layer\n","                param.data = alexnet_original.classifier.state_dict()[layer].data\n","\n","    return model"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"t1wrQIZQXVrr","executionInfo":{"status":"ok","timestamp":1701906184866,"user_tz":300,"elapsed":3,"user":{"displayName":"Star Liu","userId":"03747231952172118717"}}},"outputs":[],"source":["class ResNet(nn.Module):\n","    def __init__(self, block, layers, jigsaw_classes=1000, classes=100):\n","        self.inplanes = 64\n","        super(ResNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","        self.avgpool = nn.AvgPool2d(7, stride=1)\n","        # self.jigsaw_classifier = nn.Linear(512 * block.expansion, jigsaw_classes)\n","        self.class_classifier = nn.Linear(512 * block.expansion, classes)\n","        #self.domain_classifier = nn.Linear(512 * block.expansion, domains)\n","        self.pecent = 1/3\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def is_patch_based(self):\n","        return False\n","\n","    def forward(self, x, gt=None, flag=None, epoch=None):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        if flag:\n","            interval = 10\n","            if epoch % interval == 0:\n","                self.pecent = 3.0 / 10 + (epoch / interval) * 2.0 / 10\n","\n","            self.eval()\n","            x_new = x.clone().detach() # 128，512，7，7\n","            x_new = Variable(x_new.data, requires_grad=True)\n","            x_new_view = self.avgpool(x_new) # 128 512 1 1\n","            x_new_view = x_new_view.view(x_new_view.size(0), -1)\n","            output = self.class_classifier(x_new_view)\n","            class_num = output.shape[1]\n","            index = gt\n","            num_rois = x_new.shape[0]\n","            num_channel = x_new.shape[1]\n","            H = x_new.shape[2]\n","            HW = x_new.shape[2] * x_new.shape[3]\n","            one_hot = torch.zeros((1), dtype=torch.float32).cuda() #\n","            one_hot = Variable(one_hot, requires_grad=False)\n","            sp_i = torch.ones([2, num_rois]).long()\n","            sp_i[0, :] = torch.arange(num_rois)\n","            sp_i[1, :] = index # location\n","            sp_v = torch.ones([num_rois]) # value\n","            one_hot_sparse = torch.sparse.FloatTensor(sp_i, sp_v, torch.Size([num_rois, class_num])).to_dense().cuda()\n","            one_hot_sparse = Variable(one_hot_sparse, requires_grad=False)\n","            one_hot = torch.sum(output * one_hot_sparse) # output-logits; class score: good bad\n","            self.zero_grad()\n","            one_hot.backward()\n","            grads_val = x_new.grad.clone().detach() # d(class score)/dx_new [128,512,7,7]\n","            grad_channel_mean = torch.mean(grads_val.view(num_rois, num_channel, -1), dim=2) # 128,512,49\n","            channel_mean = grad_channel_mean\n","            grad_channel_mean = grad_channel_mean.view(num_rois, num_channel, 1, 1)# grad_channel_mean [128 512 1 1] -->copy49 [128 512 7 7]\n","            spatial_mean = torch.sum(x_new * grad_channel_mean, 1) # x_new [128,512,7,7] # grad_channel_mean [128 512 1 1]\n","            spatial_mean = spatial_mean.view(num_rois, HW)\n","            self.zero_grad()\n","\n","            choose_one = random.randint(0, 9)\n","            if choose_one <= 4:\n","                # ---------------------------- spatial -----------------------\n","                mask_all = self.spatial_RCS(num_rois, H, HW, spatial_mean)\n","            else:\n","                # -------------------------- channel ----------------------------\n","                mask_all = self.channel_RCS(num_rois, num_channel, channel_mean)\n","\n","            # ----------------------------------- batch ----------------------------------------\n","            cls_prob_before = F.softmax(output, dim=1)\n","            x_new_view_after = x_new * mask_all\n","            x_new_view_after = self.avgpool(x_new_view_after)\n","            x_new_view_after = x_new_view_after.view(x_new_view_after.size(0), -1)\n","            x_new_view_after = self.class_classifier(x_new_view_after)\n","            cls_prob_after = F.softmax(x_new_view_after, dim=1)\n","\n","            sp_i = torch.ones([2, num_rois]).long()\n","            sp_i[0, :] = torch.arange(num_rois)\n","            sp_i[1, :] = index\n","            sp_v = torch.ones([num_rois])\n","            one_hot_sparse = torch.sparse.FloatTensor(sp_i, sp_v, torch.Size([num_rois, class_num])).to_dense().cuda()\n","            before_vector = torch.sum(one_hot_sparse * cls_prob_before, dim=1)\n","            after_vector = torch.sum(one_hot_sparse * cls_prob_after, dim=1)\n","            change_vector = before_vector - after_vector - 0.0001\n","            change_vector = torch.where(change_vector > 0, change_vector, torch.zeros(change_vector.shape).cuda())\n","            th_fg_value = torch.sort(change_vector, dim=0, descending=True)[0][int(round(float(num_rois) * self.pecent))]\n","            drop_index_fg = change_vector.gt(th_fg_value).long()\n","            ignore_index_fg = 1 - drop_index_fg\n","            not_01_ignore_index_fg = ignore_index_fg.nonzero()[:, 0]\n","            mask_all[not_01_ignore_index_fg.long(), :] = 1\n","\n","            self.train()\n","            mask_all = Variable(mask_all, requires_grad=True)\n","            x = x * mask_all\n","\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        return self.class_classifier(x)\n","\n","    def channel_RCS(self, num_rois, num_channel, channel_mean):\n","        vector_thresh_percent = math.ceil(num_channel * 1 / 3.2)\n","        vector_thresh_value = torch.sort(channel_mean, dim=1, descending=True)[0][:, vector_thresh_percent]\n","        vector_thresh_value = vector_thresh_value.view(num_rois, 1).expand(num_rois, num_channel)\n","        vector = torch.where(channel_mean > vector_thresh_value,\n","                                     torch.zeros(channel_mean.shape).cuda(),\n","                                     torch.ones(channel_mean.shape).cuda())\n","        mask_all = vector.view(num_rois, num_channel, 1, 1)\n","        return mask_all\n","\n","    def spatial_RCS(self, num_rois, H, HW, spatial_mean):\n","        spatial_drop_num = math.ceil(HW * 1 / 3.0)\n","        th18_mask_value = torch.sort(spatial_mean, dim=1, descending=True)[0][:, spatial_drop_num]\n","        th18_mask_value = th18_mask_value.view(num_rois, 1).expand(num_rois, 49)\n","        mask_all_cuda = torch.where(spatial_mean > th18_mask_value, torch.zeros(spatial_mean.shape).cuda(),\n","                                            torch.ones(spatial_mean.shape).cuda())\n","        mask_all = mask_all_cuda.reshape(num_rois, H, H).view(num_rois, 1, H, H)\n","        return mask_all\n","\n","def resnet18(pretrained=True, **kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet18-5c106cde.pth'), strict=False)\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"wqwl7tVsXVrs"},"source":["# With domain adaption Office home dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"X0_KC7ipXVrt","outputId":"8b1427ab-c404-4de7-c6a4-2e5ef780de5f","colab":{"base_uri":"https://localhost:8080/","height":672},"executionInfo":{"status":"error","timestamp":1701906482807,"user_tz":300,"elapsed":230526,"user":{"displayName":"Star Liu","userId":"03747231952172118717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Target domain: Real world\n","Step size: 16\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","<ipython-input-4-a55837eb3b2b>:81: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:605.)\n","  one_hot_sparse = torch.sparse.FloatTensor(sp_i, sp_v, torch.Size([num_rois, class_num])).to_dense().cuda()\n"]},{"output_type":"stream","name":"stdout","text":["Training - running batch 0/176 of epoch 0/20 - loss: 5.453074932098389 - acc:  0.000000%\n","Training - running batch 1/176 of epoch 0/20 - loss: 5.686984539031982 - acc:  0.000000%\n","Training - running batch 2/176 of epoch 0/20 - loss: 5.615288734436035 - acc:  0.000000%\n","Training - running batch 3/176 of epoch 0/20 - loss: 5.542474269866943 - acc:  0.000000%\n","Training - running batch 4/176 of epoch 0/20 - loss: 5.4631876945495605 - acc:  0.000000%\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-6f49f6e2699a>\u001b[0m in \u001b[0;36m<cell line: 82>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","\n","def get_model(model_name, pretrained=True, **kwargs):\n","    if model_name == 'resnet':\n","        model_class = resnet18\n","    elif model_name == 'alexnet':\n","        model_class = alex\n","    else:\n","        raise ValueError(f\"Unsupported model name: {model_name}\")\n","\n","    return model_class(pretrained=pretrained, **kwargs)\n","\n","#Accelerate the training\n","torch.backends.cudnn.benchmark = True\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","\n","\n","print(\"Target domain: {}\".format('Real world'))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize the model\n","n_classes = 65\n","model = get_model('resnet', pretrained=True, classes=n_classes)\n","model = model.to(device)\n","\n","# Data loaders\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),\n","    transforms.RandomGrayscale(0.1),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","val_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize(size=(224,224)),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","target_dataset = datasets.ImageFolder(\n","    f'/content/drive/MyDrive/MLDL/Final Project/OfficeHomeDataset_10072016/Real World',\n","    transform=val_transform)\n","target_loader = torch.utils.data.DataLoader(target_dataset , batch_size=64, shuffle=True)\n","\n","concatenated_datasets = []\n","total_size = 0\n","\n","for name in [\"Art\",\"Clipart\",\"Product\"]:\n","    dataset = datasets.ImageFolder(\n","        f'/content/drive/MyDrive/MLDL/Final Project/OfficeHomeDataset_10072016/{name}',\n","        transform=train_transform\n","    )\n","    total_size += len(dataset)\n","    concatenated_datasets.append(dataset)\n","\n","combined_dataset = ConcatDataset(concatenated_datasets)\n","indices = list(range(total_size))\n","random.shuffle(indices)\n","random_subset = Subset(combined_dataset, indices)\n","source_loader = DataLoader(random_subset, batch_size=64, shuffle=True)\n","\n","\n","\n","# Optimizer and scheduler\n","\n","params = model.parameters()\n","\n","optimizer = optim.SGD(params, weight_decay=.0005, momentum=.9, nesterov=False, lr=0.01)\n","#optimizer = optim.Adam(params, lr=lr)\n","step_size = int(20 * .8)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size)\n","\n","print(\"Step size: %d\" % step_size)\n","\n","# Training and testing loop\n","results = {\"test\": torch.zeros(20)}\n","criterion = nn.CrossEntropyLoss()\n","\n","writer = SummaryWriter('test')\n","global_step = 0\n","\n","for now_epoch in range(20):\n","    scheduler.step()\n","    lr = scheduler.get_lr()[0]\n","    writer.add_scalar('Learning Rate', lr, now_epoch)\n","\n","    # Training\n","    model.train()\n","\n","\n","    for it, batch in enumerate(source_loader):\n","        data, labels = batch\n","\n","        data, labels = data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","\n","        data_fliped = torch.flip(data, (3,)).detach().clone()\n","        data = torch.cat((data, data_fliped))\n","        labels_labels_flip = torch.cat((labels, labels))\n","\n","        class_logit = model(data, labels_labels_flip, True, now_epoch)\n","        class_loss = criterion(class_logit, labels_labels_flip)\n","        _, cls_pred = class_logit.max(dim=1)\n","        loss = class_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        class_loss_train = class_loss.item()\n","        accuracy = torch.sum(cls_pred == labels_labels_flip.data).item() / data.shape[0]\n","        writer.add_scalar('Loss/Train', class_loss_train, global_step)\n","        writer.add_scalar('Accuracy/Train', accuracy, global_step)\n","        global_step += 1\n","\n","        print(f\"Training - running batch {it}/{len(source_loader)} of epoch {now_epoch}/{20} - \"\n","                f\"loss: {class_loss_train} - acc: {accuracy * 100 : 2f}%\")\n","        del loss, class_loss, class_logit\n","\n","    # Testing\n","    model.eval()\n","    with torch.no_grad():\n","        class_correct = 0\n","        total = len(target_loader.dataset)\n","        for it, batch in enumerate(target_loader):\n","            data, labels = batch\n","\n","            data, labels = data.to(device), labels.to(device)\n","\n","            class_logit = model(data, labels, False)\n","            _, cls_pred = class_logit.max(dim=1)\n","\n","            class_correct += torch.sum(cls_pred == labels.data)\n","\n","        class_acc = float(class_correct) / total\n","        writer.add_scalar('Test/Accuracy', class_acc, now_epoch)\n","        print(f\"Testing - acc: {class_acc : 2f}%\")\n","        results['test'][now_epoch] = class_acc\n","\n","# Output results\n","test_res = results[\"test\"]\n","idx_best = test_res.argmax()\n","print(f\"Best test {test_res.max()}, corresponding test {test_res[idx_best]} - best test: {test_res.max()}, best epoch: {idx_best}\")\n","\n","# Save the model\n","model_path = './model_domain_adaption_last.pth'\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved at {model_path}\")"]},{"cell_type":"markdown","metadata":{"id":"HP2DYbiXXVru"},"source":["# With domain adaption PACS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdExPGdfXVru","executionInfo":{"status":"aborted","timestamp":1701906197801,"user_tz":300,"elapsed":3,"user":{"displayName":"Star Liu","userId":"03747231952172118717"}}},"outputs":[],"source":["\n","\n","def get_model(model_name, pretrained=True, **kwargs):\n","    if model_name == 'resnet':\n","        model_class = resnet18\n","    elif model_name == 'alexnet':\n","        model_class = alex\n","    else:\n","        raise ValueError(f\"Unsupported model name: {model_name}\")\n","\n","    return model_class(pretrained=pretrained, **kwargs)\n","\n","#Accelerate the training\n","torch.backends.cudnn.benchmark = True\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","\n","\n","print(\"Target domain: {}\".format('sketch'))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize the model\n","n_classes = 7\n","model = get_model('resnet', pretrained=True, classes=n_classes)\n","model = model.to(device)\n","\n","# Data loaders\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),\n","    transforms.RandomGrayscale(0.1),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","val_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize(size=(224,224)),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","target_dataset = datasets.ImageFolder(\n","    f'./Domain_Generalization/PACS/kfold/sketch',\n","    transform=val_transform)\n","target_loader = torch.utils.data.DataLoader(target_dataset , batch_size=64, shuffle=True)\n","\n","concatenated_datasets = []\n","total_size = 0\n","\n","for name in ['art_painting', 'cartoon', 'photo']:\n","    dataset = datasets.ImageFolder(\n","        f'./Domain_Generalization/PACS/kfold/{name}',\n","        transform=train_transform\n","    )\n","    total_size += len(dataset)\n","    concatenated_datasets.append(dataset)\n","\n","combined_dataset = ConcatDataset(concatenated_datasets)\n","indices = list(range(total_size))\n","random.shuffle(indices)\n","random_subset = Subset(combined_dataset, indices)\n","source_loader = DataLoader(random_subset, batch_size=64, shuffle=True)\n","\n","\n","\n","# Optimizer and scheduler\n","\n","params = model.parameters()\n","\n","optimizer = optim.SGD(params, weight_decay=.0005, momentum=.9, nesterov=False, lr=0.01)\n","#optimizer = optim.Adam(params, lr=lr)\n","step_size = int(20 * .8)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size)\n","\n","print(\"Step size: %d\" % step_size)\n","\n","# Training and testing loop\n","results = {\"test\": torch.zeros(20)}\n","criterion = nn.CrossEntropyLoss()\n","\n","writer = SummaryWriter('test_PACS')\n","global_step = 0\n","\n","for now_epoch in range(20):\n","    scheduler.step()\n","    lr = scheduler.get_lr()[0]\n","    writer.add_scalar('Learning Rate', lr, now_epoch)\n","\n","    # Training\n","    model.train()\n","\n","\n","    for it, batch in enumerate(source_loader):\n","        data, labels = batch\n","\n","        data, labels = data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","\n","        data_fliped = torch.flip(data, (3,)).detach().clone()\n","        data = torch.cat((data, data_fliped))\n","        labels_labels_flip = torch.cat((labels, labels))\n","\n","        class_logit = model(data, labels_labels_flip, True, now_epoch)\n","        class_loss = criterion(class_logit, labels_labels_flip)\n","        _, cls_pred = class_logit.max(dim=1)\n","        loss = class_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        class_loss_train = class_loss.item()\n","        accuracy = torch.sum(cls_pred == labels_labels_flip.data).item() / data.shape[0]\n","        writer.add_scalar('Loss/Train', class_loss_train, global_step)\n","        writer.add_scalar('Accuracy/Train', accuracy, global_step)\n","        global_step += 1\n","\n","        print(f\"Training - running batch {it}/{len(source_loader)} of epoch {now_epoch}/{20} - \"\n","                f\"loss: {class_loss_train} - acc: {accuracy * 100 : 2f}%\")\n","        del loss, class_loss, class_logit\n","\n","    # Testing\n","    model.eval()\n","    with torch.no_grad():\n","        class_correct = 0\n","        total = len(target_loader.dataset)\n","        for it, batch in enumerate(target_loader):\n","            data, labels = batch\n","\n","            data, labels = data.to(device), labels.to(device)\n","\n","            class_logit = model(data, labels, False)\n","            _, cls_pred = class_logit.max(dim=1)\n","\n","            class_correct += torch.sum(cls_pred == labels.data)\n","\n","        class_acc = float(class_correct) / total\n","        writer.add_scalar('Test/Accuracy', class_acc, now_epoch)\n","        print(f\"Testing - acc: {class_acc : 2f}%\")\n","        results['test'][now_epoch] = class_acc\n","\n","# Output results\n","test_res = results[\"test\"]\n","idx_best = test_res.argmax()\n","print(f\"Best test {test_res.max()}, corresponding test {test_res[idx_best]} - best test: {test_res.max()}, best epoch: {idx_best}\")\n","\n","# Save the model\n","model_path = './model_PACS_domain_adaption_last.pth'\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved at {model_path}\")"]},{"cell_type":"markdown","metadata":{"id":"1YbYZLbQXVrv"},"source":["# With domain adaption 16 class imagenet dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-i2XmPCoXVrv"},"outputs":[],"source":["\n","\n","def get_model(model_name, pretrained=True, **kwargs):\n","    if model_name == 'resnet':\n","        model_class = resnet18\n","    elif model_name == 'alexnet':\n","        model_class = alex\n","    else:\n","        raise ValueError(f\"Unsupported model name: {model_name}\")\n","\n","    return model_class(pretrained=pretrained, **kwargs)\n","\n","#Accelerate the training\n","torch.backends.cudnn.benchmark = True\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","\n","print(\"Target domain: {}\".format('edge'))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize the model\n","n_classes = 7\n","model = get_model('resnet', pretrained=True, classes=n_classes)\n","model = model.to(device)\n","\n","# Data loaders\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),\n","    transforms.RandomGrayscale(0.1),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","val_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize(size=(224,224)),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","target_dataset = datasets.ImageFolder(\n","    f'./Domain_Generalization16_class_imagenet_datasets/edge',\n","    transform=val_transform)\n","target_loader = torch.utils.data.DataLoader(target_dataset , batch_size=64, shuffle=True)\n","\n","concatenated_datasets = []\n","total_size = 0\n","\n","for name in ['silhouette', 'cue-conflict']:\n","    dataset = datasets.ImageFolder(\n","        f'./Domain_Generalization/16_class_imagenet_datasets/{name}',\n","        transform=train_transform\n","    )\n","    total_size += len(dataset)\n","    concatenated_datasets.append(dataset)\n","\n","combined_dataset = ConcatDataset(concatenated_datasets)\n","indices = list(range(total_size))\n","random.shuffle(indices)\n","random_subset = Subset(combined_dataset, indices)\n","source_loader = DataLoader(random_subset, batch_size=64, shuffle=True)\n","\n","\n","\n","# Optimizer and scheduler\n","\n","params = model.parameters()\n","\n","optimizer = optim.SGD(params, weight_decay=.0005, momentum=.9, nesterov=False, lr=0.01)\n","#optimizer = optim.Adam(params, lr=lr)\n","step_size = int(20 * .8)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size)\n","\n","print(\"Step size: %d\" % step_size)\n","\n","# Training and testing loop\n","results = {\"test\": torch.zeros(20)}\n","criterion = nn.CrossEntropyLoss()\n","\n","writer = SummaryWriter('test_PACS')\n","global_step = 0\n","\n","for now_epoch in range(20):\n","    scheduler.step()\n","    lr = scheduler.get_lr()[0]\n","    writer.add_scalar('Learning Rate', lr, now_epoch)\n","\n","    # Training\n","    model.train()\n","\n","\n","    for it, batch in enumerate(source_loader):\n","        data, labels = batch\n","\n","        data, labels = data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","\n","        data_fliped = torch.flip(data, (3,)).detach().clone()\n","        data = torch.cat((data, data_fliped))\n","        labels_labels_flip = torch.cat((labels, labels))\n","\n","        class_logit = model(data, labels_labels_flip, True, now_epoch)\n","        class_loss = criterion(class_logit, labels_labels_flip)\n","        _, cls_pred = class_logit.max(dim=1)\n","        loss = class_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        class_loss_train = class_loss.item()\n","        accuracy = torch.sum(cls_pred == labels_labels_flip.data).item() / data.shape[0]\n","        writer.add_scalar('Loss/Train', class_loss_train, global_step)\n","        writer.add_scalar('Accuracy/Train', accuracy, global_step)\n","        global_step += 1\n","\n","        print(f\"Training - running batch {it}/{len(source_loader)} of epoch {now_epoch}/{20} - \"\n","                f\"loss: {class_loss_train} - acc: {accuracy * 100 : 2f}%\")\n","        del loss, class_loss, class_logit\n","\n","    # Testing\n","    model.eval()\n","    with torch.no_grad():\n","        class_correct = 0\n","        total = len(target_loader.dataset)\n","        for it, batch in enumerate(target_loader):\n","            data, labels = batch\n","\n","            data, labels = data.to(device), labels.to(device)\n","\n","            class_logit = model(data, labels, False)\n","            _, cls_pred = class_logit.max(dim=1)\n","\n","            class_correct += torch.sum(cls_pred == labels.data)\n","\n","        class_acc = float(class_correct) / total\n","        writer.add_scalar('Test/Accuracy', class_acc, now_epoch)\n","        print(f\"Testing - acc: {class_acc : 2f}%\")\n","        results['test'][now_epoch] = class_acc\n","\n","# Output results\n","test_res = results[\"test\"]\n","idx_best = test_res.argmax()\n","print(f\"Best test {test_res.max()}, corresponding test {test_res[idx_best]} - best test: {test_res.max()}, best epoch: {idx_best}\")\n","\n","# Save the model\n","model_path = './model_PACS_domain_adaption_last.pth'\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved at {model_path}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[],"gpuType":"V100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}