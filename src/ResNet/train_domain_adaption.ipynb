{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aJCjRceXXVrm","outputId":"a468bd0a-0c0b-4f97-aa69-48067b8b36c1"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-04 11:36:48.624727: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-12-04 11:36:48.659306: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-12-04 11:36:48.824236: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-04 11:36:48.824275: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-04 11:36:48.856957: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-04 11:36:48.927048: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-12-04 11:36:48.928499: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-04 11:36:49.785759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["timport math\n","import random\n","\n","import torch\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from torch import nn, optim\n","from torch.autograd import Variable\n","from torch.utils import model_zoo\n","from torch.utils.data import ConcatDataset, DataLoader, Subset\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import datasets, transforms\n","from torchvision.models.resnet import BasicBlock\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lyhyGmo4XVrq"},"outputs":[],"source":["\n","class AlexNet(nn.Module):\n","    def __init__(self, classes=100, dropout: float = 0.5) -> None:\n","        super().__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=dropout),\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=dropout),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, classes),\n","        )\n","\n","    def forward(self, x, gt=None, flag=None, epoch=None) -> torch.Tensor:\n","        x = self.features(x)\n","\n","        if flag:\n","            interval = 10\n","            if epoch % interval == 0:\n","                self.pecent = 3.0 / 10 + (epoch / interval) * 2.0 / 10\n","\n","            self.eval()\n","            x_new = x.clone().detach()\n","            x_new = Variable(x_new.data, requires_grad=True)\n","            x_new_view = self.avgpool(x_new)\n","            x_new_view = x_new_view.view(x_new_view.size(0), -1)\n","            output = self.classifier(x_new_view)\n","            class_num = output.shape[1]\n","            index = gt\n","            num_rois = x_new.shape[0]\n","            num_channel = x_new.shape[1]\n","            H = x_new.shape[2]\n","            HW = x_new.shape[2] * x_new.shape[3]\n","            one_hot = torch.zeros((1), dtype=torch.float32).cuda()\n","            one_hot = Variable(one_hot, requires_grad=False)\n","            sp_i = torch.ones([2, num_rois]).long()\n","            sp_i[0, :] = torch.arange(num_rois)\n","            sp_i[1, :] = index\n","            sp_v = torch.ones([num_rois])\n","            one_hot_sparse = torch.sparse.FloatTensor(sp_i, sp_v, torch.Size([num_rois, class_num])).to_dense().cuda()\n","            one_hot_sparse = Variable(one_hot_sparse, requires_grad=False)\n","            one_hot = torch.sum(output * one_hot_sparse)\n","            self.zero_grad()\n","            one_hot.backward()\n","            grads_val = x_new.grad.clone().detach()\n","            grad_channel_mean = torch.mean(grads_val.view(num_rois, num_channel, -1), dim=2)\n","            channel_mean = grad_channel_mean\n","            grad_channel_mean = grad_channel_mean.view(num_rois, num_channel, 1, 1)\n","            spatial_mean = torch.sum(x_new * grad_channel_mean, 1)\n","            spatial_mean = spatial_mean.view(num_rois, HW)\n","            self.zero_grad()\n","\n","            choose_one = random.randint(0, 9)\n","            if choose_one <= 4:\n","                # ---------------------------- spatial -----------------------\n","                mask_all = self.spatial_RCS(num_rois, H, HW, spatial_mean)\n","            else:\n","                # -------------------------- channel ----------------------------\n","                mask_all = self.channel_RCS(num_rois, num_channel, channel_mean)\n","\n","            # ----------------------------------- batch ----------------------------------------\n","            cls_prob_before = F.softmax(output, dim=1)\n","            x_new_view_after = x_new * mask_all\n","            x_new_view_after = self.avgpool(x_new_view_after)\n","            x_new_view_after = x_new_view_after.view(x_new_view_after.size(0), -1)\n","            x_new_view_after = self.classifier(x_new_view_after)\n","            cls_prob_after = F.softmax(x_new_view_after, dim=1)\n","\n","            sp_i = torch.ones([2, num_rois]).long()\n","            sp_i[0, :] = torch.arange(num_rois)\n","            sp_i[1, :] = index\n","            sp_v = torch.ones([num_rois])\n","            one_hot_sparse = torch.sparse.FloatTensor(sp_i, sp_v, torch.Size([num_rois, class_num])).to_dense().cuda()\n","            before_vector = torch.sum(one_hot_sparse * cls_prob_before, dim=1)\n","            after_vector = torch.sum(one_hot_sparse * cls_prob_after, dim=1)\n","            change_vector = before_vector - after_vector - 0.0001\n","            change_vector = torch.where(change_vector > 0, change_vector, torch.zeros(change_vector.shape).cuda())\n","            th_fg_value = torch.sort(change_vector, dim=0, descending=True)[0][int(round(float(num_rois) * self.pecent))]\n","            drop_index_fg = change_vector.gt(th_fg_value).long()\n","            ignore_index_fg = 1 - drop_index_fg\n","            not_01_ignore_index_fg = ignore_index_fg.nonzero()[:, 0]\n","            mask_all[not_01_ignore_index_fg.long(), :] = 1\n","\n","            self.train()\n","            mask_all = Variable(mask_all, requires_grad=True)\n","            x = x * mask_all\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def channel_RCS(self, num_rois, num_channel, channel_mean):\n","        vector_thresh_percent = math.ceil(num_channel * 1 / 3.2)\n","        vector_thresh_value = torch.sort(channel_mean, dim=1, descending=True)[0][:, vector_thresh_percent]\n","        vector_thresh_value = vector_thresh_value.view(num_rois, 1).expand(num_rois, num_channel)\n","        vector = torch.where(channel_mean > vector_thresh_value,\n","                                     torch.zeros(channel_mean.shape).cuda(),\n","                                     torch.ones(channel_mean.shape).cuda())\n","        mask_all = vector.view(num_rois, num_channel, 1, 1)\n","        return mask_all\n","\n","    def spatial_RCS(self, num_rois, H, HW, spatial_mean):\n","        spatial_drop_num = math.ceil(HW * 1 / 3.0)\n","        th18_mask_value = torch.sort(spatial_mean, dim=1, descending=True)[0][:, spatial_drop_num]\n","        th18_mask_value = th18_mask_value.view(num_rois, 1).expand(num_rois, 36)\n","        mask_all_cuda = torch.where(spatial_mean > th18_mask_value, torch.zeros(spatial_mean.shape).cuda(),\n","                                            torch.ones(spatial_mean.shape).cuda())\n","        mask_all = mask_all_cuda.reshape(num_rois, H, H).view(num_rois, 1, H, H)\n","        return mask_all\n","\n","def alex(pretrained=True, **kwargs):\n","    \"\"\"\n","    Constructs a custom alexnet model.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = AlexNet(**kwargs)\n","    if pretrained:\n","        # Load the original AlexNet\n","        alexnet_original = models.alexnet(pretrained=True)\n","\n","        # Copy weights for the common layers\n","        # Features\n","        for layer, param in model.features.named_parameters():\n","            param.data = alexnet_original.features.state_dict()[layer].data\n","\n","        # Classifier (except for the final layer)\n","        for layer, param in model.classifier.named_parameters():\n","            if '6' not in layer:  # Skip the final layer\n","                param.data = alexnet_original.classifier.state_dict()[layer].data\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1wrQIZQXVrr"},"outputs":[],"source":["class ResNet(nn.Module):\n","    def __init__(self, block, layers, jigsaw_classes=1000, classes=100):\n","        self.inplanes = 64\n","        super(ResNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","        self.avgpool = nn.AvgPool2d(7, stride=1)\n","        # self.jigsaw_classifier = nn.Linear(512 * block.expansion, jigsaw_classes)\n","        self.class_classifier = nn.Linear(512 * block.expansion, classes)\n","        #self.domain_classifier = nn.Linear(512 * block.expansion, domains)\n","        self.pecent = 1/3\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def is_patch_based(self):\n","        return False\n","\n","    def forward(self, x, gt=None, flag=None, epoch=None):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        if flag:\n","            interval = 10\n","            if epoch % interval == 0:\n","                self.pecent = 3.0 / 10 + (epoch / interval) * 2.0 / 10\n","\n","            self.eval()\n","            x_new = x.clone().detach() # 128，512，7，7\n","            x_new = Variable(x_new.data, requires_grad=True)\n","            x_new_view = self.avgpool(x_new) # 128 512 1 1\n","            x_new_view = x_new_view.view(x_new_view.size(0), -1)\n","            output = self.class_classifier(x_new_view)\n","            class_num = output.shape[1]\n","            index = gt\n","            num_rois = x_new.shape[0]\n","            num_channel = x_new.shape[1]\n","            H = x_new.shape[2]\n","            HW = x_new.shape[2] * x_new.shape[3]\n","            one_hot = torch.zeros((1), dtype=torch.float32).cuda() #\n","            one_hot = Variable(one_hot, requires_grad=False)\n","            sp_i = torch.ones([2, num_rois]).long()\n","            sp_i[0, :] = torch.arange(num_rois)\n","            sp_i[1, :] = index # location\n","            sp_v = torch.ones([num_rois]) # value\n","            one_hot_sparse = torch.sparse.FloatTensor(sp_i, sp_v, torch.Size([num_rois, class_num])).to_dense().cuda()\n","            one_hot_sparse = Variable(one_hot_sparse, requires_grad=False)\n","            one_hot = torch.sum(output * one_hot_sparse) # output-logits; class score: good bad\n","            self.zero_grad()\n","            one_hot.backward()\n","            grads_val = x_new.grad.clone().detach() # d(class score)/dx_new [128,512,7,7]\n","            grad_channel_mean = torch.mean(grads_val.view(num_rois, num_channel, -1), dim=2) # 128,512,49\n","            channel_mean = grad_channel_mean\n","            grad_channel_mean = grad_channel_mean.view(num_rois, num_channel, 1, 1)# grad_channel_mean [128 512 1 1] -->copy49 [128 512 7 7]\n","            spatial_mean = torch.sum(x_new * grad_channel_mean, 1) # x_new [128,512,7,7] # grad_channel_mean [128 512 1 1]\n","            spatial_mean = spatial_mean.view(num_rois, HW)\n","            self.zero_grad()\n","\n","            choose_one = random.randint(0, 9)\n","            if choose_one <= 4:\n","                # ---------------------------- spatial -----------------------\n","                mask_all = self.spatial_RCS(num_rois, H, HW, spatial_mean)\n","            else:\n","                # -------------------------- channel ----------------------------\n","                mask_all = self.channel_RCS(num_rois, num_channel, channel_mean)\n","\n","            # ----------------------------------- batch ----------------------------------------\n","            cls_prob_before = F.softmax(output, dim=1)\n","            x_new_view_after = x_new * mask_all\n","            x_new_view_after = self.avgpool(x_new_view_after)\n","            x_new_view_after = x_new_view_after.view(x_new_view_after.size(0), -1)\n","            x_new_view_after = self.class_classifier(x_new_view_after)\n","            cls_prob_after = F.softmax(x_new_view_after, dim=1)\n","\n","            sp_i = torch.ones([2, num_rois]).long()\n","            sp_i[0, :] = torch.arange(num_rois)\n","            sp_i[1, :] = index\n","            sp_v = torch.ones([num_rois])\n","            one_hot_sparse = torch.sparse.FloatTensor(sp_i, sp_v, torch.Size([num_rois, class_num])).to_dense().cuda()\n","            before_vector = torch.sum(one_hot_sparse * cls_prob_before, dim=1)\n","            after_vector = torch.sum(one_hot_sparse * cls_prob_after, dim=1)\n","            change_vector = before_vector - after_vector - 0.0001\n","            change_vector = torch.where(change_vector > 0, change_vector, torch.zeros(change_vector.shape).cuda())\n","            th_fg_value = torch.sort(change_vector, dim=0, descending=True)[0][int(round(float(num_rois) * self.pecent))]\n","            drop_index_fg = change_vector.gt(th_fg_value).long()\n","            ignore_index_fg = 1 - drop_index_fg\n","            not_01_ignore_index_fg = ignore_index_fg.nonzero()[:, 0]\n","            mask_all[not_01_ignore_index_fg.long(), :] = 1\n","\n","            self.train()\n","            mask_all = Variable(mask_all, requires_grad=True)\n","            x = x * mask_all\n","\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        return self.class_classifier(x)\n","\n","    def channel_RCS(self, num_rois, num_channel, channel_mean):\n","        vector_thresh_percent = math.ceil(num_channel * 1 / 3.2)\n","        vector_thresh_value = torch.sort(channel_mean, dim=1, descending=True)[0][:, vector_thresh_percent]\n","        vector_thresh_value = vector_thresh_value.view(num_rois, 1).expand(num_rois, num_channel)\n","        vector = torch.where(channel_mean > vector_thresh_value,\n","                                     torch.zeros(channel_mean.shape).cuda(),\n","                                     torch.ones(channel_mean.shape).cuda())\n","        mask_all = vector.view(num_rois, num_channel, 1, 1)\n","        return mask_all\n","\n","    def spatial_RCS(self, num_rois, H, HW, spatial_mean):\n","        spatial_drop_num = math.ceil(HW * 1 / 3.0)\n","        th18_mask_value = torch.sort(spatial_mean, dim=1, descending=True)[0][:, spatial_drop_num]\n","        th18_mask_value = th18_mask_value.view(num_rois, 1).expand(num_rois, 49)\n","        mask_all_cuda = torch.where(spatial_mean > th18_mask_value, torch.zeros(spatial_mean.shape).cuda(),\n","                                            torch.ones(spatial_mean.shape).cuda())\n","        mask_all = mask_all_cuda.reshape(num_rois, H, H).view(num_rois, 1, H, H)\n","        return mask_all\n","\n","def resnet18(pretrained=True, **kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet18-5c106cde.pth'), strict=False)\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"wqwl7tVsXVrs"},"source":["# With domain generalization Office home dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X0_KC7ipXVrt","outputId":"fc1198fd-c359-4794-e5c2-4ba69c33168f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Target domain: sketch\n","Step size: 16\n","Training - running batch 0/176 of epoch 0/20 - loss: 4.458860397338867 - acc:  0.000000%\n","Training - running batch 1/176 of epoch 0/20 - loss: 5.627851486206055 - acc:  0.000000%\n","Training - running batch 2/176 of epoch 0/20 - loss: 4.492359638214111 - acc:  0.000000%\n","Training - running batch 3/176 of epoch 0/20 - loss: 5.380625247955322 - acc:  0.000000%\n","Training - running batch 4/176 of epoch 0/20 - loss: 4.356329441070557 - acc:  0.781250%\n","Training - running batch 5/176 of epoch 0/20 - loss: 4.245321273803711 - acc:  3.125000%\n","Training - running batch 6/176 of epoch 0/20 - loss: 5.350448131561279 - acc:  0.000000%\n","Training - running batch 7/176 of epoch 0/20 - loss: 4.2944560050964355 - acc:  0.781250%\n","Training - running batch 8/176 of epoch 0/20 - loss: 4.1612420082092285 - acc:  5.468750%\n","Training - running batch 9/176 of epoch 0/20 - loss: 5.2776594161987305 - acc:  0.000000%\n","Training - running batch 10/176 of epoch 0/20 - loss: 5.3641486167907715 - acc:  0.000000%\n","Training - running batch 11/176 of epoch 0/20 - loss: 4.028609752655029 - acc:  8.593750%\n","Training - running batch 12/176 of epoch 0/20 - loss: 3.9991188049316406 - acc:  4.687500%\n","Training - running batch 13/176 of epoch 0/20 - loss: 3.995638370513916 - acc:  5.468750%\n","Training - running batch 14/176 of epoch 0/20 - loss: 3.8750882148742676 - acc:  7.812500%\n","Training - running batch 15/176 of epoch 0/20 - loss: 3.9126319885253906 - acc:  12.500000%\n","Training - running batch 16/176 of epoch 0/20 - loss: 3.9502432346343994 - acc:  9.375000%\n","Training - running batch 17/176 of epoch 0/20 - loss: 5.000544548034668 - acc:  1.562500%\n","Training - running batch 18/176 of epoch 0/20 - loss: 5.118539333343506 - acc:  1.562500%\n","Training - running batch 19/176 of epoch 0/20 - loss: 3.4017796516418457 - acc:  17.187500%\n","Training - running batch 20/176 of epoch 0/20 - loss: 3.420309543609619 - acc:  26.562500%\n","Training - running batch 21/176 of epoch 0/20 - loss: 4.828824996948242 - acc:  1.562500%\n","Training - running batch 22/176 of epoch 0/20 - loss: 4.521125793457031 - acc:  1.562500%\n","Training - running batch 23/176 of epoch 0/20 - loss: 4.537518501281738 - acc:  5.468750%\n","Training - running batch 24/176 of epoch 0/20 - loss: 3.477851390838623 - acc:  12.500000%\n","Training - running batch 25/176 of epoch 0/20 - loss: 4.478841781616211 - acc:  5.468750%\n","Training - running batch 26/176 of epoch 0/20 - loss: 3.330476760864258 - acc:  21.875000%\n","Training - running batch 27/176 of epoch 0/20 - loss: 4.323278427124023 - acc:  15.625000%\n","Training - running batch 28/176 of epoch 0/20 - loss: 3.0939552783966064 - acc:  30.468750%\n","Training - running batch 29/176 of epoch 0/20 - loss: 3.1668076515197754 - acc:  21.093750%\n","Training - running batch 30/176 of epoch 0/20 - loss: 3.200136184692383 - acc:  20.312500%\n","Training - running batch 31/176 of epoch 0/20 - loss: 3.9060420989990234 - acc:  16.406250%\n","Training - running batch 32/176 of epoch 0/20 - loss: 3.9740047454833984 - acc:  16.406250%\n","Training - running batch 33/176 of epoch 0/20 - loss: 3.7760024070739746 - acc:  27.343750%\n","Training - running batch 34/176 of epoch 0/20 - loss: 2.7586333751678467 - acc:  42.187500%\n","Training - running batch 35/176 of epoch 0/20 - loss: 2.7047226428985596 - acc:  51.562500%\n","Training - running batch 36/176 of epoch 0/20 - loss: 3.9959394931793213 - acc:  15.625000%\n","Training - running batch 37/176 of epoch 0/20 - loss: 2.9073638916015625 - acc:  35.937500%\n","Training - running batch 38/176 of epoch 0/20 - loss: 3.680094003677368 - acc:  22.656250%\n","Training - running batch 39/176 of epoch 0/20 - loss: 3.8495724201202393 - acc:  17.187500%\n","Training - running batch 40/176 of epoch 0/20 - loss: 2.4452269077301025 - acc:  61.718750%\n","Training - running batch 41/176 of epoch 0/20 - loss: 2.6798126697540283 - acc:  46.875000%\n","Training - running batch 42/176 of epoch 0/20 - loss: 2.5225203037261963 - acc:  42.968750%\n","Training - running batch 43/176 of epoch 0/20 - loss: 2.539423704147339 - acc:  46.875000%\n","Training - running batch 44/176 of epoch 0/20 - loss: 2.614043951034546 - acc:  46.875000%\n","Training - running batch 45/176 of epoch 0/20 - loss: 2.479525566101074 - acc:  54.687500%\n","Training - running batch 46/176 of epoch 0/20 - loss: 3.9558117389678955 - acc:  14.843750%\n","Training - running batch 47/176 of epoch 0/20 - loss: 3.5782058238983154 - acc:  17.968750%\n","Training - running batch 48/176 of epoch 0/20 - loss: 3.3642780780792236 - acc:  25.000000%\n","Training - running batch 49/176 of epoch 0/20 - loss: 2.4429104328155518 - acc:  51.562500%\n","Training - running batch 50/176 of epoch 0/20 - loss: 3.520073890686035 - acc:  21.875000%\n","Training - running batch 51/176 of epoch 0/20 - loss: 2.5008997917175293 - acc:  40.625000%\n","Training - running batch 52/176 of epoch 0/20 - loss: 3.422039031982422 - acc:  21.093750%\n","Training - running batch 53/176 of epoch 0/20 - loss: 2.0117528438568115 - acc:  61.718750%\n","Training - running batch 54/176 of epoch 0/20 - loss: 3.4969534873962402 - acc:  17.187500%\n","Training - running batch 55/176 of epoch 0/20 - loss: 3.657726526260376 - acc:  13.281250%\n","Training - running batch 56/176 of epoch 0/20 - loss: 2.3994088172912598 - acc:  41.406250%\n","Training - running batch 57/176 of epoch 0/20 - loss: 2.2912046909332275 - acc:  43.750000%\n","Training - running batch 58/176 of epoch 0/20 - loss: 2.2639050483703613 - acc:  46.875000%\n","Training - running batch 59/176 of epoch 0/20 - loss: 3.4066662788391113 - acc:  21.875000%\n","Training - running batch 60/176 of epoch 0/20 - loss: 2.4219696521759033 - acc:  46.093750%\n","Training - running batch 61/176 of epoch 0/20 - loss: 3.5220155715942383 - acc:  19.531250%\n","Training - running batch 62/176 of epoch 0/20 - loss: 3.466168165206909 - acc:  20.312500%\n","Training - running batch 63/176 of epoch 0/20 - loss: 1.9846981763839722 - acc:  55.468750%\n","Training - running batch 64/176 of epoch 0/20 - loss: 2.960761308670044 - acc:  27.343750%\n","Training - running batch 65/176 of epoch 0/20 - loss: 2.143531322479248 - acc:  53.906250%\n","Training - running batch 66/176 of epoch 0/20 - loss: 3.6322245597839355 - acc:  23.437500%\n","Training - running batch 67/176 of epoch 0/20 - loss: 1.9107847213745117 - acc:  57.812500%\n","Training - running batch 68/176 of epoch 0/20 - loss: 2.2622365951538086 - acc:  52.343750%\n","Training - running batch 69/176 of epoch 0/20 - loss: 2.0649163722991943 - acc:  57.031250%\n","Training - running batch 70/176 of epoch 0/20 - loss: 2.095078468322754 - acc:  56.250000%\n","Training - running batch 71/176 of epoch 0/20 - loss: 2.3689770698547363 - acc:  41.406250%\n","Training - running batch 72/176 of epoch 0/20 - loss: 2.165841579437256 - acc:  44.531250%\n","Training - running batch 73/176 of epoch 0/20 - loss: 2.1032142639160156 - acc:  44.531250%\n","Training - running batch 74/176 of epoch 0/20 - loss: 2.936878204345703 - acc:  32.812500%\n","Training - running batch 75/176 of epoch 0/20 - loss: 2.2244045734405518 - acc:  45.312500%\n","Training - running batch 76/176 of epoch 0/20 - loss: 3.077561140060425 - acc:  27.343750%\n","Training - running batch 77/176 of epoch 0/20 - loss: 2.3673291206359863 - acc:  48.437500%\n","Training - running batch 78/176 of epoch 0/20 - loss: 1.9983201026916504 - acc:  53.125000%\n","Training - running batch 79/176 of epoch 0/20 - loss: 2.7946388721466064 - acc:  41.406250%\n","Training - running batch 80/176 of epoch 0/20 - loss: 3.308788776397705 - acc:  19.531250%\n","Training - running batch 81/176 of epoch 0/20 - loss: 2.0308597087860107 - acc:  58.593750%\n","Training - running batch 82/176 of epoch 0/20 - loss: 3.0453217029571533 - acc:  34.375000%\n","Training - running batch 83/176 of epoch 0/20 - loss: 3.3259806632995605 - acc:  22.656250%\n","Training - running batch 84/176 of epoch 0/20 - loss: 2.236574649810791 - acc:  49.218750%\n","Training - running batch 85/176 of epoch 0/20 - loss: 1.7610859870910645 - acc:  64.062500%\n","Training - running batch 86/176 of epoch 0/20 - loss: 3.1264166831970215 - acc:  27.343750%\n","Training - running batch 87/176 of epoch 0/20 - loss: 1.908471941947937 - acc:  60.937500%\n","Training - running batch 88/176 of epoch 0/20 - loss: 1.603169322013855 - acc:  64.843750%\n","Training - running batch 89/176 of epoch 0/20 - loss: 3.149740219116211 - acc:  25.781250%\n","Training - running batch 90/176 of epoch 0/20 - loss: 1.6474984884262085 - acc:  69.531250%\n","Training - running batch 91/176 of epoch 0/20 - loss: 2.6793274879455566 - acc:  34.375000%\n","Training - running batch 92/176 of epoch 0/20 - loss: 3.1784653663635254 - acc:  26.562500%\n","Training - running batch 93/176 of epoch 0/20 - loss: 3.2216923236846924 - acc:  29.687500%\n","Training - running batch 94/176 of epoch 0/20 - loss: 1.6770230531692505 - acc:  60.156250%\n","Training - running batch 95/176 of epoch 0/20 - loss: 1.8226321935653687 - acc:  61.718750%\n","Training - running batch 96/176 of epoch 0/20 - loss: 2.554372549057007 - acc:  42.187500%\n","Training - running batch 97/176 of epoch 0/20 - loss: 3.114104747772217 - acc:  30.468750%\n","Training - running batch 98/176 of epoch 0/20 - loss: 1.9524643421173096 - acc:  53.125000%\n","Training - running batch 99/176 of epoch 0/20 - loss: 2.2897789478302 - acc:  48.437500%\n","Training - running batch 100/176 of epoch 0/20 - loss: 3.430859088897705 - acc:  20.312500%\n","Training - running batch 101/176 of epoch 0/20 - loss: 3.1870217323303223 - acc:  31.250000%\n","Training - running batch 102/176 of epoch 0/20 - loss: 2.929116725921631 - acc:  34.375000%\n","Training - running batch 103/176 of epoch 0/20 - loss: 2.972919464111328 - acc:  34.375000%\n","Training - running batch 104/176 of epoch 0/20 - loss: 2.0232789516448975 - acc:  56.250000%\n","Training - running batch 105/176 of epoch 0/20 - loss: 3.332611322402954 - acc:  20.312500%\n","Training - running batch 106/176 of epoch 0/20 - loss: 1.9235929250717163 - acc:  50.781250%\n","Training - running batch 107/176 of epoch 0/20 - loss: 1.9841769933700562 - acc:  56.250000%\n","Training - running batch 108/176 of epoch 0/20 - loss: 1.8895505666732788 - acc:  59.375000%\n","Training - running batch 109/176 of epoch 0/20 - loss: 2.0641298294067383 - acc:  52.343750%\n","Training - running batch 110/176 of epoch 0/20 - loss: 3.377821683883667 - acc:  24.218750%\n","Training - running batch 111/176 of epoch 0/20 - loss: 3.3880650997161865 - acc:  25.781250%\n","Training - running batch 112/176 of epoch 0/20 - loss: 1.9493858814239502 - acc:  53.125000%\n","Training - running batch 113/176 of epoch 0/20 - loss: 1.8258320093154907 - acc:  57.812500%\n","Training - running batch 114/176 of epoch 0/20 - loss: 3.0169944763183594 - acc:  33.593750%\n","Training - running batch 115/176 of epoch 0/20 - loss: 3.1029915809631348 - acc:  27.343750%\n","Training - running batch 116/176 of epoch 0/20 - loss: 2.079678535461426 - acc:  47.656250%\n","Training - running batch 117/176 of epoch 0/20 - loss: 1.8306609392166138 - acc:  48.437500%\n","Training - running batch 118/176 of epoch 0/20 - loss: 2.7834582328796387 - acc:  31.250000%\n","Training - running batch 119/176 of epoch 0/20 - loss: 1.4035437107086182 - acc:  70.312500%\n","Training - running batch 120/176 of epoch 0/20 - loss: 2.8545174598693848 - acc:  28.906250%\n","Training - running batch 121/176 of epoch 0/20 - loss: 1.7163244485855103 - acc:  58.593750%\n","Training - running batch 122/176 of epoch 0/20 - loss: 2.803682327270508 - acc:  34.375000%\n","Training - running batch 123/176 of epoch 0/20 - loss: 1.6177020072937012 - acc:  62.500000%\n","Training - running batch 124/176 of epoch 0/20 - loss: 1.7728766202926636 - acc:  58.593750%\n","Training - running batch 125/176 of epoch 0/20 - loss: 1.7242158651351929 - acc:  56.250000%\n","Training - running batch 126/176 of epoch 0/20 - loss: 1.80312180519104 - acc:  60.156250%\n","Training - running batch 127/176 of epoch 0/20 - loss: 2.6873505115509033 - acc:  32.812500%\n","Training - running batch 128/176 of epoch 0/20 - loss: 1.816638708114624 - acc:  53.906250%\n","Training - running batch 129/176 of epoch 0/20 - loss: 1.3496941328048706 - acc:  67.968750%\n","Training - running batch 130/176 of epoch 0/20 - loss: 1.7193398475646973 - acc:  64.062500%\n","Training - running batch 131/176 of epoch 0/20 - loss: 2.939039945602417 - acc:  32.812500%\n","Training - running batch 132/176 of epoch 0/20 - loss: 2.2026028633117676 - acc:  47.656250%\n","Training - running batch 133/176 of epoch 0/20 - loss: 1.8013207912445068 - acc:  60.156250%\n","Training - running batch 134/176 of epoch 0/20 - loss: 3.2400612831115723 - acc:  26.562500%\n","Training - running batch 135/176 of epoch 0/20 - loss: 2.927293300628662 - acc:  34.375000%\n","Training - running batch 136/176 of epoch 0/20 - loss: 1.9282341003417969 - acc:  47.656250%\n","Training - running batch 137/176 of epoch 0/20 - loss: 3.0478084087371826 - acc:  27.343750%\n","Training - running batch 138/176 of epoch 0/20 - loss: 1.3770298957824707 - acc:  76.562500%\n","Training - running batch 139/176 of epoch 0/20 - loss: 1.648751974105835 - acc:  67.187500%\n","Training - running batch 140/176 of epoch 0/20 - loss: 2.6953771114349365 - acc:  38.281250%\n","Training - running batch 141/176 of epoch 0/20 - loss: 1.8151609897613525 - acc:  62.500000%\n","Training - running batch 142/176 of epoch 0/20 - loss: 2.7531425952911377 - acc:  39.843750%\n","Training - running batch 143/176 of epoch 0/20 - loss: 1.6625641584396362 - acc:  64.843750%\n","Training - running batch 144/176 of epoch 0/20 - loss: 2.8214776515960693 - acc:  32.812500%\n","Training - running batch 145/176 of epoch 0/20 - loss: 2.61824107170105 - acc:  40.625000%\n","Training - running batch 146/176 of epoch 0/20 - loss: 2.6666574478149414 - acc:  35.156250%\n","Training - running batch 147/176 of epoch 0/20 - loss: 2.4846577644348145 - acc:  41.406250%\n","Training - running batch 148/176 of epoch 0/20 - loss: 2.623560905456543 - acc:  40.625000%\n","Training - running batch 149/176 of epoch 0/20 - loss: 1.8826884031295776 - acc:  60.156250%\n","Training - running batch 150/176 of epoch 0/20 - loss: 1.7003055810928345 - acc:  60.937500%\n","Training - running batch 151/176 of epoch 0/20 - loss: 2.429267644882202 - acc:  37.500000%\n","Training - running batch 152/176 of epoch 0/20 - loss: 1.3955575227737427 - acc:  69.531250%\n","Training - running batch 153/176 of epoch 0/20 - loss: 1.8104077577590942 - acc:  53.125000%\n","Training - running batch 154/176 of epoch 0/20 - loss: 1.5251275300979614 - acc:  66.406250%\n","Training - running batch 155/176 of epoch 0/20 - loss: 2.4203948974609375 - acc:  45.312500%\n","Training - running batch 156/176 of epoch 0/20 - loss: 1.5292725563049316 - acc:  64.062500%\n","Training - running batch 157/176 of epoch 0/20 - loss: 1.7919838428497314 - acc:  56.250000%\n","Training - running batch 158/176 of epoch 0/20 - loss: 1.621102213859558 - acc:  60.156250%\n","Training - running batch 159/176 of epoch 0/20 - loss: 2.537693977355957 - acc:  34.375000%\n","Training - running batch 160/176 of epoch 0/20 - loss: 1.9309117794036865 - acc:  54.687500%\n","Training - running batch 161/176 of epoch 0/20 - loss: 2.6714413166046143 - acc:  32.812500%\n","Training - running batch 162/176 of epoch 0/20 - loss: 1.9206043481826782 - acc:  50.781250%\n","Training - running batch 163/176 of epoch 0/20 - loss: 2.572895050048828 - acc:  46.093750%\n","Training - running batch 164/176 of epoch 0/20 - loss: 2.6282174587249756 - acc:  40.625000%\n","Training - running batch 165/176 of epoch 0/20 - loss: 2.698551893234253 - acc:  36.718750%\n","Training - running batch 166/176 of epoch 0/20 - loss: 2.5190742015838623 - acc:  42.968750%\n","Training - running batch 167/176 of epoch 0/20 - loss: 1.8959553241729736 - acc:  59.375000%\n","Training - running batch 168/176 of epoch 0/20 - loss: 2.254801034927368 - acc:  50.000000%\n","Training - running batch 169/176 of epoch 0/20 - loss: 2.9313089847564697 - acc:  32.031250%\n","Training - running batch 170/176 of epoch 0/20 - loss: 1.8749868869781494 - acc:  53.125000%\n","Training - running batch 171/176 of epoch 0/20 - loss: 1.6412842273712158 - acc:  60.156250%\n","Training - running batch 172/176 of epoch 0/20 - loss: 1.6702518463134766 - acc:  62.500000%\n","Training - running batch 173/176 of epoch 0/20 - loss: 1.6647019386291504 - acc:  60.937500%\n","Training - running batch 174/176 of epoch 0/20 - loss: 1.3875917196273804 - acc:  71.093750%\n","Training - running batch 175/176 of epoch 0/20 - loss: 1.3947964906692505 - acc:  72.580645%\n","Testing - acc:  0.651825%\n","Training - running batch 0/176 of epoch 1/20 - loss: 1.4414074420928955 - acc:  72.656250%\n","Training - running batch 1/176 of epoch 1/20 - loss: 1.1982576847076416 - acc:  75.000000%\n","Training - running batch 2/176 of epoch 1/20 - loss: 2.4164905548095703 - acc:  43.750000%\n","Training - running batch 3/176 of epoch 1/20 - loss: 2.3148691654205322 - acc:  46.875000%\n","Training - running batch 4/176 of epoch 1/20 - loss: 2.63440203666687 - acc:  46.093750%\n","Training - running batch 5/176 of epoch 1/20 - loss: 1.1466807126998901 - acc:  75.000000%\n","Training - running batch 6/176 of epoch 1/20 - loss: 2.52654767036438 - acc:  37.500000%\n","Training - running batch 7/176 of epoch 1/20 - loss: 2.1121349334716797 - acc:  53.125000%\n","Training - running batch 8/176 of epoch 1/20 - loss: 1.5036441087722778 - acc:  70.312500%\n","Training - running batch 9/176 of epoch 1/20 - loss: 1.2998974323272705 - acc:  71.875000%\n","Training - running batch 10/176 of epoch 1/20 - loss: 2.1519205570220947 - acc:  47.656250%\n","Training - running batch 11/176 of epoch 1/20 - loss: 2.31852126121521 - acc:  38.281250%\n","Training - running batch 12/176 of epoch 1/20 - loss: 2.7082479000091553 - acc:  43.750000%\n","Training - running batch 13/176 of epoch 1/20 - loss: 2.3482327461242676 - acc:  45.312500%\n","Training - running batch 14/176 of epoch 1/20 - loss: 2.733886241912842 - acc:  32.031250%\n","Training - running batch 15/176 of epoch 1/20 - loss: 1.4095759391784668 - acc:  68.750000%\n","Training - running batch 16/176 of epoch 1/20 - loss: 2.5451619625091553 - acc:  37.500000%\n","Training - running batch 17/176 of epoch 1/20 - loss: 1.1197432279586792 - acc:  81.250000%\n","Training - running batch 18/176 of epoch 1/20 - loss: 1.174772024154663 - acc:  73.437500%\n","Training - running batch 19/176 of epoch 1/20 - loss: 1.48018479347229 - acc:  73.437500%\n","Training - running batch 20/176 of epoch 1/20 - loss: 2.2268877029418945 - acc:  51.562500%\n","Training - running batch 21/176 of epoch 1/20 - loss: 1.525229811668396 - acc:  63.281250%\n","Training - running batch 22/176 of epoch 1/20 - loss: 2.290708541870117 - acc:  44.531250%\n","Training - running batch 23/176 of epoch 1/20 - loss: 2.098324775695801 - acc:  48.437500%\n","Training - running batch 24/176 of epoch 1/20 - loss: 2.3389954566955566 - acc:  46.093750%\n","Training - running batch 25/176 of epoch 1/20 - loss: 1.6887542009353638 - acc:  67.187500%\n","Training - running batch 26/176 of epoch 1/20 - loss: 1.2909533977508545 - acc:  70.312500%\n","Training - running batch 27/176 of epoch 1/20 - loss: 1.3157740831375122 - acc:  72.656250%\n","Training - running batch 28/176 of epoch 1/20 - loss: 2.4347729682922363 - acc:  42.187500%\n","Training - running batch 29/176 of epoch 1/20 - loss: 1.1310549974441528 - acc:  71.093750%\n","Training - running batch 30/176 of epoch 1/20 - loss: 2.249661922454834 - acc:  45.312500%\n","Training - running batch 31/176 of epoch 1/20 - loss: 1.5826077461242676 - acc:  69.531250%\n","Training - running batch 32/176 of epoch 1/20 - loss: 2.330836534500122 - acc:  37.500000%\n","Training - running batch 33/176 of epoch 1/20 - loss: 1.291940689086914 - acc:  73.437500%\n","Training - running batch 34/176 of epoch 1/20 - loss: 1.6708329916000366 - acc:  67.187500%\n","Training - running batch 35/176 of epoch 1/20 - loss: 2.2152724266052246 - acc:  50.000000%\n","Training - running batch 36/176 of epoch 1/20 - loss: 2.4333057403564453 - acc:  45.312500%\n","Training - running batch 37/176 of epoch 1/20 - loss: 2.1533470153808594 - acc:  44.531250%\n","Training - running batch 38/176 of epoch 1/20 - loss: 1.455322504043579 - acc:  67.968750%\n","Training - running batch 39/176 of epoch 1/20 - loss: 2.2576799392700195 - acc:  45.312500%\n","Training - running batch 40/176 of epoch 1/20 - loss: 1.2443450689315796 - acc:  73.437500%\n","Training - running batch 41/176 of epoch 1/20 - loss: 1.3276406526565552 - acc:  75.000000%\n","Training - running batch 42/176 of epoch 1/20 - loss: 1.2896162271499634 - acc:  66.406250%\n","Training - running batch 43/176 of epoch 1/20 - loss: 1.3994470834732056 - acc:  70.312500%\n","Training - running batch 44/176 of epoch 1/20 - loss: 1.4954166412353516 - acc:  73.437500%\n","Training - running batch 45/176 of epoch 1/20 - loss: 1.1537706851959229 - acc:  73.437500%\n","Training - running batch 46/176 of epoch 1/20 - loss: 2.0432491302490234 - acc:  48.437500%\n","Training - running batch 47/176 of epoch 1/20 - loss: 1.3052839040756226 - acc:  71.875000%\n","Training - running batch 48/176 of epoch 1/20 - loss: 2.315831422805786 - acc:  39.843750%\n","Training - running batch 49/176 of epoch 1/20 - loss: 2.2919554710388184 - acc:  44.531250%\n","Training - running batch 50/176 of epoch 1/20 - loss: 1.3431366682052612 - acc:  73.437500%\n","Training - running batch 51/176 of epoch 1/20 - loss: 2.2961103916168213 - acc:  39.843750%\n","Training - running batch 52/176 of epoch 1/20 - loss: 1.3648749589920044 - acc:  60.156250%\n","Training - running batch 53/176 of epoch 1/20 - loss: 2.110413074493408 - acc:  45.312500%\n","Training - running batch 54/176 of epoch 1/20 - loss: 2.4185245037078857 - acc:  41.406250%\n","Training - running batch 55/176 of epoch 1/20 - loss: 2.4549691677093506 - acc:  38.281250%\n","Training - running batch 56/176 of epoch 1/20 - loss: 1.5852652788162231 - acc:  55.468750%\n","Training - running batch 57/176 of epoch 1/20 - loss: 2.115725517272949 - acc:  46.093750%\n","Training - running batch 58/176 of epoch 1/20 - loss: 2.308847188949585 - acc:  44.531250%\n","Training - running batch 59/176 of epoch 1/20 - loss: 1.3945932388305664 - acc:  68.750000%\n","Training - running batch 60/176 of epoch 1/20 - loss: 1.4149011373519897 - acc:  67.968750%\n","Training - running batch 61/176 of epoch 1/20 - loss: 2.4059574604034424 - acc:  37.500000%\n","Training - running batch 62/176 of epoch 1/20 - loss: 1.36397123336792 - acc:  74.218750%\n","Training - running batch 63/176 of epoch 1/20 - loss: 2.196861505508423 - acc:  48.437500%\n","Training - running batch 64/176 of epoch 1/20 - loss: 1.3709131479263306 - acc:  67.968750%\n","Training - running batch 65/176 of epoch 1/20 - loss: 1.501217246055603 - acc:  64.062500%\n","Training - running batch 66/176 of epoch 1/20 - loss: 1.6013199090957642 - acc:  62.500000%\n","Training - running batch 67/176 of epoch 1/20 - loss: 2.109609603881836 - acc:  53.125000%\n","Training - running batch 68/176 of epoch 1/20 - loss: 1.3121278285980225 - acc:  64.062500%\n","Training - running batch 69/176 of epoch 1/20 - loss: 2.189953327178955 - acc:  40.625000%\n","Training - running batch 70/176 of epoch 1/20 - loss: 2.483794927597046 - acc:  39.843750%\n","Training - running batch 71/176 of epoch 1/20 - loss: 2.06809663772583 - acc:  43.750000%\n","Training - running batch 72/176 of epoch 1/20 - loss: 1.9304219484329224 - acc:  42.187500%\n","Training - running batch 73/176 of epoch 1/20 - loss: 2.2703232765197754 - acc:  43.750000%\n","Training - running batch 74/176 of epoch 1/20 - loss: 2.1209452152252197 - acc:  51.562500%\n","Training - running batch 75/176 of epoch 1/20 - loss: 1.7900067567825317 - acc:  53.125000%\n","Training - running batch 76/176 of epoch 1/20 - loss: 2.1788008213043213 - acc:  50.781250%\n","Training - running batch 77/176 of epoch 1/20 - loss: 2.159722089767456 - acc:  46.093750%\n","Training - running batch 78/176 of epoch 1/20 - loss: 1.369269847869873 - acc:  64.062500%\n","Training - running batch 79/176 of epoch 1/20 - loss: 2.030107021331787 - acc:  53.906250%\n","Training - running batch 80/176 of epoch 1/20 - loss: 2.0694921016693115 - acc:  46.875000%\n","Training - running batch 81/176 of epoch 1/20 - loss: 1.9090383052825928 - acc:  53.125000%\n","Training - running batch 82/176 of epoch 1/20 - loss: 1.8855154514312744 - acc:  57.812500%\n","Training - running batch 83/176 of epoch 1/20 - loss: 1.406163215637207 - acc:  62.500000%\n","Training - running batch 84/176 of epoch 1/20 - loss: 1.4472814798355103 - acc:  64.843750%\n","Training - running batch 85/176 of epoch 1/20 - loss: 1.4528605937957764 - acc:  66.406250%\n","Training - running batch 86/176 of epoch 1/20 - loss: 2.3681514263153076 - acc:  39.843750%\n","Training - running batch 87/176 of epoch 1/20 - loss: 1.475706934928894 - acc:  73.437500%\n","Training - running batch 88/176 of epoch 1/20 - loss: 2.4178524017333984 - acc:  42.968750%\n","Training - running batch 89/176 of epoch 1/20 - loss: 1.376243233680725 - acc:  64.843750%\n","Training - running batch 90/176 of epoch 1/20 - loss: 2.2362751960754395 - acc:  40.625000%\n","Training - running batch 91/176 of epoch 1/20 - loss: 2.3843886852264404 - acc:  41.406250%\n","Training - running batch 92/176 of epoch 1/20 - loss: 1.2102991342544556 - acc:  72.656250%\n","Training - running batch 93/176 of epoch 1/20 - loss: 2.03180193901062 - acc:  51.562500%\n","Training - running batch 94/176 of epoch 1/20 - loss: 1.4740498065948486 - acc:  71.093750%\n","Training - running batch 95/176 of epoch 1/20 - loss: 2.0195958614349365 - acc:  51.562500%\n","Training - running batch 96/176 of epoch 1/20 - loss: 1.6518383026123047 - acc:  60.937500%\n","Training - running batch 97/176 of epoch 1/20 - loss: 1.2514151334762573 - acc:  71.875000%\n","Training - running batch 98/176 of epoch 1/20 - loss: 2.111650228500366 - acc:  49.218750%\n","Training - running batch 99/176 of epoch 1/20 - loss: 1.2471644878387451 - acc:  68.750000%\n","Training - running batch 100/176 of epoch 1/20 - loss: 2.0782876014709473 - acc:  50.781250%\n","Training - running batch 101/176 of epoch 1/20 - loss: 1.2893272638320923 - acc:  67.968750%\n","Training - running batch 102/176 of epoch 1/20 - loss: 1.9550435543060303 - acc:  52.343750%\n","Training - running batch 103/176 of epoch 1/20 - loss: 1.2682454586029053 - acc:  68.750000%\n","Training - running batch 104/176 of epoch 1/20 - loss: 2.3724539279937744 - acc:  42.187500%\n","Training - running batch 105/176 of epoch 1/20 - loss: 1.8918956518173218 - acc:  50.000000%\n","Training - running batch 106/176 of epoch 1/20 - loss: 2.2649505138397217 - acc:  43.750000%\n","Training - running batch 107/176 of epoch 1/20 - loss: 2.1702075004577637 - acc:  44.531250%\n","Training - running batch 108/176 of epoch 1/20 - loss: 1.2050637006759644 - acc:  75.781250%\n","Training - running batch 109/176 of epoch 1/20 - loss: 2.2115328311920166 - acc:  41.406250%\n","Training - running batch 110/176 of epoch 1/20 - loss: 1.364647626876831 - acc:  71.875000%\n","Training - running batch 111/176 of epoch 1/20 - loss: 1.9212706089019775 - acc:  52.343750%\n","Training - running batch 112/176 of epoch 1/20 - loss: 1.2858662605285645 - acc:  71.093750%\n","Training - running batch 113/176 of epoch 1/20 - loss: 1.0859023332595825 - acc:  73.437500%\n","Training - running batch 114/176 of epoch 1/20 - loss: 2.1927268505096436 - acc:  45.312500%\n","Training - running batch 115/176 of epoch 1/20 - loss: 1.1547259092330933 - acc:  75.781250%\n","Training - running batch 116/176 of epoch 1/20 - loss: 2.0611462593078613 - acc:  48.437500%\n","Training - running batch 117/176 of epoch 1/20 - loss: 1.1725635528564453 - acc:  73.437500%\n","Training - running batch 118/176 of epoch 1/20 - loss: 1.387449860572815 - acc:  70.312500%\n","Training - running batch 119/176 of epoch 1/20 - loss: 1.2182800769805908 - acc:  68.750000%\n","Training - running batch 120/176 of epoch 1/20 - loss: 2.5174436569213867 - acc:  38.281250%\n","Training - running batch 121/176 of epoch 1/20 - loss: 1.4213474988937378 - acc:  61.718750%\n","Training - running batch 122/176 of epoch 1/20 - loss: 1.5708218812942505 - acc:  59.375000%\n","Training - running batch 123/176 of epoch 1/20 - loss: 2.4005887508392334 - acc:  39.062500%\n","Training - running batch 124/176 of epoch 1/20 - loss: 1.478274941444397 - acc:  65.625000%\n","Training - running batch 125/176 of epoch 1/20 - loss: 2.3872463703155518 - acc:  39.843750%\n","Training - running batch 126/176 of epoch 1/20 - loss: 1.3192272186279297 - acc:  67.968750%\n","Training - running batch 127/176 of epoch 1/20 - loss: 2.1200127601623535 - acc:  42.968750%\n","Training - running batch 128/176 of epoch 1/20 - loss: 2.0074450969696045 - acc:  46.093750%\n","Training - running batch 129/176 of epoch 1/20 - loss: 2.2733712196350098 - acc:  42.968750%\n","Training - running batch 130/176 of epoch 1/20 - loss: 1.5592236518859863 - acc:  64.843750%\n","Training - running batch 131/176 of epoch 1/20 - loss: 1.159072995185852 - acc:  71.093750%\n","Training - running batch 132/176 of epoch 1/20 - loss: 1.5011955499649048 - acc:  59.375000%\n","Training - running batch 133/176 of epoch 1/20 - loss: 1.4788302183151245 - acc:  68.750000%\n","Training - running batch 134/176 of epoch 1/20 - loss: 2.017131805419922 - acc:  49.218750%\n","Training - running batch 135/176 of epoch 1/20 - loss: 1.1388548612594604 - acc:  72.656250%\n","Training - running batch 136/176 of epoch 1/20 - loss: 1.5667028427124023 - acc:  62.500000%\n","Training - running batch 137/176 of epoch 1/20 - loss: 1.4636642932891846 - acc:  66.406250%\n","Training - running batch 138/176 of epoch 1/20 - loss: 1.620661973953247 - acc:  60.937500%\n","Training - running batch 139/176 of epoch 1/20 - loss: 1.237749695777893 - acc:  67.187500%\n","Training - running batch 140/176 of epoch 1/20 - loss: 1.683921217918396 - acc:  62.500000%\n","Training - running batch 141/176 of epoch 1/20 - loss: 2.155566930770874 - acc:  46.093750%\n","Training - running batch 142/176 of epoch 1/20 - loss: 2.3785829544067383 - acc:  41.406250%\n","Training - running batch 143/176 of epoch 1/20 - loss: 2.236818552017212 - acc:  42.187500%\n","Training - running batch 144/176 of epoch 1/20 - loss: 1.2764614820480347 - acc:  73.437500%\n","Training - running batch 145/176 of epoch 1/20 - loss: 2.2274229526519775 - acc:  41.406250%\n","Training - running batch 146/176 of epoch 1/20 - loss: 2.1250596046447754 - acc:  48.437500%\n","Training - running batch 147/176 of epoch 1/20 - loss: 2.2531025409698486 - acc:  43.750000%\n","Training - running batch 148/176 of epoch 1/20 - loss: 1.2778981924057007 - acc:  68.750000%\n","Training - running batch 149/176 of epoch 1/20 - loss: 1.2600913047790527 - acc:  71.093750%\n","Training - running batch 150/176 of epoch 1/20 - loss: 1.3470972776412964 - acc:  64.843750%\n","Training - running batch 151/176 of epoch 1/20 - loss: 1.8877370357513428 - acc:  48.437500%\n","Training - running batch 152/176 of epoch 1/20 - loss: 1.5340603590011597 - acc:  68.750000%\n","Training - running batch 153/176 of epoch 1/20 - loss: 2.32487154006958 - acc:  38.281250%\n","Training - running batch 154/176 of epoch 1/20 - loss: 2.4271531105041504 - acc:  37.500000%\n","Training - running batch 155/176 of epoch 1/20 - loss: 2.0559773445129395 - acc:  47.656250%\n","Training - running batch 156/176 of epoch 1/20 - loss: 1.9648460149765015 - acc:  53.906250%\n","Training - running batch 157/176 of epoch 1/20 - loss: 1.2354001998901367 - acc:  71.093750%\n","Training - running batch 158/176 of epoch 1/20 - loss: 2.1530258655548096 - acc:  42.187500%\n","Training - running batch 159/176 of epoch 1/20 - loss: 2.2851643562316895 - acc:  42.187500%\n","Training - running batch 160/176 of epoch 1/20 - loss: 1.1719233989715576 - acc:  74.218750%\n","Training - running batch 161/176 of epoch 1/20 - loss: 1.9708068370819092 - acc:  50.781250%\n","Training - running batch 162/176 of epoch 1/20 - loss: 2.389634370803833 - acc:  41.406250%\n","Training - running batch 163/176 of epoch 1/20 - loss: 2.182199239730835 - acc:  39.843750%\n","Training - running batch 164/176 of epoch 1/20 - loss: 2.1468300819396973 - acc:  46.093750%\n","Training - running batch 165/176 of epoch 1/20 - loss: 1.1021279096603394 - acc:  75.000000%\n","Training - running batch 166/176 of epoch 1/20 - loss: 1.7137830257415771 - acc:  57.812500%\n","Training - running batch 167/176 of epoch 1/20 - loss: 1.1154991388320923 - acc:  69.531250%\n","Training - running batch 168/176 of epoch 1/20 - loss: 1.8959102630615234 - acc:  50.000000%\n","Training - running batch 169/176 of epoch 1/20 - loss: 1.0686578750610352 - acc:  78.125000%\n","Training - running batch 170/176 of epoch 1/20 - loss: 1.3145688772201538 - acc:  70.312500%\n","Training - running batch 171/176 of epoch 1/20 - loss: 1.449520468711853 - acc:  67.968750%\n","Training - running batch 172/176 of epoch 1/20 - loss: 1.973206877708435 - acc:  46.875000%\n","Training - running batch 173/176 of epoch 1/20 - loss: 2.0324032306671143 - acc:  47.656250%\n","Training - running batch 174/176 of epoch 1/20 - loss: 1.2833209037780762 - acc:  75.000000%\n","Training - running batch 175/176 of epoch 1/20 - loss: 2.2907025814056396 - acc:  33.870968%\n","Testing - acc:  0.666973%\n","Training - running batch 0/176 of epoch 2/20 - loss: 1.0787029266357422 - acc:  73.437500%\n","Training - running batch 1/176 of epoch 2/20 - loss: 1.2825721502304077 - acc:  71.875000%\n","Training - running batch 2/176 of epoch 2/20 - loss: 1.4069898128509521 - acc:  73.437500%\n","Training - running batch 3/176 of epoch 2/20 - loss: 1.91573965549469 - acc:  47.656250%\n","Training - running batch 4/176 of epoch 2/20 - loss: 1.841212272644043 - acc:  52.343750%\n","Training - running batch 5/176 of epoch 2/20 - loss: 1.4952938556671143 - acc:  62.500000%\n","Training - running batch 6/176 of epoch 2/20 - loss: 1.2478264570236206 - acc:  73.437500%\n","Training - running batch 7/176 of epoch 2/20 - loss: 1.1715822219848633 - acc:  78.906250%\n","Training - running batch 8/176 of epoch 2/20 - loss: 1.038691520690918 - acc:  78.125000%\n","Training - running batch 9/176 of epoch 2/20 - loss: 1.28704035282135 - acc:  68.750000%\n","Training - running batch 10/176 of epoch 2/20 - loss: 1.2747464179992676 - acc:  75.781250%\n","Training - running batch 11/176 of epoch 2/20 - loss: 1.8118433952331543 - acc:  51.562500%\n","Training - running batch 12/176 of epoch 2/20 - loss: 1.8006887435913086 - acc:  45.312500%\n","Training - running batch 13/176 of epoch 2/20 - loss: 1.8386831283569336 - acc:  53.906250%\n","Training - running batch 14/176 of epoch 2/20 - loss: 1.1450351476669312 - acc:  74.218750%\n","Training - running batch 15/176 of epoch 2/20 - loss: 1.1221582889556885 - acc:  79.687500%\n","Training - running batch 16/176 of epoch 2/20 - loss: 1.4488340616226196 - acc:  57.812500%\n","Training - running batch 17/176 of epoch 2/20 - loss: 1.8160526752471924 - acc:  56.250000%\n","Training - running batch 18/176 of epoch 2/20 - loss: 0.7880741953849792 - acc:  79.687500%\n","Training - running batch 19/176 of epoch 2/20 - loss: 1.736701250076294 - acc:  55.468750%\n","Training - running batch 20/176 of epoch 2/20 - loss: 1.9029285907745361 - acc:  52.343750%\n","Training - running batch 21/176 of epoch 2/20 - loss: 1.0752264261245728 - acc:  81.250000%\n","Training - running batch 22/176 of epoch 2/20 - loss: 0.7942085266113281 - acc:  82.812500%\n","Training - running batch 23/176 of epoch 2/20 - loss: 1.0112218856811523 - acc:  76.562500%\n","Training - running batch 24/176 of epoch 2/20 - loss: 1.9005615711212158 - acc:  53.906250%\n","Training - running batch 25/176 of epoch 2/20 - loss: 1.8353952169418335 - acc:  52.343750%\n","Training - running batch 26/176 of epoch 2/20 - loss: 1.6141802072525024 - acc:  61.718750%\n","Training - running batch 27/176 of epoch 2/20 - loss: 1.8951746225357056 - acc:  54.687500%\n","Training - running batch 28/176 of epoch 2/20 - loss: 0.918074905872345 - acc:  82.031250%\n","Training - running batch 29/176 of epoch 2/20 - loss: 1.1054030656814575 - acc:  77.343750%\n","Training - running batch 30/176 of epoch 2/20 - loss: 1.0047332048416138 - acc:  75.781250%\n","Training - running batch 31/176 of epoch 2/20 - loss: 1.7426269054412842 - acc:  60.937500%\n","Training - running batch 32/176 of epoch 2/20 - loss: 1.7073614597320557 - acc:  57.812500%\n","Training - running batch 33/176 of epoch 2/20 - loss: 1.959676742553711 - acc:  46.093750%\n","Training - running batch 34/176 of epoch 2/20 - loss: 0.9989150166511536 - acc:  75.781250%\n","Training - running batch 35/176 of epoch 2/20 - loss: 0.9831164479255676 - acc:  74.218750%\n","Training - running batch 36/176 of epoch 2/20 - loss: 1.9115906953811646 - acc:  55.468750%\n","Training - running batch 37/176 of epoch 2/20 - loss: 1.0543572902679443 - acc:  73.437500%\n","Training - running batch 38/176 of epoch 2/20 - loss: 1.149010181427002 - acc:  74.218750%\n","Training - running batch 39/176 of epoch 2/20 - loss: 1.0193860530853271 - acc:  75.781250%\n","Training - running batch 40/176 of epoch 2/20 - loss: 1.013940453529358 - acc:  78.906250%\n","Training - running batch 41/176 of epoch 2/20 - loss: 2.059502601623535 - acc:  46.093750%\n","Training - running batch 42/176 of epoch 2/20 - loss: 1.1592706441879272 - acc:  76.562500%\n","Training - running batch 43/176 of epoch 2/20 - loss: 1.9174731969833374 - acc:  48.437500%\n","Training - running batch 44/176 of epoch 2/20 - loss: 1.1363134384155273 - acc:  76.562500%\n","Training - running batch 45/176 of epoch 2/20 - loss: 0.997775137424469 - acc:  73.437500%\n","Training - running batch 46/176 of epoch 2/20 - loss: 0.8351616263389587 - acc:  86.718750%\n","Training - running batch 47/176 of epoch 2/20 - loss: 1.0577950477600098 - acc:  73.437500%\n","Training - running batch 48/176 of epoch 2/20 - loss: 1.9237757921218872 - acc:  49.218750%\n","Training - running batch 49/176 of epoch 2/20 - loss: 1.7653824090957642 - acc:  58.593750%\n","Training - running batch 50/176 of epoch 2/20 - loss: 0.9040257334709167 - acc:  80.468750%\n","Training - running batch 51/176 of epoch 2/20 - loss: 1.1945682764053345 - acc:  75.000000%\n","Training - running batch 52/176 of epoch 2/20 - loss: 1.9951118230819702 - acc:  46.875000%\n","Training - running batch 53/176 of epoch 2/20 - loss: 1.9110668897628784 - acc:  57.812500%\n","Training - running batch 54/176 of epoch 2/20 - loss: 1.9590531587600708 - acc:  45.312500%\n","Training - running batch 55/176 of epoch 2/20 - loss: 1.213044285774231 - acc:  67.968750%\n","Training - running batch 56/176 of epoch 2/20 - loss: 1.9929085969924927 - acc:  42.968750%\n","Training - running batch 57/176 of epoch 2/20 - loss: 2.0886309146881104 - acc:  46.093750%\n","Training - running batch 58/176 of epoch 2/20 - loss: 0.9323874115943909 - acc:  79.687500%\n","Training - running batch 59/176 of epoch 2/20 - loss: 1.1786178350448608 - acc:  67.968750%\n","Training - running batch 60/176 of epoch 2/20 - loss: 1.8906168937683105 - acc:  49.218750%\n","Training - running batch 61/176 of epoch 2/20 - loss: 1.53420889377594 - acc:  62.500000%\n","Training - running batch 62/176 of epoch 2/20 - loss: 1.163399338722229 - acc:  72.656250%\n","Training - running batch 63/176 of epoch 2/20 - loss: 0.9954441785812378 - acc:  79.687500%\n","Training - running batch 64/176 of epoch 2/20 - loss: 1.1513769626617432 - acc:  71.875000%\n","Training - running batch 65/176 of epoch 2/20 - loss: 0.9784855246543884 - acc:  77.343750%\n","Training - running batch 66/176 of epoch 2/20 - loss: 1.146654725074768 - acc:  76.562500%\n","Training - running batch 67/176 of epoch 2/20 - loss: 1.1507627964019775 - acc:  75.000000%\n","Training - running batch 68/176 of epoch 2/20 - loss: 1.995193600654602 - acc:  42.968750%\n","Training - running batch 69/176 of epoch 2/20 - loss: 1.0043047666549683 - acc:  81.250000%\n","Training - running batch 70/176 of epoch 2/20 - loss: 1.2665702104568481 - acc:  75.000000%\n","Training - running batch 71/176 of epoch 2/20 - loss: 1.919692039489746 - acc:  53.906250%\n","Training - running batch 72/176 of epoch 2/20 - loss: 1.9113988876342773 - acc:  44.531250%\n","Training - running batch 73/176 of epoch 2/20 - loss: 1.1141570806503296 - acc:  75.000000%\n","Training - running batch 74/176 of epoch 2/20 - loss: 1.3782846927642822 - acc:  63.281250%\n","Training - running batch 75/176 of epoch 2/20 - loss: 0.9886661767959595 - acc:  78.906250%\n","Training - running batch 76/176 of epoch 2/20 - loss: 1.7851532697677612 - acc:  47.656250%\n","Training - running batch 77/176 of epoch 2/20 - loss: 1.748779296875 - acc:  54.687500%\n","Training - running batch 78/176 of epoch 2/20 - loss: 1.0113897323608398 - acc:  77.343750%\n","Training - running batch 79/176 of epoch 2/20 - loss: 1.7732237577438354 - acc:  50.000000%\n","Training - running batch 80/176 of epoch 2/20 - loss: 1.7183189392089844 - acc:  54.687500%\n","Training - running batch 81/176 of epoch 2/20 - loss: 1.7057075500488281 - acc:  54.687500%\n","Training - running batch 82/176 of epoch 2/20 - loss: 1.4809426069259644 - acc:  64.843750%\n","Training - running batch 83/176 of epoch 2/20 - loss: 1.8811752796173096 - acc:  54.687500%\n","Training - running batch 84/176 of epoch 2/20 - loss: 1.6935662031173706 - acc:  53.125000%\n","Training - running batch 85/176 of epoch 2/20 - loss: 1.704593539237976 - acc:  56.250000%\n","Training - running batch 86/176 of epoch 2/20 - loss: 1.0050190687179565 - acc:  76.562500%\n","Training - running batch 87/176 of epoch 2/20 - loss: 1.1937814950942993 - acc:  73.437500%\n","Training - running batch 88/176 of epoch 2/20 - loss: 1.7479742765426636 - acc:  50.781250%\n","Training - running batch 89/176 of epoch 2/20 - loss: 1.0183955430984497 - acc:  78.906250%\n","Training - running batch 90/176 of epoch 2/20 - loss: 0.8846753239631653 - acc:  75.000000%\n","Training - running batch 91/176 of epoch 2/20 - loss: 1.9515018463134766 - acc:  48.437500%\n","Training - running batch 92/176 of epoch 2/20 - loss: 1.8712518215179443 - acc:  55.468750%\n","Training - running batch 93/176 of epoch 2/20 - loss: 1.7213153839111328 - acc:  57.812500%\n","Training - running batch 94/176 of epoch 2/20 - loss: 1.0899626016616821 - acc:  75.000000%\n","Training - running batch 95/176 of epoch 2/20 - loss: 1.1375221014022827 - acc:  75.781250%\n","Training - running batch 96/176 of epoch 2/20 - loss: 1.6570078134536743 - acc:  57.031250%\n","Training - running batch 97/176 of epoch 2/20 - loss: 1.641652226448059 - acc:  53.125000%\n","Training - running batch 98/176 of epoch 2/20 - loss: 1.6814552545547485 - acc:  53.906250%\n","Training - running batch 99/176 of epoch 2/20 - loss: 1.6000535488128662 - acc:  58.593750%\n","Training - running batch 100/176 of epoch 2/20 - loss: 1.2090412378311157 - acc:  74.218750%\n","Training - running batch 101/176 of epoch 2/20 - loss: 1.023057222366333 - acc:  76.562500%\n","Training - running batch 102/176 of epoch 2/20 - loss: 0.9913427233695984 - acc:  79.687500%\n","Training - running batch 103/176 of epoch 2/20 - loss: 0.9807314872741699 - acc:  73.437500%\n","Training - running batch 104/176 of epoch 2/20 - loss: 1.189271092414856 - acc:  67.968750%\n","Training - running batch 105/176 of epoch 2/20 - loss: 1.639695167541504 - acc:  57.812500%\n","Training - running batch 106/176 of epoch 2/20 - loss: 1.3118282556533813 - acc:  65.625000%\n","Training - running batch 107/176 of epoch 2/20 - loss: 1.8918205499649048 - acc:  58.593750%\n","Training - running batch 108/176 of epoch 2/20 - loss: 1.6460298299789429 - acc:  53.906250%\n","Training - running batch 109/176 of epoch 2/20 - loss: 1.2214337587356567 - acc:  74.218750%\n","Training - running batch 110/176 of epoch 2/20 - loss: 1.065368413925171 - acc:  69.531250%\n","Training - running batch 111/176 of epoch 2/20 - loss: 1.785569190979004 - acc:  51.562500%\n","Training - running batch 112/176 of epoch 2/20 - loss: 1.2027696371078491 - acc:  75.000000%\n","Training - running batch 113/176 of epoch 2/20 - loss: 1.6390935182571411 - acc:  59.375000%\n","Training - running batch 114/176 of epoch 2/20 - loss: 1.1483335494995117 - acc:  78.906250%\n","Training - running batch 115/176 of epoch 2/20 - loss: 0.9433517456054688 - acc:  78.906250%\n","Training - running batch 116/176 of epoch 2/20 - loss: 2.042325258255005 - acc:  52.343750%\n","Training - running batch 117/176 of epoch 2/20 - loss: 1.0012017488479614 - acc:  76.562500%\n","Training - running batch 118/176 of epoch 2/20 - loss: 1.431368112564087 - acc:  60.937500%\n","Training - running batch 119/176 of epoch 2/20 - loss: 1.757935643196106 - acc:  50.781250%\n","Training - running batch 120/176 of epoch 2/20 - loss: 1.1062567234039307 - acc:  67.187500%\n","Training - running batch 121/176 of epoch 2/20 - loss: 1.1290452480316162 - acc:  69.531250%\n","Training - running batch 122/176 of epoch 2/20 - loss: 1.937241792678833 - acc:  53.125000%\n","Training - running batch 123/176 of epoch 2/20 - loss: 1.0038392543792725 - acc:  77.343750%\n","Training - running batch 124/176 of epoch 2/20 - loss: 1.8526324033737183 - acc:  51.562500%\n","Training - running batch 125/176 of epoch 2/20 - loss: 1.0828547477722168 - acc:  81.250000%\n","Training - running batch 126/176 of epoch 2/20 - loss: 1.3623113632202148 - acc:  68.750000%\n","Training - running batch 127/176 of epoch 2/20 - loss: 1.6697295904159546 - acc:  50.000000%\n","Training - running batch 128/176 of epoch 2/20 - loss: 1.9401720762252808 - acc:  49.218750%\n","Training - running batch 129/176 of epoch 2/20 - loss: 1.3878287076950073 - acc:  64.843750%\n","Training - running batch 130/176 of epoch 2/20 - loss: 1.7776927947998047 - acc:  44.531250%\n","Training - running batch 131/176 of epoch 2/20 - loss: 1.5625112056732178 - acc:  58.593750%\n","Training - running batch 132/176 of epoch 2/20 - loss: 1.8047212362289429 - acc:  46.875000%\n","Training - running batch 133/176 of epoch 2/20 - loss: 0.9901673793792725 - acc:  76.562500%\n","Training - running batch 134/176 of epoch 2/20 - loss: 1.1278090476989746 - acc:  74.218750%\n","Training - running batch 135/176 of epoch 2/20 - loss: 1.5754399299621582 - acc:  59.375000%\n","Training - running batch 136/176 of epoch 2/20 - loss: 0.8452788591384888 - acc:  85.937500%\n","Training - running batch 137/176 of epoch 2/20 - loss: 1.9108891487121582 - acc:  49.218750%\n","Training - running batch 138/176 of epoch 2/20 - loss: 1.0109446048736572 - acc:  78.906250%\n","Training - running batch 139/176 of epoch 2/20 - loss: 1.8829503059387207 - acc:  48.437500%\n","Training - running batch 140/176 of epoch 2/20 - loss: 1.6551727056503296 - acc:  53.906250%\n","Training - running batch 141/176 of epoch 2/20 - loss: 1.1301804780960083 - acc:  67.968750%\n","Training - running batch 142/176 of epoch 2/20 - loss: 0.9650768041610718 - acc:  72.656250%\n","Training - running batch 143/176 of epoch 2/20 - loss: 0.8233543634414673 - acc:  81.250000%\n","Training - running batch 144/176 of epoch 2/20 - loss: 1.7618769407272339 - acc:  51.562500%\n","Training - running batch 145/176 of epoch 2/20 - loss: 1.912712574005127 - acc:  52.343750%\n","Training - running batch 146/176 of epoch 2/20 - loss: 1.618074893951416 - acc:  57.031250%\n","Training - running batch 147/176 of epoch 2/20 - loss: 1.4851055145263672 - acc:  59.375000%\n","Training - running batch 148/176 of epoch 2/20 - loss: 1.045343279838562 - acc:  74.218750%\n","Training - running batch 149/176 of epoch 2/20 - loss: 1.6338496208190918 - acc:  57.812500%\n","Training - running batch 150/176 of epoch 2/20 - loss: 0.9803141951560974 - acc:  82.031250%\n","Training - running batch 151/176 of epoch 2/20 - loss: 1.5935558080673218 - acc:  54.687500%\n","Training - running batch 152/176 of epoch 2/20 - loss: 1.871203899383545 - acc:  50.781250%\n","Training - running batch 153/176 of epoch 2/20 - loss: 1.6634669303894043 - acc:  57.031250%\n","Training - running batch 154/176 of epoch 2/20 - loss: 0.7984902858734131 - acc:  83.593750%\n","Training - running batch 155/176 of epoch 2/20 - loss: 1.7862675189971924 - acc:  54.687500%\n","Training - running batch 156/176 of epoch 2/20 - loss: 1.919539213180542 - acc:  53.125000%\n","Training - running batch 157/176 of epoch 2/20 - loss: 1.7983975410461426 - acc:  51.562500%\n","Training - running batch 158/176 of epoch 2/20 - loss: 1.1064690351486206 - acc:  73.437500%\n","Training - running batch 159/176 of epoch 2/20 - loss: 1.3935422897338867 - acc:  55.468750%\n","Training - running batch 160/176 of epoch 2/20 - loss: 1.536636471748352 - acc:  57.031250%\n","Training - running batch 161/176 of epoch 2/20 - loss: 1.5347588062286377 - acc:  54.687500%\n","Training - running batch 162/176 of epoch 2/20 - loss: 1.3430801630020142 - acc:  72.656250%\n","Training - running batch 163/176 of epoch 2/20 - loss: 1.5045045614242554 - acc:  59.375000%\n","Training - running batch 164/176 of epoch 2/20 - loss: 1.7047322988510132 - acc:  57.812500%\n","Training - running batch 165/176 of epoch 2/20 - loss: 1.532030701637268 - acc:  57.031250%\n","Training - running batch 166/176 of epoch 2/20 - loss: 1.138814926147461 - acc:  65.625000%\n","Training - running batch 167/176 of epoch 2/20 - loss: 2.119927406311035 - acc:  46.875000%\n","Training - running batch 168/176 of epoch 2/20 - loss: 1.2459239959716797 - acc:  72.656250%\n","Training - running batch 169/176 of epoch 2/20 - loss: 1.0055724382400513 - acc:  77.343750%\n","Training - running batch 170/176 of epoch 2/20 - loss: 1.6538283824920654 - acc:  51.562500%\n","Training - running batch 171/176 of epoch 2/20 - loss: 1.5377836227416992 - acc:  54.687500%\n","Training - running batch 172/176 of epoch 2/20 - loss: 1.1395208835601807 - acc:  78.906250%\n","Training - running batch 173/176 of epoch 2/20 - loss: 0.7325537800788879 - acc:  89.843750%\n","Training - running batch 174/176 of epoch 2/20 - loss: 1.3721693754196167 - acc:  59.375000%\n","Training - running batch 175/176 of epoch 2/20 - loss: 2.334132432937622 - acc:  43.548387%\n","Testing - acc:  0.682580%\n","Training - running batch 0/176 of epoch 3/20 - loss: 0.7066118717193604 - acc:  91.406250%\n","Training - running batch 1/176 of epoch 3/20 - loss: 0.6581009030342102 - acc:  92.187500%\n","Training - running batch 2/176 of epoch 3/20 - loss: 1.40017831325531 - acc:  64.062500%\n","Training - running batch 3/176 of epoch 3/20 - loss: 1.4676584005355835 - acc:  63.281250%\n","Training - running batch 4/176 of epoch 3/20 - loss: 0.7810601592063904 - acc:  85.156250%\n","Training - running batch 5/176 of epoch 3/20 - loss: 1.5974701642990112 - acc:  58.593750%\n","Training - running batch 6/176 of epoch 3/20 - loss: 0.8484088778495789 - acc:  80.468750%\n","Training - running batch 7/176 of epoch 3/20 - loss: 1.1963863372802734 - acc:  75.000000%\n","Training - running batch 8/176 of epoch 3/20 - loss: 1.4875199794769287 - acc:  58.593750%\n","Training - running batch 9/176 of epoch 3/20 - loss: 0.8763428330421448 - acc:  77.343750%\n","Training - running batch 10/176 of epoch 3/20 - loss: 0.8889963626861572 - acc:  85.156250%\n","Training - running batch 11/176 of epoch 3/20 - loss: 1.6652531623840332 - acc:  53.906250%\n","Training - running batch 12/176 of epoch 3/20 - loss: 1.5074994564056396 - acc:  58.593750%\n","Training - running batch 13/176 of epoch 3/20 - loss: 0.9579199552536011 - acc:  78.125000%\n","Training - running batch 14/176 of epoch 3/20 - loss: 1.4965351819992065 - acc:  57.812500%\n","Training - running batch 15/176 of epoch 3/20 - loss: 1.0095751285552979 - acc:  75.000000%\n","Training - running batch 16/176 of epoch 3/20 - loss: 1.4616661071777344 - acc:  62.500000%\n","Training - running batch 17/176 of epoch 3/20 - loss: 1.468396782875061 - acc:  57.031250%\n","Training - running batch 18/176 of epoch 3/20 - loss: 0.7618424892425537 - acc:  87.500000%\n","Training - running batch 19/176 of epoch 3/20 - loss: 0.7799620628356934 - acc:  86.718750%\n","Training - running batch 20/176 of epoch 3/20 - loss: 0.9954167008399963 - acc:  78.906250%\n","Training - running batch 21/176 of epoch 3/20 - loss: 0.759105920791626 - acc:  86.718750%\n","Training - running batch 22/176 of epoch 3/20 - loss: 0.7968740463256836 - acc:  78.906250%\n","Training - running batch 23/176 of epoch 3/20 - loss: 1.0329219102859497 - acc:  71.093750%\n","Training - running batch 24/176 of epoch 3/20 - loss: 1.38167405128479 - acc:  59.375000%\n","Training - running batch 25/176 of epoch 3/20 - loss: 1.011745572090149 - acc:  81.250000%\n","Training - running batch 26/176 of epoch 3/20 - loss: 1.457517385482788 - acc:  60.937500%\n","Training - running batch 27/176 of epoch 3/20 - loss: 1.3900749683380127 - acc:  66.406250%\n","Training - running batch 28/176 of epoch 3/20 - loss: 1.7150487899780273 - acc:  57.031250%\n","Training - running batch 29/176 of epoch 3/20 - loss: 0.9840989708900452 - acc:  77.343750%\n","Training - running batch 30/176 of epoch 3/20 - loss: 0.9136732220649719 - acc:  82.031250%\n","Training - running batch 31/176 of epoch 3/20 - loss: 1.548143744468689 - acc:  60.156250%\n","Training - running batch 32/176 of epoch 3/20 - loss: 1.6078797578811646 - acc:  55.468750%\n","Training - running batch 33/176 of epoch 3/20 - loss: 1.3166685104370117 - acc:  67.187500%\n","Training - running batch 34/176 of epoch 3/20 - loss: 0.7509900331497192 - acc:  85.156250%\n","Training - running batch 35/176 of epoch 3/20 - loss: 0.7503482103347778 - acc:  85.156250%\n","Training - running batch 36/176 of epoch 3/20 - loss: 0.5908438563346863 - acc:  93.750000%\n","Training - running batch 37/176 of epoch 3/20 - loss: 1.513067603111267 - acc:  58.593750%\n","Training - running batch 38/176 of epoch 3/20 - loss: 1.6509860754013062 - acc:  54.687500%\n","Training - running batch 39/176 of epoch 3/20 - loss: 0.7290295362472534 - acc:  84.375000%\n","Training - running batch 40/176 of epoch 3/20 - loss: 0.6976080536842346 - acc:  85.156250%\n","Training - running batch 41/176 of epoch 3/20 - loss: 1.2871304750442505 - acc:  64.843750%\n","Training - running batch 42/176 of epoch 3/20 - loss: 0.8009375929832458 - acc:  82.812500%\n","Training - running batch 43/176 of epoch 3/20 - loss: 1.6294316053390503 - acc:  53.125000%\n","Training - running batch 44/176 of epoch 3/20 - loss: 1.462043285369873 - acc:  57.031250%\n","Training - running batch 45/176 of epoch 3/20 - loss: 0.792236864566803 - acc:  82.812500%\n","Training - running batch 46/176 of epoch 3/20 - loss: 0.8166055679321289 - acc:  78.125000%\n","Training - running batch 47/176 of epoch 3/20 - loss: 1.5354441404342651 - acc:  57.812500%\n","Training - running batch 48/176 of epoch 3/20 - loss: 0.8971954584121704 - acc:  78.125000%\n","Training - running batch 49/176 of epoch 3/20 - loss: 1.7855119705200195 - acc:  50.781250%\n","Training - running batch 50/176 of epoch 3/20 - loss: 1.4344618320465088 - acc:  56.250000%\n","Training - running batch 51/176 of epoch 3/20 - loss: 1.4078162908554077 - acc:  58.593750%\n","Training - running batch 52/176 of epoch 3/20 - loss: 1.5305640697479248 - acc:  60.156250%\n","Training - running batch 53/176 of epoch 3/20 - loss: 1.5662249326705933 - acc:  53.125000%\n","Training - running batch 54/176 of epoch 3/20 - loss: 1.2539584636688232 - acc:  62.500000%\n","Training - running batch 55/176 of epoch 3/20 - loss: 1.5419186353683472 - acc:  57.812500%\n","Training - running batch 56/176 of epoch 3/20 - loss: 1.494696855545044 - acc:  59.375000%\n","Training - running batch 57/176 of epoch 3/20 - loss: 0.9073044061660767 - acc:  79.687500%\n","Training - running batch 58/176 of epoch 3/20 - loss: 1.0616400241851807 - acc:  76.562500%\n","Training - running batch 59/176 of epoch 3/20 - loss: 1.5814399719238281 - acc:  60.937500%\n","Training - running batch 60/176 of epoch 3/20 - loss: 0.8743058443069458 - acc:  78.906250%\n","Training - running batch 61/176 of epoch 3/20 - loss: 0.6968104243278503 - acc:  85.937500%\n","Training - running batch 62/176 of epoch 3/20 - loss: 1.4933178424835205 - acc:  61.718750%\n","Training - running batch 63/176 of epoch 3/20 - loss: 1.3232839107513428 - acc:  66.406250%\n","Training - running batch 64/176 of epoch 3/20 - loss: 0.7823985815048218 - acc:  84.375000%\n","Training - running batch 65/176 of epoch 3/20 - loss: 0.8048768639564514 - acc:  82.812500%\n","Training - running batch 66/176 of epoch 3/20 - loss: 1.046080470085144 - acc:  76.562500%\n","Training - running batch 67/176 of epoch 3/20 - loss: 1.1486705541610718 - acc:  65.625000%\n","Training - running batch 68/176 of epoch 3/20 - loss: 0.986824631690979 - acc:  77.343750%\n","Training - running batch 69/176 of epoch 3/20 - loss: 1.395143747329712 - acc:  63.281250%\n","Training - running batch 70/176 of epoch 3/20 - loss: 1.123808741569519 - acc:  74.218750%\n","Training - running batch 71/176 of epoch 3/20 - loss: 1.5746017694473267 - acc:  50.000000%\n","Training - running batch 72/176 of epoch 3/20 - loss: 1.5330482721328735 - acc:  57.812500%\n","Training - running batch 73/176 of epoch 3/20 - loss: 1.3416472673416138 - acc:  64.843750%\n","Training - running batch 74/176 of epoch 3/20 - loss: 1.3699872493743896 - acc:  57.031250%\n","Training - running batch 75/176 of epoch 3/20 - loss: 1.3853708505630493 - acc:  58.593750%\n","Training - running batch 76/176 of epoch 3/20 - loss: 1.4938496351242065 - acc:  56.250000%\n","Training - running batch 77/176 of epoch 3/20 - loss: 1.0382912158966064 - acc:  73.437500%\n","Training - running batch 78/176 of epoch 3/20 - loss: 1.2679649591445923 - acc:  60.937500%\n","Training - running batch 79/176 of epoch 3/20 - loss: 1.0295642614364624 - acc:  75.781250%\n","Training - running batch 80/176 of epoch 3/20 - loss: 1.6820697784423828 - acc:  53.906250%\n","Training - running batch 81/176 of epoch 3/20 - loss: 1.3809902667999268 - acc:  64.062500%\n","Training - running batch 82/176 of epoch 3/20 - loss: 1.3817098140716553 - acc:  63.281250%\n","Training - running batch 83/176 of epoch 3/20 - loss: 1.5184574127197266 - acc:  53.906250%\n","Training - running batch 84/176 of epoch 3/20 - loss: 0.9552196264266968 - acc:  75.000000%\n","Training - running batch 85/176 of epoch 3/20 - loss: 1.08740234375 - acc:  75.781250%\n","Training - running batch 86/176 of epoch 3/20 - loss: 0.7463101744651794 - acc:  83.593750%\n","Training - running batch 87/176 of epoch 3/20 - loss: 1.8103710412979126 - acc:  53.906250%\n","Training - running batch 88/176 of epoch 3/20 - loss: 1.0347163677215576 - acc:  75.781250%\n","Training - running batch 89/176 of epoch 3/20 - loss: 1.5655829906463623 - acc:  60.156250%\n","Training - running batch 90/176 of epoch 3/20 - loss: 1.4962207078933716 - acc:  60.156250%\n","Training - running batch 91/176 of epoch 3/20 - loss: 0.6474762558937073 - acc:  91.406250%\n","Training - running batch 92/176 of epoch 3/20 - loss: 0.8196153044700623 - acc:  81.250000%\n","Training - running batch 93/176 of epoch 3/20 - loss: 1.3631513118743896 - acc:  58.593750%\n","Training - running batch 94/176 of epoch 3/20 - loss: 0.7877869009971619 - acc:  83.593750%\n","Training - running batch 95/176 of epoch 3/20 - loss: 1.3885225057601929 - acc:  60.156250%\n","Training - running batch 96/176 of epoch 3/20 - loss: 0.9500597715377808 - acc:  80.468750%\n","Training - running batch 97/176 of epoch 3/20 - loss: 1.2550735473632812 - acc:  65.625000%\n","Training - running batch 98/176 of epoch 3/20 - loss: 1.5164799690246582 - acc:  55.468750%\n","Training - running batch 99/176 of epoch 3/20 - loss: 0.9755446314811707 - acc:  82.031250%\n","Training - running batch 100/176 of epoch 3/20 - loss: 0.9445058703422546 - acc:  81.250000%\n","Training - running batch 101/176 of epoch 3/20 - loss: 1.5128309726715088 - acc:  57.812500%\n","Training - running batch 102/176 of epoch 3/20 - loss: 1.350823998451233 - acc:  66.406250%\n","Training - running batch 103/176 of epoch 3/20 - loss: 0.8383249044418335 - acc:  85.937500%\n","Training - running batch 104/176 of epoch 3/20 - loss: 1.45371413230896 - acc:  64.843750%\n","Training - running batch 105/176 of epoch 3/20 - loss: 0.7988176345825195 - acc:  82.812500%\n","Training - running batch 106/176 of epoch 3/20 - loss: 1.372483730316162 - acc:  63.281250%\n","Training - running batch 107/176 of epoch 3/20 - loss: 1.0032933950424194 - acc:  80.468750%\n","Training - running batch 108/176 of epoch 3/20 - loss: 1.2476215362548828 - acc:  65.625000%\n","Training - running batch 109/176 of epoch 3/20 - loss: 0.8807603120803833 - acc:  81.250000%\n","Training - running batch 110/176 of epoch 3/20 - loss: 0.6911768317222595 - acc:  89.062500%\n","Training - running batch 111/176 of epoch 3/20 - loss: 0.9260239601135254 - acc:  74.218750%\n","Training - running batch 112/176 of epoch 3/20 - loss: 1.072455644607544 - acc:  69.531250%\n","Training - running batch 113/176 of epoch 3/20 - loss: 1.7161580324172974 - acc:  49.218750%\n","Training - running batch 114/176 of epoch 3/20 - loss: 1.3756699562072754 - acc:  58.593750%\n","Training - running batch 115/176 of epoch 3/20 - loss: 0.6658307313919067 - acc:  88.281250%\n","Training - running batch 116/176 of epoch 3/20 - loss: 0.9491065144538879 - acc:  78.906250%\n","Training - running batch 117/176 of epoch 3/20 - loss: 1.4670864343643188 - acc:  60.937500%\n","Training - running batch 118/176 of epoch 3/20 - loss: 0.8531754612922668 - acc:  82.031250%\n","Training - running batch 119/176 of epoch 3/20 - loss: 1.0092148780822754 - acc:  75.781250%\n","Training - running batch 120/176 of epoch 3/20 - loss: 1.4478449821472168 - acc:  60.156250%\n","Training - running batch 121/176 of epoch 3/20 - loss: 0.9557703733444214 - acc:  79.687500%\n","Training - running batch 122/176 of epoch 3/20 - loss: 1.0430504083633423 - acc:  73.437500%\n","Training - running batch 123/176 of epoch 3/20 - loss: 1.4230836629867554 - acc:  60.156250%\n","Training - running batch 124/176 of epoch 3/20 - loss: 1.4184755086898804 - acc:  60.937500%\n","Training - running batch 125/176 of epoch 3/20 - loss: 0.8444812297821045 - acc:  75.000000%\n","Training - running batch 126/176 of epoch 3/20 - loss: 1.304243803024292 - acc:  60.937500%\n","Training - running batch 127/176 of epoch 3/20 - loss: 1.327852725982666 - acc:  62.500000%\n","Training - running batch 128/176 of epoch 3/20 - loss: 1.589779257774353 - acc:  59.375000%\n","Training - running batch 129/176 of epoch 3/20 - loss: 0.6425368189811707 - acc:  90.625000%\n","Training - running batch 130/176 of epoch 3/20 - loss: 0.9302705526351929 - acc:  76.562500%\n","Training - running batch 131/176 of epoch 3/20 - loss: 0.676913321018219 - acc:  88.281250%\n","Training - running batch 132/176 of epoch 3/20 - loss: 1.642697811126709 - acc:  53.906250%\n","Training - running batch 133/176 of epoch 3/20 - loss: 1.3956555128097534 - acc:  60.937500%\n","Training - running batch 134/176 of epoch 3/20 - loss: 1.072690725326538 - acc:  75.000000%\n","Training - running batch 135/176 of epoch 3/20 - loss: 1.4853696823120117 - acc:  60.156250%\n","Training - running batch 136/176 of epoch 3/20 - loss: 1.644465684890747 - acc:  52.343750%\n","Training - running batch 137/176 of epoch 3/20 - loss: 1.6570545434951782 - acc:  53.906250%\n","Training - running batch 138/176 of epoch 3/20 - loss: 1.0469260215759277 - acc:  76.562500%\n","Training - running batch 139/176 of epoch 3/20 - loss: 1.2776434421539307 - acc:  60.156250%\n","Training - running batch 140/176 of epoch 3/20 - loss: 0.8353719711303711 - acc:  77.343750%\n","Training - running batch 141/176 of epoch 3/20 - loss: 1.6370365619659424 - acc:  50.781250%\n","Training - running batch 142/176 of epoch 3/20 - loss: 0.6134273409843445 - acc:  89.062500%\n","Training - running batch 143/176 of epoch 3/20 - loss: 1.3791619539260864 - acc:  65.625000%\n","Training - running batch 144/176 of epoch 3/20 - loss: 0.8489490151405334 - acc:  85.937500%\n","Training - running batch 145/176 of epoch 3/20 - loss: 0.8649510145187378 - acc:  82.812500%\n","Training - running batch 146/176 of epoch 3/20 - loss: 0.810391902923584 - acc:  79.687500%\n","Training - running batch 147/176 of epoch 3/20 - loss: 0.710054337978363 - acc:  87.500000%\n","Training - running batch 148/176 of epoch 3/20 - loss: 1.6796482801437378 - acc:  49.218750%\n","Training - running batch 149/176 of epoch 3/20 - loss: 0.8650543689727783 - acc:  75.781250%\n","Training - running batch 150/176 of epoch 3/20 - loss: 0.8144821524620056 - acc:  79.687500%\n","Training - running batch 151/176 of epoch 3/20 - loss: 1.1055994033813477 - acc:  72.656250%\n","Training - running batch 152/176 of epoch 3/20 - loss: 0.7779402732849121 - acc:  82.031250%\n","Training - running batch 153/176 of epoch 3/20 - loss: 0.9017767906188965 - acc:  79.687500%\n","Training - running batch 154/176 of epoch 3/20 - loss: 0.8767381310462952 - acc:  84.375000%\n","Training - running batch 155/176 of epoch 3/20 - loss: 1.792180061340332 - acc:  49.218750%\n","Training - running batch 156/176 of epoch 3/20 - loss: 1.756160855293274 - acc:  50.781250%\n","Training - running batch 157/176 of epoch 3/20 - loss: 0.9314153790473938 - acc:  75.000000%\n","Training - running batch 158/176 of epoch 3/20 - loss: 1.389190435409546 - acc:  57.812500%\n","Training - running batch 159/176 of epoch 3/20 - loss: 0.9658187031745911 - acc:  79.687500%\n","Training - running batch 160/176 of epoch 3/20 - loss: 1.4184470176696777 - acc:  57.812500%\n","Training - running batch 161/176 of epoch 3/20 - loss: 0.7790167331695557 - acc:  79.687500%\n","Training - running batch 162/176 of epoch 3/20 - loss: 1.5881335735321045 - acc:  53.125000%\n","Training - running batch 163/176 of epoch 3/20 - loss: 0.9925135970115662 - acc:  75.000000%\n","Training - running batch 164/176 of epoch 3/20 - loss: 1.5864471197128296 - acc:  53.906250%\n","Training - running batch 165/176 of epoch 3/20 - loss: 1.3600561618804932 - acc:  55.468750%\n","Training - running batch 166/176 of epoch 3/20 - loss: 1.376546025276184 - acc:  57.812500%\n","Training - running batch 167/176 of epoch 3/20 - loss: 1.232335090637207 - acc:  63.281250%\n","Training - running batch 168/176 of epoch 3/20 - loss: 1.0595027208328247 - acc:  70.312500%\n","Training - running batch 169/176 of epoch 3/20 - loss: 1.371021032333374 - acc:  63.281250%\n","Training - running batch 170/176 of epoch 3/20 - loss: 1.499609351158142 - acc:  56.250000%\n","Training - running batch 171/176 of epoch 3/20 - loss: 0.6524327397346497 - acc:  89.062500%\n","Training - running batch 172/176 of epoch 3/20 - loss: 1.0639230012893677 - acc:  74.218750%\n","Training - running batch 173/176 of epoch 3/20 - loss: 1.2777400016784668 - acc:  60.156250%\n","Training - running batch 174/176 of epoch 3/20 - loss: 1.6569194793701172 - acc:  51.562500%\n","Training - running batch 175/176 of epoch 3/20 - loss: 1.4388954639434814 - acc:  66.129032%\n","Testing - acc:  0.673399%\n","Training - running batch 0/176 of epoch 4/20 - loss: 0.8238970637321472 - acc:  84.375000%\n","Training - running batch 1/176 of epoch 4/20 - loss: 1.3222801685333252 - acc:  62.500000%\n","Training - running batch 2/176 of epoch 4/20 - loss: 1.45537269115448 - acc:  57.031250%\n","Training - running batch 3/176 of epoch 4/20 - loss: 1.223275065422058 - acc:  63.281250%\n","Training - running batch 4/176 of epoch 4/20 - loss: 1.432186245918274 - acc:  52.343750%\n","Training - running batch 5/176 of epoch 4/20 - loss: 1.1356416940689087 - acc:  71.093750%\n","Training - running batch 6/176 of epoch 4/20 - loss: 1.1852271556854248 - acc:  62.500000%\n","Training - running batch 7/176 of epoch 4/20 - loss: 0.7761577367782593 - acc:  83.593750%\n","Training - running batch 8/176 of epoch 4/20 - loss: 1.264968752861023 - acc:  64.062500%\n","Training - running batch 9/176 of epoch 4/20 - loss: 1.2298612594604492 - acc:  64.843750%\n","Training - running batch 10/176 of epoch 4/20 - loss: 1.3153280019760132 - acc:  61.718750%\n","Training - running batch 11/176 of epoch 4/20 - loss: 0.7163040041923523 - acc:  83.593750%\n","Training - running batch 12/176 of epoch 4/20 - loss: 0.8129916787147522 - acc:  80.468750%\n","Training - running batch 13/176 of epoch 4/20 - loss: 1.380922794342041 - acc:  63.281250%\n","Training - running batch 14/176 of epoch 4/20 - loss: 0.6501759886741638 - acc:  89.843750%\n","Training - running batch 15/176 of epoch 4/20 - loss: 1.4572546482086182 - acc:  60.937500%\n","Training - running batch 16/176 of epoch 4/20 - loss: 0.7517518997192383 - acc:  84.375000%\n","Training - running batch 17/176 of epoch 4/20 - loss: 1.3898158073425293 - acc:  61.718750%\n","Training - running batch 18/176 of epoch 4/20 - loss: 0.7588381767272949 - acc:  83.593750%\n","Training - running batch 19/176 of epoch 4/20 - loss: 1.2856043577194214 - acc:  63.281250%\n","Training - running batch 20/176 of epoch 4/20 - loss: 0.6907979249954224 - acc:  85.937500%\n","Training - running batch 21/176 of epoch 4/20 - loss: 0.7226815223693848 - acc:  89.843750%\n","Training - running batch 22/176 of epoch 4/20 - loss: 0.7792433500289917 - acc:  85.937500%\n","Training - running batch 23/176 of epoch 4/20 - loss: 1.167189598083496 - acc:  65.625000%\n","Training - running batch 24/176 of epoch 4/20 - loss: 1.3109304904937744 - acc:  61.718750%\n","Training - running batch 25/176 of epoch 4/20 - loss: 0.6857016682624817 - acc:  87.500000%\n","Training - running batch 26/176 of epoch 4/20 - loss: 0.7696338295936584 - acc:  82.031250%\n","Training - running batch 27/176 of epoch 4/20 - loss: 1.1481890678405762 - acc:  64.843750%\n","Training - running batch 28/176 of epoch 4/20 - loss: 1.0918288230895996 - acc:  67.187500%\n","Training - running batch 29/176 of epoch 4/20 - loss: 1.2698876857757568 - acc:  64.843750%\n","Training - running batch 30/176 of epoch 4/20 - loss: 0.6916244029998779 - acc:  83.593750%\n","Training - running batch 31/176 of epoch 4/20 - loss: 1.3346920013427734 - acc:  60.937500%\n","Training - running batch 32/176 of epoch 4/20 - loss: 1.095033049583435 - acc:  70.312500%\n","Training - running batch 33/176 of epoch 4/20 - loss: 0.9858836531639099 - acc:  75.000000%\n","Training - running batch 34/176 of epoch 4/20 - loss: 0.6961169838905334 - acc:  86.718750%\n","Training - running batch 35/176 of epoch 4/20 - loss: 1.1973618268966675 - acc:  65.625000%\n","Training - running batch 36/176 of epoch 4/20 - loss: 1.280908465385437 - acc:  60.937500%\n","Training - running batch 37/176 of epoch 4/20 - loss: 0.8321260809898376 - acc:  78.906250%\n","Training - running batch 38/176 of epoch 4/20 - loss: 1.280970811843872 - acc:  66.406250%\n","Training - running batch 39/176 of epoch 4/20 - loss: 0.7523834109306335 - acc:  79.687500%\n","Training - running batch 40/176 of epoch 4/20 - loss: 1.1331795454025269 - acc:  62.500000%\n","Training - running batch 41/176 of epoch 4/20 - loss: 1.1944690942764282 - acc:  62.500000%\n","Training - running batch 42/176 of epoch 4/20 - loss: 0.7358483672142029 - acc:  84.375000%\n","Training - running batch 43/176 of epoch 4/20 - loss: 1.2613095045089722 - acc:  60.156250%\n","Training - running batch 44/176 of epoch 4/20 - loss: 1.0349339246749878 - acc:  72.656250%\n","Training - running batch 45/176 of epoch 4/20 - loss: 1.2154535055160522 - acc:  65.625000%\n","Training - running batch 46/176 of epoch 4/20 - loss: 0.7014346718788147 - acc:  85.937500%\n","Training - running batch 47/176 of epoch 4/20 - loss: 0.7007237076759338 - acc:  85.937500%\n","Training - running batch 48/176 of epoch 4/20 - loss: 1.350877285003662 - acc:  60.937500%\n","Training - running batch 49/176 of epoch 4/20 - loss: 0.7809562683105469 - acc:  82.812500%\n","Training - running batch 50/176 of epoch 4/20 - loss: 0.965850293636322 - acc:  78.125000%\n","Training - running batch 51/176 of epoch 4/20 - loss: 0.6240792870521545 - acc:  89.062500%\n","Training - running batch 52/176 of epoch 4/20 - loss: 0.8895983695983887 - acc:  76.562500%\n","Training - running batch 53/176 of epoch 4/20 - loss: 0.5849224328994751 - acc:  89.062500%\n","Training - running batch 54/176 of epoch 4/20 - loss: 0.706978440284729 - acc:  82.812500%\n","Training - running batch 55/176 of epoch 4/20 - loss: 0.6852412223815918 - acc:  79.687500%\n","Training - running batch 56/176 of epoch 4/20 - loss: 0.8032400012016296 - acc:  82.812500%\n","Training - running batch 57/176 of epoch 4/20 - loss: 0.8965931534767151 - acc:  79.687500%\n","Training - running batch 58/176 of epoch 4/20 - loss: 0.7430198788642883 - acc:  86.718750%\n","Training - running batch 59/176 of epoch 4/20 - loss: 0.7800883054733276 - acc:  85.156250%\n","Training - running batch 60/176 of epoch 4/20 - loss: 1.2176399230957031 - acc:  60.937500%\n","Training - running batch 61/176 of epoch 4/20 - loss: 0.49831974506378174 - acc:  90.625000%\n","Training - running batch 62/176 of epoch 4/20 - loss: 1.1461035013198853 - acc:  64.062500%\n","Training - running batch 63/176 of epoch 4/20 - loss: 0.6197934150695801 - acc:  88.281250%\n","Training - running batch 64/176 of epoch 4/20 - loss: 1.2436494827270508 - acc:  64.843750%\n","Training - running batch 65/176 of epoch 4/20 - loss: 1.1905351877212524 - acc:  64.843750%\n","Training - running batch 66/176 of epoch 4/20 - loss: 1.1912097930908203 - acc:  63.281250%\n","Training - running batch 67/176 of epoch 4/20 - loss: 0.8511917591094971 - acc:  81.250000%\n","Training - running batch 68/176 of epoch 4/20 - loss: 0.5907419919967651 - acc:  85.156250%\n","Training - running batch 69/176 of epoch 4/20 - loss: 1.2317869663238525 - acc:  59.375000%\n","Training - running batch 70/176 of epoch 4/20 - loss: 0.8374942541122437 - acc:  82.031250%\n","Training - running batch 71/176 of epoch 4/20 - loss: 0.7993860840797424 - acc:  85.937500%\n","Training - running batch 72/176 of epoch 4/20 - loss: 1.2863476276397705 - acc:  59.375000%\n","Training - running batch 73/176 of epoch 4/20 - loss: 1.269972324371338 - acc:  68.750000%\n","Training - running batch 74/176 of epoch 4/20 - loss: 0.7399342060089111 - acc:  82.031250%\n","Training - running batch 75/176 of epoch 4/20 - loss: 1.1119015216827393 - acc:  70.312500%\n","Training - running batch 76/176 of epoch 4/20 - loss: 0.8680300116539001 - acc:  82.812500%\n","Training - running batch 77/176 of epoch 4/20 - loss: 1.3645659685134888 - acc:  61.718750%\n","Training - running batch 78/176 of epoch 4/20 - loss: 0.5709633827209473 - acc:  87.500000%\n","Training - running batch 79/176 of epoch 4/20 - loss: 1.2351781129837036 - acc:  67.187500%\n","Training - running batch 80/176 of epoch 4/20 - loss: 0.6953285932540894 - acc:  82.812500%\n","Training - running batch 81/176 of epoch 4/20 - loss: 0.8257339000701904 - acc:  80.468750%\n","Training - running batch 82/176 of epoch 4/20 - loss: 1.3686665296554565 - acc:  54.687500%\n","Training - running batch 83/176 of epoch 4/20 - loss: 0.5567779541015625 - acc:  89.843750%\n","Training - running batch 84/176 of epoch 4/20 - loss: 1.3150126934051514 - acc:  61.718750%\n","Training - running batch 85/176 of epoch 4/20 - loss: 0.8230497241020203 - acc:  82.031250%\n","Training - running batch 86/176 of epoch 4/20 - loss: 0.6780840158462524 - acc:  83.593750%\n","Training - running batch 87/176 of epoch 4/20 - loss: 0.7103767395019531 - acc:  84.375000%\n","Training - running batch 88/176 of epoch 4/20 - loss: 1.4812430143356323 - acc:  57.031250%\n","Training - running batch 89/176 of epoch 4/20 - loss: 1.2401372194290161 - acc:  58.593750%\n","Training - running batch 90/176 of epoch 4/20 - loss: 1.2344571352005005 - acc:  60.156250%\n","Training - running batch 91/176 of epoch 4/20 - loss: 1.4474881887435913 - acc:  54.687500%\n","Training - running batch 92/176 of epoch 4/20 - loss: 0.4882289171218872 - acc:  92.187500%\n","Training - running batch 93/176 of epoch 4/20 - loss: 0.7214988470077515 - acc:  85.937500%\n","Training - running batch 94/176 of epoch 4/20 - loss: 1.5458225011825562 - acc:  46.875000%\n","Training - running batch 95/176 of epoch 4/20 - loss: 0.7485405802726746 - acc:  87.500000%\n","Training - running batch 96/176 of epoch 4/20 - loss: 1.0307480096817017 - acc:  69.531250%\n","Training - running batch 97/176 of epoch 4/20 - loss: 1.224889874458313 - acc:  62.500000%\n","Training - running batch 98/176 of epoch 4/20 - loss: 0.8095017671585083 - acc:  85.156250%\n","Training - running batch 99/176 of epoch 4/20 - loss: 0.7527868151664734 - acc:  85.156250%\n","Training - running batch 100/176 of epoch 4/20 - loss: 0.5003529787063599 - acc:  89.843750%\n","Training - running batch 101/176 of epoch 4/20 - loss: 0.7394086718559265 - acc:  85.156250%\n","Training - running batch 102/176 of epoch 4/20 - loss: 1.2063205242156982 - acc:  62.500000%\n","Training - running batch 103/176 of epoch 4/20 - loss: 1.20169198513031 - acc:  67.968750%\n","Training - running batch 104/176 of epoch 4/20 - loss: 0.7115309238433838 - acc:  82.812500%\n","Training - running batch 105/176 of epoch 4/20 - loss: 1.1926846504211426 - acc:  66.406250%\n","Training - running batch 106/176 of epoch 4/20 - loss: 0.92638099193573 - acc:  79.687500%\n","Training - running batch 107/176 of epoch 4/20 - loss: 1.318967342376709 - acc:  64.843750%\n","Training - running batch 108/176 of epoch 4/20 - loss: 1.1219825744628906 - acc:  65.625000%\n","Training - running batch 109/176 of epoch 4/20 - loss: 1.068035364151001 - acc:  68.750000%\n","Training - running batch 110/176 of epoch 4/20 - loss: 0.5185744762420654 - acc:  90.625000%\n","Training - running batch 111/176 of epoch 4/20 - loss: 1.2423222064971924 - acc:  62.500000%\n","Training - running batch 112/176 of epoch 4/20 - loss: 1.4784836769104004 - acc:  54.687500%\n","Training - running batch 113/176 of epoch 4/20 - loss: 0.5597583651542664 - acc:  89.062500%\n","Training - running batch 114/176 of epoch 4/20 - loss: 1.3519625663757324 - acc:  60.937500%\n","Training - running batch 115/176 of epoch 4/20 - loss: 0.7112187743186951 - acc:  87.500000%\n","Training - running batch 116/176 of epoch 4/20 - loss: 0.5140503644943237 - acc:  91.406250%\n","Training - running batch 117/176 of epoch 4/20 - loss: 0.7436726689338684 - acc:  81.250000%\n","Training - running batch 118/176 of epoch 4/20 - loss: 1.1558127403259277 - acc:  64.843750%\n","Training - running batch 119/176 of epoch 4/20 - loss: 1.1573587656021118 - acc:  66.406250%\n","Training - running batch 120/176 of epoch 4/20 - loss: 1.3369542360305786 - acc:  62.500000%\n","Training - running batch 121/176 of epoch 4/20 - loss: 0.7155845165252686 - acc:  81.250000%\n","Training - running batch 122/176 of epoch 4/20 - loss: 1.0696067810058594 - acc:  66.406250%\n","Training - running batch 123/176 of epoch 4/20 - loss: 1.0412652492523193 - acc:  69.531250%\n","Training - running batch 124/176 of epoch 4/20 - loss: 0.6453129649162292 - acc:  85.937500%\n","Training - running batch 125/176 of epoch 4/20 - loss: 1.3308547735214233 - acc:  63.281250%\n","Training - running batch 126/176 of epoch 4/20 - loss: 0.5670663118362427 - acc:  91.406250%\n","Training - running batch 127/176 of epoch 4/20 - loss: 0.8908056020736694 - acc:  83.593750%\n","Training - running batch 128/176 of epoch 4/20 - loss: 1.1809085607528687 - acc:  66.406250%\n","Training - running batch 129/176 of epoch 4/20 - loss: 1.3389017581939697 - acc:  61.718750%\n","Training - running batch 130/176 of epoch 4/20 - loss: 1.098177194595337 - acc:  67.968750%\n","Training - running batch 131/176 of epoch 4/20 - loss: 0.6263150572776794 - acc:  89.062500%\n","Training - running batch 132/176 of epoch 4/20 - loss: 0.9242316484451294 - acc:  78.125000%\n","Training - running batch 133/176 of epoch 4/20 - loss: 0.6164608001708984 - acc:  87.500000%\n","Training - running batch 134/176 of epoch 4/20 - loss: 1.3178061246871948 - acc:  58.593750%\n","Training - running batch 135/176 of epoch 4/20 - loss: 1.1622302532196045 - acc:  67.187500%\n","Training - running batch 136/176 of epoch 4/20 - loss: 0.6955101490020752 - acc:  84.375000%\n","Training - running batch 137/176 of epoch 4/20 - loss: 0.6334599852561951 - acc:  88.281250%\n","Training - running batch 138/176 of epoch 4/20 - loss: 1.2161967754364014 - acc:  64.062500%\n","Training - running batch 139/176 of epoch 4/20 - loss: 0.7904609441757202 - acc:  86.718750%\n","Training - running batch 140/176 of epoch 4/20 - loss: 0.7143682837486267 - acc:  87.500000%\n","Training - running batch 141/176 of epoch 4/20 - loss: 0.6904280781745911 - acc:  85.156250%\n","Training - running batch 142/176 of epoch 4/20 - loss: 0.7366964221000671 - acc:  81.250000%\n","Training - running batch 143/176 of epoch 4/20 - loss: 0.646969199180603 - acc:  88.281250%\n","Training - running batch 144/176 of epoch 4/20 - loss: 1.1665029525756836 - acc:  69.531250%\n","Training - running batch 145/176 of epoch 4/20 - loss: 1.2565069198608398 - acc:  62.500000%\n","Training - running batch 146/176 of epoch 4/20 - loss: 0.5896742343902588 - acc:  86.718750%\n","Training - running batch 147/176 of epoch 4/20 - loss: 0.7837121486663818 - acc:  78.906250%\n","Training - running batch 148/176 of epoch 4/20 - loss: 0.6883038878440857 - acc:  83.593750%\n","Training - running batch 149/176 of epoch 4/20 - loss: 0.7841899394989014 - acc:  78.125000%\n","Training - running batch 150/176 of epoch 4/20 - loss: 1.440851092338562 - acc:  58.593750%\n","Training - running batch 151/176 of epoch 4/20 - loss: 1.153368592262268 - acc:  67.968750%\n","Training - running batch 152/176 of epoch 4/20 - loss: 1.1107205152511597 - acc:  69.531250%\n","Training - running batch 153/176 of epoch 4/20 - loss: 1.212615728378296 - acc:  64.062500%\n","Training - running batch 154/176 of epoch 4/20 - loss: 1.2803798913955688 - acc:  56.250000%\n","Training - running batch 155/176 of epoch 4/20 - loss: 0.5966960191726685 - acc:  85.156250%\n","Training - running batch 156/176 of epoch 4/20 - loss: 1.3779373168945312 - acc:  60.937500%\n","Training - running batch 157/176 of epoch 4/20 - loss: 1.3471565246582031 - acc:  57.812500%\n","Training - running batch 158/176 of epoch 4/20 - loss: 1.455504298210144 - acc:  56.250000%\n","Training - running batch 159/176 of epoch 4/20 - loss: 1.2110469341278076 - acc:  64.062500%\n","Training - running batch 160/176 of epoch 4/20 - loss: 0.8640819787979126 - acc:  80.468750%\n","Training - running batch 161/176 of epoch 4/20 - loss: 1.1991097927093506 - acc:  65.625000%\n","Training - running batch 162/176 of epoch 4/20 - loss: 0.800948977470398 - acc:  80.468750%\n","Training - running batch 163/176 of epoch 4/20 - loss: 1.0823001861572266 - acc:  64.843750%\n","Training - running batch 164/176 of epoch 4/20 - loss: 1.1260038614273071 - acc:  66.406250%\n","Training - running batch 165/176 of epoch 4/20 - loss: 1.1404756307601929 - acc:  68.750000%\n","Training - running batch 166/176 of epoch 4/20 - loss: 0.8688486814498901 - acc:  80.468750%\n","Training - running batch 167/176 of epoch 4/20 - loss: 1.1076781749725342 - acc:  75.781250%\n","Training - running batch 168/176 of epoch 4/20 - loss: 0.9274851083755493 - acc:  80.468750%\n","Training - running batch 169/176 of epoch 4/20 - loss: 1.3317296504974365 - acc:  59.375000%\n","Training - running batch 170/176 of epoch 4/20 - loss: 0.711593747138977 - acc:  77.343750%\n","Training - running batch 171/176 of epoch 4/20 - loss: 1.0976603031158447 - acc:  71.093750%\n","Training - running batch 172/176 of epoch 4/20 - loss: 0.692811131477356 - acc:  82.812500%\n","Training - running batch 173/176 of epoch 4/20 - loss: 0.9944994449615479 - acc:  67.968750%\n","Training - running batch 174/176 of epoch 4/20 - loss: 1.1556943655014038 - acc:  66.406250%\n","Training - running batch 175/176 of epoch 4/20 - loss: 0.645841658115387 - acc:  90.322581%\n","Testing - acc:  0.687170%\n","Training - running batch 0/176 of epoch 5/20 - loss: 1.0693213939666748 - acc:  67.187500%\n","Training - running batch 1/176 of epoch 5/20 - loss: 0.5756914615631104 - acc:  88.281250%\n","Training - running batch 2/176 of epoch 5/20 - loss: 0.7322434782981873 - acc:  85.156250%\n","Training - running batch 3/176 of epoch 5/20 - loss: 0.606397807598114 - acc:  86.718750%\n","Training - running batch 4/176 of epoch 5/20 - loss: 0.8909258842468262 - acc:  75.000000%\n","Training - running batch 5/176 of epoch 5/20 - loss: 0.6540416479110718 - acc:  86.718750%\n","Training - running batch 6/176 of epoch 5/20 - loss: 0.7362335920333862 - acc:  82.812500%\n","Training - running batch 7/176 of epoch 5/20 - loss: 0.727814257144928 - acc:  83.593750%\n","Training - running batch 8/176 of epoch 5/20 - loss: 1.2969883680343628 - acc:  65.625000%\n","Training - running batch 9/176 of epoch 5/20 - loss: 1.0547171831130981 - acc:  68.750000%\n","Training - running batch 10/176 of epoch 5/20 - loss: 0.8102484941482544 - acc:  87.500000%\n","Training - running batch 11/176 of epoch 5/20 - loss: 1.1822988986968994 - acc:  65.625000%\n","Training - running batch 12/176 of epoch 5/20 - loss: 0.9831801056861877 - acc:  69.531250%\n","Training - running batch 13/176 of epoch 5/20 - loss: 1.0874114036560059 - acc:  67.187500%\n","Training - running batch 14/176 of epoch 5/20 - loss: 1.1426589488983154 - acc:  67.187500%\n","Training - running batch 15/176 of epoch 5/20 - loss: 0.6497620940208435 - acc:  85.937500%\n","Training - running batch 16/176 of epoch 5/20 - loss: 0.5571191906929016 - acc:  87.500000%\n","Training - running batch 17/176 of epoch 5/20 - loss: 1.0987149477005005 - acc:  65.625000%\n","Training - running batch 18/176 of epoch 5/20 - loss: 0.5121897459030151 - acc:  92.187500%\n","Training - running batch 19/176 of epoch 5/20 - loss: 0.717375636100769 - acc:  86.718750%\n","Training - running batch 20/176 of epoch 5/20 - loss: 0.7612138390541077 - acc:  82.031250%\n","Training - running batch 21/176 of epoch 5/20 - loss: 1.021054983139038 - acc:  71.875000%\n","Training - running batch 22/176 of epoch 5/20 - loss: 0.6194775700569153 - acc:  86.718750%\n","Training - running batch 23/176 of epoch 5/20 - loss: 0.6637999415397644 - acc:  84.375000%\n","Training - running batch 24/176 of epoch 5/20 - loss: 1.0581644773483276 - acc:  67.187500%\n","Training - running batch 25/176 of epoch 5/20 - loss: 0.5790596604347229 - acc:  85.937500%\n","Training - running batch 26/176 of epoch 5/20 - loss: 0.6069018840789795 - acc:  86.718750%\n","Training - running batch 27/176 of epoch 5/20 - loss: 0.7137255072593689 - acc:  89.843750%\n","Training - running batch 28/176 of epoch 5/20 - loss: 0.9785128235816956 - acc:  71.875000%\n","Training - running batch 29/176 of epoch 5/20 - loss: 1.0366485118865967 - acc:  69.531250%\n","Training - running batch 30/176 of epoch 5/20 - loss: 0.6860875487327576 - acc:  85.937500%\n","Training - running batch 31/176 of epoch 5/20 - loss: 0.6087089776992798 - acc:  89.062500%\n","Training - running batch 32/176 of epoch 5/20 - loss: 1.042973518371582 - acc:  68.750000%\n","Training - running batch 33/176 of epoch 5/20 - loss: 1.177987813949585 - acc:  60.937500%\n","Training - running batch 34/176 of epoch 5/20 - loss: 0.9444808959960938 - acc:  81.250000%\n","Training - running batch 35/176 of epoch 5/20 - loss: 0.6805733442306519 - acc:  81.250000%\n","Training - running batch 36/176 of epoch 5/20 - loss: 1.004421591758728 - acc:  71.093750%\n","Training - running batch 37/176 of epoch 5/20 - loss: 0.5081830024719238 - acc:  85.937500%\n","Training - running batch 38/176 of epoch 5/20 - loss: 0.6700785160064697 - acc:  85.156250%\n","Training - running batch 39/176 of epoch 5/20 - loss: 0.5823414921760559 - acc:  93.750000%\n","Training - running batch 40/176 of epoch 5/20 - loss: 0.5112214684486389 - acc:  91.406250%\n","Training - running batch 41/176 of epoch 5/20 - loss: 0.7987045049667358 - acc:  72.656250%\n","Training - running batch 42/176 of epoch 5/20 - loss: 0.4977189600467682 - acc:  91.406250%\n","Training - running batch 43/176 of epoch 5/20 - loss: 0.663899838924408 - acc:  87.500000%\n","Training - running batch 44/176 of epoch 5/20 - loss: 1.32001531124115 - acc:  62.500000%\n","Training - running batch 45/176 of epoch 5/20 - loss: 1.097377061843872 - acc:  62.500000%\n","Training - running batch 46/176 of epoch 5/20 - loss: 0.6311711072921753 - acc:  84.375000%\n","Training - running batch 47/176 of epoch 5/20 - loss: 0.7606865167617798 - acc:  82.812500%\n","Training - running batch 48/176 of epoch 5/20 - loss: 0.6312609910964966 - acc:  84.375000%\n","Training - running batch 49/176 of epoch 5/20 - loss: 0.4735676348209381 - acc:  89.062500%\n","Training - running batch 50/176 of epoch 5/20 - loss: 1.132371187210083 - acc:  67.187500%\n","Training - running batch 51/176 of epoch 5/20 - loss: 0.9790496826171875 - acc:  71.875000%\n","Training - running batch 52/176 of epoch 5/20 - loss: 0.5673761367797852 - acc:  89.062500%\n","Training - running batch 53/176 of epoch 5/20 - loss: 1.0862483978271484 - acc:  62.500000%\n","Training - running batch 54/176 of epoch 5/20 - loss: 1.1595228910446167 - acc:  60.156250%\n","Training - running batch 55/176 of epoch 5/20 - loss: 0.5123283863067627 - acc:  89.843750%\n","Training - running batch 56/176 of epoch 5/20 - loss: 0.5598181486129761 - acc:  88.281250%\n","Training - running batch 57/176 of epoch 5/20 - loss: 0.5770054459571838 - acc:  87.500000%\n","Training - running batch 58/176 of epoch 5/20 - loss: 1.1638473272323608 - acc:  64.843750%\n","Training - running batch 59/176 of epoch 5/20 - loss: 1.1348320245742798 - acc:  67.187500%\n","Training - running batch 60/176 of epoch 5/20 - loss: 1.1415190696716309 - acc:  63.281250%\n","Training - running batch 61/176 of epoch 5/20 - loss: 1.2578181028366089 - acc:  64.062500%\n","Training - running batch 62/176 of epoch 5/20 - loss: 0.6119073629379272 - acc:  82.812500%\n","Training - running batch 63/176 of epoch 5/20 - loss: 1.1132113933563232 - acc:  67.187500%\n","Training - running batch 64/176 of epoch 5/20 - loss: 0.5815272331237793 - acc:  85.156250%\n","Training - running batch 65/176 of epoch 5/20 - loss: 1.3553204536437988 - acc:  57.031250%\n","Training - running batch 66/176 of epoch 5/20 - loss: 0.48854172229766846 - acc:  91.406250%\n","Training - running batch 67/176 of epoch 5/20 - loss: 1.1520776748657227 - acc:  62.500000%\n","Training - running batch 68/176 of epoch 5/20 - loss: 0.6534388661384583 - acc:  82.031250%\n","Training - running batch 69/176 of epoch 5/20 - loss: 0.9903661012649536 - acc:  67.187500%\n","Training - running batch 70/176 of epoch 5/20 - loss: 0.578277051448822 - acc:  89.062500%\n","Training - running batch 71/176 of epoch 5/20 - loss: 1.120398998260498 - acc:  62.500000%\n","Training - running batch 72/176 of epoch 5/20 - loss: 0.6976459622383118 - acc:  82.812500%\n","Training - running batch 73/176 of epoch 5/20 - loss: 0.5225819945335388 - acc:  91.406250%\n","Training - running batch 74/176 of epoch 5/20 - loss: 0.5219919681549072 - acc:  89.062500%\n","Training - running batch 75/176 of epoch 5/20 - loss: 0.9401795268058777 - acc:  69.531250%\n","Training - running batch 76/176 of epoch 5/20 - loss: 0.7637283802032471 - acc:  83.593750%\n","Training - running batch 77/176 of epoch 5/20 - loss: 1.153244137763977 - acc:  65.625000%\n","Training - running batch 78/176 of epoch 5/20 - loss: 1.0490323305130005 - acc:  68.750000%\n","Training - running batch 79/176 of epoch 5/20 - loss: 0.5958581566810608 - acc:  88.281250%\n","Training - running batch 80/176 of epoch 5/20 - loss: 0.6133999228477478 - acc:  85.937500%\n","Training - running batch 81/176 of epoch 5/20 - loss: 1.0843762159347534 - acc:  68.750000%\n","Training - running batch 82/176 of epoch 5/20 - loss: 1.1026380062103271 - acc:  65.625000%\n","Training - running batch 83/176 of epoch 5/20 - loss: 1.152732014656067 - acc:  64.062500%\n","Training - running batch 84/176 of epoch 5/20 - loss: 1.1930530071258545 - acc:  60.937500%\n","Training - running batch 85/176 of epoch 5/20 - loss: 0.5451836585998535 - acc:  88.281250%\n","Training - running batch 86/176 of epoch 5/20 - loss: 1.0976392030715942 - acc:  67.968750%\n","Training - running batch 87/176 of epoch 5/20 - loss: 0.7038492560386658 - acc:  84.375000%\n","Training - running batch 88/176 of epoch 5/20 - loss: 1.098595380783081 - acc:  67.187500%\n","Training - running batch 89/176 of epoch 5/20 - loss: 0.9586275815963745 - acc:  67.968750%\n","Training - running batch 90/176 of epoch 5/20 - loss: 1.1150436401367188 - acc:  66.406250%\n","Training - running batch 91/176 of epoch 5/20 - loss: 0.6017908453941345 - acc:  87.500000%\n","Training - running batch 92/176 of epoch 5/20 - loss: 1.172358512878418 - acc:  67.968750%\n","Training - running batch 93/176 of epoch 5/20 - loss: 1.0110160112380981 - acc:  67.968750%\n","Training - running batch 94/176 of epoch 5/20 - loss: 0.6377578377723694 - acc:  84.375000%\n","Training - running batch 95/176 of epoch 5/20 - loss: 1.163119912147522 - acc:  65.625000%\n","Training - running batch 96/176 of epoch 5/20 - loss: 0.6247422099113464 - acc:  84.375000%\n","Training - running batch 97/176 of epoch 5/20 - loss: 0.6362975239753723 - acc:  82.812500%\n","Training - running batch 98/176 of epoch 5/20 - loss: 0.6781538724899292 - acc:  85.937500%\n","Training - running batch 99/176 of epoch 5/20 - loss: 0.43237385153770447 - acc:  94.531250%\n","Training - running batch 100/176 of epoch 5/20 - loss: 1.0142182111740112 - acc:  72.656250%\n","Training - running batch 101/176 of epoch 5/20 - loss: 0.6102631688117981 - acc:  82.031250%\n","Training - running batch 102/176 of epoch 5/20 - loss: 0.9635665416717529 - acc:  69.531250%\n","Training - running batch 103/176 of epoch 5/20 - loss: 0.9577938914299011 - acc:  71.875000%\n","Training - running batch 104/176 of epoch 5/20 - loss: 1.1432313919067383 - acc:  61.718750%\n","Training - running batch 105/176 of epoch 5/20 - loss: 0.9625802040100098 - acc:  68.750000%\n","Training - running batch 106/176 of epoch 5/20 - loss: 1.0292019844055176 - acc:  64.843750%\n","Training - running batch 107/176 of epoch 5/20 - loss: 0.5289471745491028 - acc:  88.281250%\n","Training - running batch 108/176 of epoch 5/20 - loss: 0.7923896908760071 - acc:  80.468750%\n","Training - running batch 109/176 of epoch 5/20 - loss: 0.6840927600860596 - acc:  83.593750%\n","Training - running batch 110/176 of epoch 5/20 - loss: 0.9712581038475037 - acc:  71.093750%\n","Training - running batch 111/176 of epoch 5/20 - loss: 1.1330137252807617 - acc:  64.062500%\n","Training - running batch 112/176 of epoch 5/20 - loss: 0.95484459400177 - acc:  69.531250%\n","Training - running batch 113/176 of epoch 5/20 - loss: 0.5617620348930359 - acc:  87.500000%\n","Training - running batch 114/176 of epoch 5/20 - loss: 0.7301464080810547 - acc:  87.500000%\n","Training - running batch 115/176 of epoch 5/20 - loss: 0.8998153209686279 - acc:  71.093750%\n","Training - running batch 116/176 of epoch 5/20 - loss: 1.1850206851959229 - acc:  66.406250%\n","Training - running batch 117/176 of epoch 5/20 - loss: 0.6282923817634583 - acc:  85.937500%\n","Training - running batch 118/176 of epoch 5/20 - loss: 0.9326473474502563 - acc:  71.093750%\n","Training - running batch 119/176 of epoch 5/20 - loss: 0.6949566006660461 - acc:  86.718750%\n","Training - running batch 120/176 of epoch 5/20 - loss: 0.551405131816864 - acc:  84.375000%\n","Training - running batch 121/176 of epoch 5/20 - loss: 0.6059020161628723 - acc:  84.375000%\n","Training - running batch 122/176 of epoch 5/20 - loss: 0.7304831743240356 - acc:  85.937500%\n","Training - running batch 123/176 of epoch 5/20 - loss: 1.0407910346984863 - acc:  71.093750%\n","Training - running batch 124/176 of epoch 5/20 - loss: 0.6403483152389526 - acc:  85.156250%\n","Training - running batch 125/176 of epoch 5/20 - loss: 1.5695031881332397 - acc:  52.343750%\n","Training - running batch 126/176 of epoch 5/20 - loss: 0.6278976798057556 - acc:  83.593750%\n","Training - running batch 127/176 of epoch 5/20 - loss: 1.0155186653137207 - acc:  68.750000%\n","Training - running batch 128/176 of epoch 5/20 - loss: 0.5860162973403931 - acc:  89.062500%\n","Training - running batch 129/176 of epoch 5/20 - loss: 0.9619706869125366 - acc:  68.750000%\n","Training - running batch 130/176 of epoch 5/20 - loss: 0.8185281157493591 - acc:  78.125000%\n","Training - running batch 131/176 of epoch 5/20 - loss: 1.1494323015213013 - acc:  67.187500%\n","Training - running batch 132/176 of epoch 5/20 - loss: 1.2756741046905518 - acc:  59.375000%\n","Training - running batch 133/176 of epoch 5/20 - loss: 0.8360493183135986 - acc:  81.250000%\n","Training - running batch 134/176 of epoch 5/20 - loss: 0.8888713717460632 - acc:  71.093750%\n","Training - running batch 135/176 of epoch 5/20 - loss: 1.0035039186477661 - acc:  66.406250%\n","Training - running batch 136/176 of epoch 5/20 - loss: 1.1367346048355103 - acc:  64.062500%\n","Training - running batch 137/176 of epoch 5/20 - loss: 1.0104328393936157 - acc:  71.875000%\n","Training - running batch 138/176 of epoch 5/20 - loss: 1.0337508916854858 - acc:  67.968750%\n","Training - running batch 139/176 of epoch 5/20 - loss: 0.6541425585746765 - acc:  84.375000%\n","Training - running batch 140/176 of epoch 5/20 - loss: 1.2139935493469238 - acc:  60.156250%\n","Training - running batch 141/176 of epoch 5/20 - loss: 0.9603018760681152 - acc:  71.875000%\n","Training - running batch 142/176 of epoch 5/20 - loss: 0.6864771842956543 - acc:  85.937500%\n","Training - running batch 143/176 of epoch 5/20 - loss: 0.921692967414856 - acc:  74.218750%\n","Training - running batch 144/176 of epoch 5/20 - loss: 1.0937563180923462 - acc:  66.406250%\n","Training - running batch 145/176 of epoch 5/20 - loss: 0.7176464200019836 - acc:  85.156250%\n","Training - running batch 146/176 of epoch 5/20 - loss: 1.0439274311065674 - acc:  64.062500%\n","Training - running batch 147/176 of epoch 5/20 - loss: 0.8589800596237183 - acc:  73.437500%\n","Training - running batch 148/176 of epoch 5/20 - loss: 1.0951234102249146 - acc:  67.187500%\n","Training - running batch 149/176 of epoch 5/20 - loss: 0.9082047939300537 - acc:  73.437500%\n","Training - running batch 150/176 of epoch 5/20 - loss: 1.1768829822540283 - acc:  60.937500%\n","Training - running batch 151/176 of epoch 5/20 - loss: 0.7079669833183289 - acc:  78.906250%\n","Training - running batch 152/176 of epoch 5/20 - loss: 0.48954853415489197 - acc:  92.968750%\n","Training - running batch 153/176 of epoch 5/20 - loss: 0.9286960363388062 - acc:  82.812500%\n","Training - running batch 154/176 of epoch 5/20 - loss: 1.1945569515228271 - acc:  60.937500%\n","Training - running batch 155/176 of epoch 5/20 - loss: 0.6910185217857361 - acc:  82.812500%\n","Training - running batch 156/176 of epoch 5/20 - loss: 0.6182064414024353 - acc:  85.937500%\n","Training - running batch 157/176 of epoch 5/20 - loss: 0.9926393032073975 - acc:  67.187500%\n","Training - running batch 158/176 of epoch 5/20 - loss: 1.028611421585083 - acc:  66.406250%\n","Training - running batch 159/176 of epoch 5/20 - loss: 1.0833125114440918 - acc:  68.750000%\n","Training - running batch 160/176 of epoch 5/20 - loss: 0.4870182275772095 - acc:  91.406250%\n","Training - running batch 161/176 of epoch 5/20 - loss: 0.8864034414291382 - acc:  82.812500%\n","Training - running batch 162/176 of epoch 5/20 - loss: 1.1491708755493164 - acc:  60.937500%\n","Training - running batch 163/176 of epoch 5/20 - loss: 0.6107807755470276 - acc:  86.718750%\n","Training - running batch 164/176 of epoch 5/20 - loss: 1.0684020519256592 - acc:  71.093750%\n","Training - running batch 165/176 of epoch 5/20 - loss: 1.256858468055725 - acc:  64.062500%\n","Training - running batch 166/176 of epoch 5/20 - loss: 1.0724340677261353 - acc:  64.062500%\n","Training - running batch 167/176 of epoch 5/20 - loss: 0.9968075752258301 - acc:  71.093750%\n","Training - running batch 168/176 of epoch 5/20 - loss: 1.1186368465423584 - acc:  61.718750%\n","Training - running batch 169/176 of epoch 5/20 - loss: 1.0145032405853271 - acc:  71.875000%\n","Training - running batch 170/176 of epoch 5/20 - loss: 0.5721752643585205 - acc:  87.500000%\n","Training - running batch 171/176 of epoch 5/20 - loss: 0.7243850827217102 - acc:  82.812500%\n","Training - running batch 172/176 of epoch 5/20 - loss: 0.5852553844451904 - acc:  89.843750%\n","Training - running batch 173/176 of epoch 5/20 - loss: 1.0164148807525635 - acc:  66.406250%\n","Training - running batch 174/176 of epoch 5/20 - loss: 1.1683673858642578 - acc:  67.187500%\n","Training - running batch 175/176 of epoch 5/20 - loss: 1.098630666732788 - acc:  64.516129%\n","Testing - acc:  0.668809%\n","Training - running batch 0/176 of epoch 6/20 - loss: 0.8466000556945801 - acc:  71.093750%\n","Training - running batch 1/176 of epoch 6/20 - loss: 0.5702182054519653 - acc:  91.406250%\n","Training - running batch 2/176 of epoch 6/20 - loss: 0.5642704367637634 - acc:  84.375000%\n","Training - running batch 3/176 of epoch 6/20 - loss: 0.5095868706703186 - acc:  89.843750%\n","Training - running batch 4/176 of epoch 6/20 - loss: 0.44558876752853394 - acc:  95.312500%\n","Training - running batch 5/176 of epoch 6/20 - loss: 0.9733936190605164 - acc:  65.625000%\n","Training - running batch 6/176 of epoch 6/20 - loss: 1.0267651081085205 - acc:  71.093750%\n","Training - running batch 7/176 of epoch 6/20 - loss: 0.6431873440742493 - acc:  85.937500%\n","Training - running batch 8/176 of epoch 6/20 - loss: 1.0167040824890137 - acc:  70.312500%\n","Training - running batch 9/176 of epoch 6/20 - loss: 1.0871268510818481 - acc:  64.843750%\n","Training - running batch 10/176 of epoch 6/20 - loss: 0.8973024487495422 - acc:  77.343750%\n","Training - running batch 11/176 of epoch 6/20 - loss: 0.5215866565704346 - acc:  92.187500%\n","Training - running batch 12/176 of epoch 6/20 - loss: 1.0515918731689453 - acc:  67.968750%\n","Training - running batch 13/176 of epoch 6/20 - loss: 0.8512672781944275 - acc:  72.656250%\n","Training - running batch 14/176 of epoch 6/20 - loss: 0.5294060707092285 - acc:  91.406250%\n","Training - running batch 15/176 of epoch 6/20 - loss: 0.8172669410705566 - acc:  72.656250%\n","Training - running batch 16/176 of epoch 6/20 - loss: 0.9250390529632568 - acc:  70.312500%\n","Training - running batch 17/176 of epoch 6/20 - loss: 0.9994529485702515 - acc:  74.218750%\n","Training - running batch 18/176 of epoch 6/20 - loss: 0.9392493963241577 - acc:  69.531250%\n","Training - running batch 19/176 of epoch 6/20 - loss: 0.568043053150177 - acc:  90.625000%\n","Training - running batch 20/176 of epoch 6/20 - loss: 0.9263526201248169 - acc:  76.562500%\n","Training - running batch 21/176 of epoch 6/20 - loss: 0.8929073810577393 - acc:  73.437500%\n","Training - running batch 22/176 of epoch 6/20 - loss: 0.8602470755577087 - acc:  75.000000%\n","Training - running batch 23/176 of epoch 6/20 - loss: 0.9062553644180298 - acc:  71.875000%\n","Training - running batch 24/176 of epoch 6/20 - loss: 1.073931336402893 - acc:  66.406250%\n","Training - running batch 25/176 of epoch 6/20 - loss: 0.4121168851852417 - acc:  90.625000%\n","Training - running batch 26/176 of epoch 6/20 - loss: 0.8416072130203247 - acc:  78.906250%\n","Training - running batch 27/176 of epoch 6/20 - loss: 0.9294882416725159 - acc:  68.750000%\n","Training - running batch 28/176 of epoch 6/20 - loss: 0.9336358904838562 - acc:  70.312500%\n","Training - running batch 29/176 of epoch 6/20 - loss: 0.7213130593299866 - acc:  85.156250%\n","Training - running batch 30/176 of epoch 6/20 - loss: 0.45415040850639343 - acc:  93.750000%\n","Training - running batch 31/176 of epoch 6/20 - loss: 0.9551650285720825 - acc:  68.750000%\n","Training - running batch 32/176 of epoch 6/20 - loss: 0.479532390832901 - acc:  85.937500%\n","Training - running batch 33/176 of epoch 6/20 - loss: 0.8346407413482666 - acc:  75.781250%\n","Training - running batch 34/176 of epoch 6/20 - loss: 0.3878123164176941 - acc:  96.093750%\n","Training - running batch 35/176 of epoch 6/20 - loss: 0.581939160823822 - acc:  89.062500%\n","Training - running batch 36/176 of epoch 6/20 - loss: 1.0001320838928223 - acc:  65.625000%\n","Training - running batch 37/176 of epoch 6/20 - loss: 0.5837372541427612 - acc:  89.062500%\n","Training - running batch 38/176 of epoch 6/20 - loss: 0.5212906002998352 - acc:  89.843750%\n","Training - running batch 39/176 of epoch 6/20 - loss: 0.5226704478263855 - acc:  89.843750%\n","Training - running batch 40/176 of epoch 6/20 - loss: 0.4532615542411804 - acc:  90.625000%\n","Training - running batch 41/176 of epoch 6/20 - loss: 0.508429765701294 - acc:  92.968750%\n","Training - running batch 42/176 of epoch 6/20 - loss: 0.6168888211250305 - acc:  86.718750%\n","Training - running batch 43/176 of epoch 6/20 - loss: 0.7683600187301636 - acc:  78.125000%\n","Training - running batch 44/176 of epoch 6/20 - loss: 0.4351702630519867 - acc:  94.531250%\n","Training - running batch 45/176 of epoch 6/20 - loss: 0.52353835105896 - acc:  90.625000%\n","Training - running batch 46/176 of epoch 6/20 - loss: 0.4667237102985382 - acc:  93.750000%\n","Training - running batch 47/176 of epoch 6/20 - loss: 1.0819483995437622 - acc:  64.062500%\n","Training - running batch 48/176 of epoch 6/20 - loss: 0.9439018368721008 - acc:  73.437500%\n","Training - running batch 49/176 of epoch 6/20 - loss: 0.4852467477321625 - acc:  87.500000%\n","Training - running batch 50/176 of epoch 6/20 - loss: 0.8132794499397278 - acc:  78.125000%\n","Training - running batch 51/176 of epoch 6/20 - loss: 0.44023293256759644 - acc:  92.187500%\n","Training - running batch 52/176 of epoch 6/20 - loss: 0.872105598449707 - acc:  78.125000%\n","Training - running batch 53/176 of epoch 6/20 - loss: 0.9184496402740479 - acc:  71.093750%\n","Training - running batch 54/176 of epoch 6/20 - loss: 0.8151258826255798 - acc:  82.031250%\n","Training - running batch 55/176 of epoch 6/20 - loss: 0.6135438084602356 - acc:  80.468750%\n","Training - running batch 56/176 of epoch 6/20 - loss: 0.8886088132858276 - acc:  69.531250%\n","Training - running batch 57/176 of epoch 6/20 - loss: 0.6112245321273804 - acc:  84.375000%\n","Training - running batch 58/176 of epoch 6/20 - loss: 0.412928968667984 - acc:  89.843750%\n","Training - running batch 59/176 of epoch 6/20 - loss: 0.9598484635353088 - acc:  68.750000%\n","Training - running batch 60/176 of epoch 6/20 - loss: 0.8946303129196167 - acc:  75.781250%\n","Training - running batch 61/176 of epoch 6/20 - loss: 0.8703781366348267 - acc:  71.093750%\n","Training - running batch 62/176 of epoch 6/20 - loss: 0.9791021943092346 - acc:  71.093750%\n","Training - running batch 63/176 of epoch 6/20 - loss: 0.49420177936553955 - acc:  90.625000%\n","Training - running batch 64/176 of epoch 6/20 - loss: 0.7774587273597717 - acc:  77.343750%\n","Training - running batch 65/176 of epoch 6/20 - loss: 0.8613690733909607 - acc:  73.437500%\n","Training - running batch 66/176 of epoch 6/20 - loss: 0.5817669034004211 - acc:  92.968750%\n","Training - running batch 67/176 of epoch 6/20 - loss: 1.023816704750061 - acc:  67.968750%\n","Training - running batch 68/176 of epoch 6/20 - loss: 0.46955668926239014 - acc:  92.968750%\n","Training - running batch 69/176 of epoch 6/20 - loss: 0.9575365781784058 - acc:  68.750000%\n","Training - running batch 70/176 of epoch 6/20 - loss: 0.4338643550872803 - acc:  92.187500%\n","Training - running batch 71/176 of epoch 6/20 - loss: 1.0013231039047241 - acc:  70.312500%\n","Training - running batch 72/176 of epoch 6/20 - loss: 0.3511384427547455 - acc:  92.968750%\n","Training - running batch 73/176 of epoch 6/20 - loss: 0.4704216718673706 - acc:  92.187500%\n","Training - running batch 74/176 of epoch 6/20 - loss: 0.4095412492752075 - acc:  93.750000%\n","Training - running batch 75/176 of epoch 6/20 - loss: 0.9718961119651794 - acc:  71.093750%\n","Training - running batch 76/176 of epoch 6/20 - loss: 0.8547936081886292 - acc:  71.875000%\n","Training - running batch 77/176 of epoch 6/20 - loss: 0.4478052258491516 - acc:  92.187500%\n","Training - running batch 78/176 of epoch 6/20 - loss: 0.5164555311203003 - acc:  89.843750%\n","Training - running batch 79/176 of epoch 6/20 - loss: 1.2699819803237915 - acc:  61.718750%\n","Training - running batch 80/176 of epoch 6/20 - loss: 0.9448205232620239 - acc:  67.968750%\n","Training - running batch 81/176 of epoch 6/20 - loss: 0.8568012714385986 - acc:  75.000000%\n","Training - running batch 82/176 of epoch 6/20 - loss: 0.45976272225379944 - acc:  89.843750%\n","Training - running batch 83/176 of epoch 6/20 - loss: 0.5796364545822144 - acc:  91.406250%\n","Training - running batch 84/176 of epoch 6/20 - loss: 0.5952472686767578 - acc:  83.593750%\n","Training - running batch 85/176 of epoch 6/20 - loss: 0.9870077967643738 - acc:  67.968750%\n","Training - running batch 86/176 of epoch 6/20 - loss: 0.7962493896484375 - acc:  83.593750%\n","Training - running batch 87/176 of epoch 6/20 - loss: 0.5131226778030396 - acc:  89.843750%\n","Training - running batch 88/176 of epoch 6/20 - loss: 0.7487069368362427 - acc:  77.343750%\n","Training - running batch 89/176 of epoch 6/20 - loss: 0.4302004873752594 - acc:  91.406250%\n","Training - running batch 90/176 of epoch 6/20 - loss: 0.7124838829040527 - acc:  83.593750%\n","Training - running batch 91/176 of epoch 6/20 - loss: 0.5354723334312439 - acc:  87.500000%\n","Training - running batch 92/176 of epoch 6/20 - loss: 0.562843382358551 - acc:  87.500000%\n","Training - running batch 93/176 of epoch 6/20 - loss: 0.7794303894042969 - acc:  73.437500%\n","Training - running batch 94/176 of epoch 6/20 - loss: 0.5197308659553528 - acc:  91.406250%\n","Training - running batch 95/176 of epoch 6/20 - loss: 0.6272472143173218 - acc:  86.718750%\n","Training - running batch 96/176 of epoch 6/20 - loss: 0.974216878414154 - acc:  66.406250%\n","Training - running batch 97/176 of epoch 6/20 - loss: 0.8886117935180664 - acc:  75.000000%\n","Training - running batch 98/176 of epoch 6/20 - loss: 0.4208783507347107 - acc:  94.531250%\n","Training - running batch 99/176 of epoch 6/20 - loss: 1.085312008857727 - acc:  67.968750%\n","Training - running batch 100/176 of epoch 6/20 - loss: 0.6290081143379211 - acc:  81.250000%\n","Training - running batch 101/176 of epoch 6/20 - loss: 0.6189466714859009 - acc:  89.843750%\n","Training - running batch 102/176 of epoch 6/20 - loss: 1.0266538858413696 - acc:  69.531250%\n","Training - running batch 103/176 of epoch 6/20 - loss: 0.44975775480270386 - acc:  85.156250%\n","Training - running batch 104/176 of epoch 6/20 - loss: 1.0775738954544067 - acc:  67.187500%\n","Training - running batch 105/176 of epoch 6/20 - loss: 0.8566775918006897 - acc:  77.343750%\n","Training - running batch 106/176 of epoch 6/20 - loss: 0.627668023109436 - acc:  88.281250%\n","Training - running batch 107/176 of epoch 6/20 - loss: 0.9674888849258423 - acc:  67.968750%\n","Training - running batch 108/176 of epoch 6/20 - loss: 0.9513559937477112 - acc:  64.843750%\n","Training - running batch 109/176 of epoch 6/20 - loss: 0.6000992655754089 - acc:  85.937500%\n","Training - running batch 110/176 of epoch 6/20 - loss: 0.8433566689491272 - acc:  75.000000%\n","Training - running batch 111/176 of epoch 6/20 - loss: 0.6643028855323792 - acc:  82.812500%\n","Training - running batch 112/176 of epoch 6/20 - loss: 0.6434887647628784 - acc:  82.812500%\n","Training - running batch 113/176 of epoch 6/20 - loss: 0.9567714333534241 - acc:  75.000000%\n","Training - running batch 114/176 of epoch 6/20 - loss: 0.9821474552154541 - acc:  69.531250%\n","Training - running batch 115/176 of epoch 6/20 - loss: 0.9220873117446899 - acc:  78.125000%\n","Training - running batch 116/176 of epoch 6/20 - loss: 0.7197619676589966 - acc:  83.593750%\n","Training - running batch 117/176 of epoch 6/20 - loss: 1.0599991083145142 - acc:  71.093750%\n","Training - running batch 118/176 of epoch 6/20 - loss: 0.49252912402153015 - acc:  90.625000%\n","Training - running batch 119/176 of epoch 6/20 - loss: 0.8439821004867554 - acc:  72.656250%\n","Training - running batch 120/176 of epoch 6/20 - loss: 0.5748927593231201 - acc:  85.937500%\n","Training - running batch 121/176 of epoch 6/20 - loss: 0.5764217972755432 - acc:  85.937500%\n","Training - running batch 122/176 of epoch 6/20 - loss: 1.1046732664108276 - acc:  70.312500%\n","Training - running batch 123/176 of epoch 6/20 - loss: 0.554560661315918 - acc:  85.156250%\n","Training - running batch 124/176 of epoch 6/20 - loss: 0.8665988445281982 - acc:  71.875000%\n","Training - running batch 125/176 of epoch 6/20 - loss: 0.9111810922622681 - acc:  70.312500%\n","Training - running batch 126/176 of epoch 6/20 - loss: 0.7796654105186462 - acc:  76.562500%\n","Training - running batch 127/176 of epoch 6/20 - loss: 0.6998295783996582 - acc:  79.687500%\n","Training - running batch 128/176 of epoch 6/20 - loss: 0.5656815767288208 - acc:  87.500000%\n","Training - running batch 129/176 of epoch 6/20 - loss: 0.6770922541618347 - acc:  87.500000%\n","Training - running batch 130/176 of epoch 6/20 - loss: 0.84674471616745 - acc:  75.000000%\n","Training - running batch 131/176 of epoch 6/20 - loss: 0.960569441318512 - acc:  67.968750%\n","Training - running batch 132/176 of epoch 6/20 - loss: 0.49158692359924316 - acc:  89.843750%\n","Training - running batch 133/176 of epoch 6/20 - loss: 1.007228970527649 - acc:  67.968750%\n","Training - running batch 134/176 of epoch 6/20 - loss: 0.9613946080207825 - acc:  69.531250%\n","Training - running batch 135/176 of epoch 6/20 - loss: 0.9326379299163818 - acc:  68.750000%\n","Training - running batch 136/176 of epoch 6/20 - loss: 0.9659646153450012 - acc:  67.187500%\n","Training - running batch 137/176 of epoch 6/20 - loss: 0.6301976442337036 - acc:  88.281250%\n","Training - running batch 138/176 of epoch 6/20 - loss: 0.9292799234390259 - acc:  69.531250%\n","Training - running batch 139/176 of epoch 6/20 - loss: 1.102652668952942 - acc:  65.625000%\n","Training - running batch 140/176 of epoch 6/20 - loss: 0.8929927945137024 - acc:  66.406250%\n","Training - running batch 141/176 of epoch 6/20 - loss: 0.4643402099609375 - acc:  90.625000%\n","Training - running batch 142/176 of epoch 6/20 - loss: 0.5996201038360596 - acc:  87.500000%\n","Training - running batch 143/176 of epoch 6/20 - loss: 0.7009350061416626 - acc:  84.375000%\n","Training - running batch 144/176 of epoch 6/20 - loss: 0.47454267740249634 - acc:  87.500000%\n","Training - running batch 145/176 of epoch 6/20 - loss: 0.5418576598167419 - acc:  83.593750%\n","Training - running batch 146/176 of epoch 6/20 - loss: 0.5451971888542175 - acc:  89.062500%\n","Training - running batch 147/176 of epoch 6/20 - loss: 1.0050323009490967 - acc:  72.656250%\n","Training - running batch 148/176 of epoch 6/20 - loss: 0.9009277820587158 - acc:  73.437500%\n","Training - running batch 149/176 of epoch 6/20 - loss: 0.5253995656967163 - acc:  85.156250%\n","Training - running batch 150/176 of epoch 6/20 - loss: 1.0572054386138916 - acc:  64.062500%\n","Training - running batch 151/176 of epoch 6/20 - loss: 0.9827229380607605 - acc:  68.750000%\n","Training - running batch 152/176 of epoch 6/20 - loss: 0.5163742899894714 - acc:  87.500000%\n","Training - running batch 153/176 of epoch 6/20 - loss: 0.6814221739768982 - acc:  82.812500%\n","Training - running batch 154/176 of epoch 6/20 - loss: 1.0283877849578857 - acc:  67.968750%\n","Training - running batch 155/176 of epoch 6/20 - loss: 1.1238654851913452 - acc:  65.625000%\n","Training - running batch 156/176 of epoch 6/20 - loss: 0.7897989749908447 - acc:  80.468750%\n","Training - running batch 157/176 of epoch 6/20 - loss: 0.6726298332214355 - acc:  82.812500%\n","Training - running batch 158/176 of epoch 6/20 - loss: 0.6034196615219116 - acc:  83.593750%\n","Training - running batch 159/176 of epoch 6/20 - loss: 0.7764968872070312 - acc:  75.000000%\n","Training - running batch 160/176 of epoch 6/20 - loss: 0.9975163340568542 - acc:  70.312500%\n","Training - running batch 161/176 of epoch 6/20 - loss: 0.6729443669319153 - acc:  80.468750%\n","Training - running batch 162/176 of epoch 6/20 - loss: 1.0833775997161865 - acc:  65.625000%\n","Training - running batch 163/176 of epoch 6/20 - loss: 0.8100760579109192 - acc:  72.656250%\n","Training - running batch 164/176 of epoch 6/20 - loss: 0.5464850068092346 - acc:  88.281250%\n","Training - running batch 165/176 of epoch 6/20 - loss: 0.8123383522033691 - acc:  77.343750%\n","Training - running batch 166/176 of epoch 6/20 - loss: 0.8411940336227417 - acc:  72.656250%\n","Training - running batch 167/176 of epoch 6/20 - loss: 0.5704612731933594 - acc:  88.281250%\n","Training - running batch 168/176 of epoch 6/20 - loss: 0.9001346826553345 - acc:  71.875000%\n","Training - running batch 169/176 of epoch 6/20 - loss: 0.6125441789627075 - acc:  85.937500%\n","Training - running batch 170/176 of epoch 6/20 - loss: 0.5420140027999878 - acc:  88.281250%\n","Training - running batch 171/176 of epoch 6/20 - loss: 1.0784252882003784 - acc:  67.968750%\n","Training - running batch 172/176 of epoch 6/20 - loss: 0.8753513097763062 - acc:  71.875000%\n","Training - running batch 173/176 of epoch 6/20 - loss: 0.9730932116508484 - acc:  69.531250%\n","Training - running batch 174/176 of epoch 6/20 - loss: 0.8435788750648499 - acc:  73.437500%\n","Training - running batch 175/176 of epoch 6/20 - loss: 0.5974058508872986 - acc:  91.935484%\n","Testing - acc:  0.658481%\n","Training - running batch 0/176 of epoch 7/20 - loss: 0.46923261880874634 - acc:  96.875000%\n","Training - running batch 1/176 of epoch 7/20 - loss: 0.7919068336486816 - acc:  75.781250%\n","Training - running batch 2/176 of epoch 7/20 - loss: 0.8536404371261597 - acc:  74.218750%\n","Training - running batch 3/176 of epoch 7/20 - loss: 0.40534672141075134 - acc:  93.750000%\n","Training - running batch 4/176 of epoch 7/20 - loss: 0.32133564352989197 - acc:  97.656250%\n","Training - running batch 5/176 of epoch 7/20 - loss: 0.8591012954711914 - acc:  72.656250%\n","Training - running batch 6/176 of epoch 7/20 - loss: 0.43470677733421326 - acc:  92.187500%\n","Training - running batch 7/176 of epoch 7/20 - loss: 0.6146857142448425 - acc:  85.937500%\n","Training - running batch 8/176 of epoch 7/20 - loss: 0.537732720375061 - acc:  88.281250%\n","Training - running batch 9/176 of epoch 7/20 - loss: 0.530900239944458 - acc:  90.625000%\n","Training - running batch 10/176 of epoch 7/20 - loss: 0.4146677553653717 - acc:  93.750000%\n","Training - running batch 11/176 of epoch 7/20 - loss: 0.4603826701641083 - acc:  90.625000%\n","Training - running batch 12/176 of epoch 7/20 - loss: 0.7141643166542053 - acc:  75.781250%\n","Training - running batch 13/176 of epoch 7/20 - loss: 0.5233627557754517 - acc:  87.500000%\n","Training - running batch 14/176 of epoch 7/20 - loss: 0.656395673751831 - acc:  80.468750%\n","Training - running batch 15/176 of epoch 7/20 - loss: 0.8524948358535767 - acc:  75.781250%\n","Training - running batch 16/176 of epoch 7/20 - loss: 0.35072794556617737 - acc:  93.750000%\n","Training - running batch 17/176 of epoch 7/20 - loss: 0.9306824803352356 - acc:  75.781250%\n","Training - running batch 18/176 of epoch 7/20 - loss: 0.7526573538780212 - acc:  76.562500%\n","Training - running batch 19/176 of epoch 7/20 - loss: 0.7574335932731628 - acc:  75.781250%\n","Training - running batch 20/176 of epoch 7/20 - loss: 0.719170331954956 - acc:  75.000000%\n","Training - running batch 21/176 of epoch 7/20 - loss: 0.5228825211524963 - acc:  89.843750%\n","Training - running batch 22/176 of epoch 7/20 - loss: 0.9517346024513245 - acc:  70.312500%\n","Training - running batch 23/176 of epoch 7/20 - loss: 0.7704867124557495 - acc:  77.343750%\n","Training - running batch 24/176 of epoch 7/20 - loss: 0.42999687790870667 - acc:  91.406250%\n","Training - running batch 25/176 of epoch 7/20 - loss: 0.8810523152351379 - acc:  73.437500%\n","Training - running batch 26/176 of epoch 7/20 - loss: 0.4995018541812897 - acc:  86.718750%\n","Training - running batch 27/176 of epoch 7/20 - loss: 0.9008696675300598 - acc:  73.437500%\n","Training - running batch 28/176 of epoch 7/20 - loss: 0.8440650105476379 - acc:  77.343750%\n","Training - running batch 29/176 of epoch 7/20 - loss: 0.9692240357398987 - acc:  71.093750%\n","Training - running batch 30/176 of epoch 7/20 - loss: 0.8550318479537964 - acc:  72.656250%\n","Training - running batch 31/176 of epoch 7/20 - loss: 0.7162476181983948 - acc:  77.343750%\n","Training - running batch 32/176 of epoch 7/20 - loss: 0.5408245325088501 - acc:  91.406250%\n","Training - running batch 33/176 of epoch 7/20 - loss: 0.35043707489967346 - acc:  92.968750%\n","Training - running batch 34/176 of epoch 7/20 - loss: 0.43993252515792847 - acc:  89.062500%\n","Training - running batch 35/176 of epoch 7/20 - loss: 0.9513038992881775 - acc:  71.875000%\n","Training - running batch 36/176 of epoch 7/20 - loss: 0.8193756937980652 - acc:  73.437500%\n","Training - running batch 37/176 of epoch 7/20 - loss: 0.9840035438537598 - acc:  64.843750%\n","Training - running batch 38/176 of epoch 7/20 - loss: 0.3157780170440674 - acc:  93.750000%\n","Training - running batch 39/176 of epoch 7/20 - loss: 0.7355514764785767 - acc:  77.343750%\n","Training - running batch 40/176 of epoch 7/20 - loss: 0.4301818013191223 - acc:  92.968750%\n","Training - running batch 41/176 of epoch 7/20 - loss: 0.8650230765342712 - acc:  69.531250%\n","Training - running batch 42/176 of epoch 7/20 - loss: 0.6231920123100281 - acc:  85.156250%\n","Training - running batch 43/176 of epoch 7/20 - loss: 0.44832587242126465 - acc:  89.062500%\n","Training - running batch 44/176 of epoch 7/20 - loss: 0.3438491225242615 - acc:  94.531250%\n","Training - running batch 45/176 of epoch 7/20 - loss: 0.40151283144950867 - acc:  92.968750%\n","Training - running batch 46/176 of epoch 7/20 - loss: 0.4754599332809448 - acc:  91.406250%\n","Training - running batch 47/176 of epoch 7/20 - loss: 0.592778205871582 - acc:  90.625000%\n","Training - running batch 48/176 of epoch 7/20 - loss: 0.8391770124435425 - acc:  73.437500%\n","Training - running batch 49/176 of epoch 7/20 - loss: 0.618150532245636 - acc:  88.281250%\n","Training - running batch 50/176 of epoch 7/20 - loss: 0.43097129464149475 - acc:  88.281250%\n","Training - running batch 51/176 of epoch 7/20 - loss: 0.3982692360877991 - acc:  92.968750%\n","Training - running batch 52/176 of epoch 7/20 - loss: 0.390621542930603 - acc:  93.750000%\n","Training - running batch 53/176 of epoch 7/20 - loss: 0.8658508062362671 - acc:  71.093750%\n","Training - running batch 54/176 of epoch 7/20 - loss: 0.5348402857780457 - acc:  82.812500%\n","Training - running batch 55/176 of epoch 7/20 - loss: 0.7517960071563721 - acc:  69.531250%\n","Training - running batch 56/176 of epoch 7/20 - loss: 0.8461769819259644 - acc:  78.125000%\n","Training - running batch 57/176 of epoch 7/20 - loss: 0.9221159815788269 - acc:  68.750000%\n","Training - running batch 58/176 of epoch 7/20 - loss: 0.7847751975059509 - acc:  75.000000%\n","Training - running batch 59/176 of epoch 7/20 - loss: 0.35484498739242554 - acc:  95.312500%\n","Training - running batch 60/176 of epoch 7/20 - loss: 0.7071244120597839 - acc:  79.687500%\n","Training - running batch 61/176 of epoch 7/20 - loss: 0.4267658293247223 - acc:  92.968750%\n","Training - running batch 62/176 of epoch 7/20 - loss: 0.7985091209411621 - acc:  78.125000%\n","Training - running batch 63/176 of epoch 7/20 - loss: 0.5134844779968262 - acc:  85.937500%\n","Training - running batch 64/176 of epoch 7/20 - loss: 0.7379251718521118 - acc:  77.343750%\n","Training - running batch 65/176 of epoch 7/20 - loss: 0.7108071446418762 - acc:  76.562500%\n","Training - running batch 66/176 of epoch 7/20 - loss: 0.738456130027771 - acc:  78.125000%\n","Training - running batch 67/176 of epoch 7/20 - loss: 0.46783044934272766 - acc:  91.406250%\n","Training - running batch 68/176 of epoch 7/20 - loss: 0.35779961943626404 - acc:  96.875000%\n","Training - running batch 69/176 of epoch 7/20 - loss: 0.7756280303001404 - acc:  80.468750%\n","Training - running batch 70/176 of epoch 7/20 - loss: 0.3495060205459595 - acc:  96.875000%\n","Training - running batch 71/176 of epoch 7/20 - loss: 0.6975554823875427 - acc:  78.906250%\n","Training - running batch 72/176 of epoch 7/20 - loss: 0.5487539172172546 - acc:  94.531250%\n","Training - running batch 73/176 of epoch 7/20 - loss: 0.4716743528842926 - acc:  90.625000%\n","Training - running batch 74/176 of epoch 7/20 - loss: 0.43766579031944275 - acc:  96.093750%\n","Training - running batch 75/176 of epoch 7/20 - loss: 0.8418949246406555 - acc:  74.218750%\n","Training - running batch 76/176 of epoch 7/20 - loss: 0.4254474639892578 - acc:  93.750000%\n","Training - running batch 77/176 of epoch 7/20 - loss: 0.35721683502197266 - acc:  95.312500%\n","Training - running batch 78/176 of epoch 7/20 - loss: 0.42581164836883545 - acc:  90.625000%\n","Training - running batch 79/176 of epoch 7/20 - loss: 0.7499885559082031 - acc:  75.781250%\n","Training - running batch 80/176 of epoch 7/20 - loss: 0.38556572794914246 - acc:  92.968750%\n","Training - running batch 81/176 of epoch 7/20 - loss: 0.669009268283844 - acc:  79.687500%\n","Training - running batch 82/176 of epoch 7/20 - loss: 0.4625006914138794 - acc:  93.750000%\n","Training - running batch 83/176 of epoch 7/20 - loss: 0.38033849000930786 - acc:  93.750000%\n","Training - running batch 84/176 of epoch 7/20 - loss: 0.8121071457862854 - acc:  73.437500%\n","Training - running batch 85/176 of epoch 7/20 - loss: 0.4344991445541382 - acc:  94.531250%\n","Training - running batch 86/176 of epoch 7/20 - loss: 0.8157352209091187 - acc:  74.218750%\n","Training - running batch 87/176 of epoch 7/20 - loss: 0.49958378076553345 - acc:  88.281250%\n","Training - running batch 88/176 of epoch 7/20 - loss: 0.6847831606864929 - acc:  80.468750%\n","Training - running batch 89/176 of epoch 7/20 - loss: 0.5666193962097168 - acc:  85.156250%\n","Training - running batch 90/176 of epoch 7/20 - loss: 0.7998896837234497 - acc:  71.875000%\n","Training - running batch 91/176 of epoch 7/20 - loss: 0.4976137578487396 - acc:  87.500000%\n","Training - running batch 92/176 of epoch 7/20 - loss: 0.6650311946868896 - acc:  81.250000%\n","Training - running batch 93/176 of epoch 7/20 - loss: 0.7857658863067627 - acc:  74.218750%\n","Training - running batch 94/176 of epoch 7/20 - loss: 0.8536333441734314 - acc:  75.781250%\n","Training - running batch 95/176 of epoch 7/20 - loss: 0.4571981132030487 - acc:  95.312500%\n","Training - running batch 96/176 of epoch 7/20 - loss: 0.735512912273407 - acc:  79.687500%\n","Training - running batch 97/176 of epoch 7/20 - loss: 0.5857332348823547 - acc:  87.500000%\n","Training - running batch 98/176 of epoch 7/20 - loss: 0.41251251101493835 - acc:  95.312500%\n","Training - running batch 99/176 of epoch 7/20 - loss: 0.42182525992393494 - acc:  92.968750%\n","Training - running batch 100/176 of epoch 7/20 - loss: 0.5877482295036316 - acc:  82.031250%\n","Training - running batch 101/176 of epoch 7/20 - loss: 0.8522456288337708 - acc:  69.531250%\n","Training - running batch 102/176 of epoch 7/20 - loss: 0.4314051568508148 - acc:  85.937500%\n","Training - running batch 103/176 of epoch 7/20 - loss: 0.5138092637062073 - acc:  89.843750%\n","Training - running batch 104/176 of epoch 7/20 - loss: 0.3548656404018402 - acc:  93.750000%\n","Training - running batch 105/176 of epoch 7/20 - loss: 0.4358192980289459 - acc:  89.062500%\n","Training - running batch 106/176 of epoch 7/20 - loss: 0.6862654685974121 - acc:  78.906250%\n","Training - running batch 107/176 of epoch 7/20 - loss: 0.44564515352249146 - acc:  92.187500%\n","Training - running batch 108/176 of epoch 7/20 - loss: 0.5457069873809814 - acc:  83.593750%\n","Training - running batch 109/176 of epoch 7/20 - loss: 0.4503514766693115 - acc:  92.968750%\n","Training - running batch 110/176 of epoch 7/20 - loss: 0.8869094848632812 - acc:  71.875000%\n","Training - running batch 111/176 of epoch 7/20 - loss: 0.4411987066268921 - acc:  88.281250%\n","Training - running batch 112/176 of epoch 7/20 - loss: 0.4086671769618988 - acc:  93.750000%\n","Training - running batch 113/176 of epoch 7/20 - loss: 0.48175883293151855 - acc:  94.531250%\n","Training - running batch 114/176 of epoch 7/20 - loss: 0.5493015050888062 - acc:  83.593750%\n","Training - running batch 115/176 of epoch 7/20 - loss: 0.8760703206062317 - acc:  71.875000%\n","Training - running batch 116/176 of epoch 7/20 - loss: 0.392251580953598 - acc:  95.312500%\n","Training - running batch 117/176 of epoch 7/20 - loss: 0.833889365196228 - acc:  72.656250%\n","Training - running batch 118/176 of epoch 7/20 - loss: 0.8183304071426392 - acc:  71.875000%\n","Training - running batch 119/176 of epoch 7/20 - loss: 0.8256489634513855 - acc:  73.437500%\n","Training - running batch 120/176 of epoch 7/20 - loss: 0.5159571170806885 - acc:  85.937500%\n","Training - running batch 121/176 of epoch 7/20 - loss: 0.874530553817749 - acc:  72.656250%\n","Training - running batch 122/176 of epoch 7/20 - loss: 0.44356903433799744 - acc:  90.625000%\n","Training - running batch 123/176 of epoch 7/20 - loss: 0.49280884861946106 - acc:  89.062500%\n","Training - running batch 124/176 of epoch 7/20 - loss: 0.7181875705718994 - acc:  74.218750%\n","Training - running batch 125/176 of epoch 7/20 - loss: 0.47719988226890564 - acc:  92.187500%\n","Training - running batch 126/176 of epoch 7/20 - loss: 0.8341689705848694 - acc:  72.656250%\n","Training - running batch 127/176 of epoch 7/20 - loss: 0.841672420501709 - acc:  78.125000%\n","Training - running batch 128/176 of epoch 7/20 - loss: 0.31827425956726074 - acc:  95.312500%\n","Training - running batch 129/176 of epoch 7/20 - loss: 0.780240535736084 - acc:  75.781250%\n","Training - running batch 130/176 of epoch 7/20 - loss: 0.3835758864879608 - acc:  88.281250%\n","Training - running batch 131/176 of epoch 7/20 - loss: 0.49986347556114197 - acc:  87.500000%\n","Training - running batch 132/176 of epoch 7/20 - loss: 1.0052099227905273 - acc:  66.406250%\n","Training - running batch 133/176 of epoch 7/20 - loss: 1.0191996097564697 - acc:  70.312500%\n","Training - running batch 134/176 of epoch 7/20 - loss: 0.5264652967453003 - acc:  89.843750%\n","Training - running batch 135/176 of epoch 7/20 - loss: 0.45620667934417725 - acc:  90.625000%\n","Training - running batch 136/176 of epoch 7/20 - loss: 0.8729031682014465 - acc:  73.437500%\n","Training - running batch 137/176 of epoch 7/20 - loss: 0.8987450003623962 - acc:  74.218750%\n","Training - running batch 138/176 of epoch 7/20 - loss: 0.4002544581890106 - acc:  93.750000%\n","Training - running batch 139/176 of epoch 7/20 - loss: 0.8739045262336731 - acc:  72.656250%\n","Training - running batch 140/176 of epoch 7/20 - loss: 0.7783267498016357 - acc:  78.906250%\n","Training - running batch 141/176 of epoch 7/20 - loss: 0.903619647026062 - acc:  73.437500%\n","Training - running batch 142/176 of epoch 7/20 - loss: 0.5063540935516357 - acc:  85.156250%\n","Training - running batch 143/176 of epoch 7/20 - loss: 0.7139965295791626 - acc:  77.343750%\n","Training - running batch 144/176 of epoch 7/20 - loss: 0.8660107851028442 - acc:  75.000000%\n","Training - running batch 145/176 of epoch 7/20 - loss: 0.7979141473770142 - acc:  75.781250%\n","Training - running batch 146/176 of epoch 7/20 - loss: 0.5152146220207214 - acc:  87.500000%\n","Training - running batch 147/176 of epoch 7/20 - loss: 0.5578193068504333 - acc:  82.031250%\n","Training - running batch 148/176 of epoch 7/20 - loss: 0.5872495174407959 - acc:  82.812500%\n","Training - running batch 149/176 of epoch 7/20 - loss: 1.0387089252471924 - acc:  68.750000%\n","Training - running batch 150/176 of epoch 7/20 - loss: 0.776084840297699 - acc:  74.218750%\n","Training - running batch 151/176 of epoch 7/20 - loss: 0.4289170801639557 - acc:  90.625000%\n","Training - running batch 152/176 of epoch 7/20 - loss: 0.5128042101860046 - acc:  85.937500%\n","Training - running batch 153/176 of epoch 7/20 - loss: 0.7744476199150085 - acc:  75.781250%\n","Training - running batch 154/176 of epoch 7/20 - loss: 0.5140905976295471 - acc:  90.625000%\n","Training - running batch 155/176 of epoch 7/20 - loss: 0.7870726585388184 - acc:  72.656250%\n","Training - running batch 156/176 of epoch 7/20 - loss: 0.41518956422805786 - acc:  92.187500%\n","Training - running batch 157/176 of epoch 7/20 - loss: 0.5439649820327759 - acc:  89.843750%\n","Training - running batch 158/176 of epoch 7/20 - loss: 0.8781890869140625 - acc:  71.093750%\n","Training - running batch 159/176 of epoch 7/20 - loss: 0.9160837531089783 - acc:  80.468750%\n","Training - running batch 160/176 of epoch 7/20 - loss: 0.7833999991416931 - acc:  80.468750%\n","Training - running batch 161/176 of epoch 7/20 - loss: 0.7410953640937805 - acc:  73.437500%\n","Training - running batch 162/176 of epoch 7/20 - loss: 0.6218099594116211 - acc:  84.375000%\n","Training - running batch 163/176 of epoch 7/20 - loss: 0.6426330208778381 - acc:  88.281250%\n","Training - running batch 164/176 of epoch 7/20 - loss: 0.8401888012886047 - acc:  70.312500%\n","Training - running batch 165/176 of epoch 7/20 - loss: 0.9503130316734314 - acc:  69.531250%\n","Training - running batch 166/176 of epoch 7/20 - loss: 0.47078385949134827 - acc:  88.281250%\n","Training - running batch 167/176 of epoch 7/20 - loss: 0.8895168304443359 - acc:  73.437500%\n","Training - running batch 168/176 of epoch 7/20 - loss: 0.5231361389160156 - acc:  90.625000%\n","Training - running batch 169/176 of epoch 7/20 - loss: 0.5880273580551147 - acc:  92.968750%\n","Training - running batch 170/176 of epoch 7/20 - loss: 0.5857373476028442 - acc:  84.375000%\n","Training - running batch 171/176 of epoch 7/20 - loss: 0.67718106508255 - acc:  85.156250%\n","Training - running batch 172/176 of epoch 7/20 - loss: 0.8394710421562195 - acc:  70.312500%\n","Training - running batch 173/176 of epoch 7/20 - loss: 0.7970360517501831 - acc:  78.125000%\n","Training - running batch 174/176 of epoch 7/20 - loss: 0.5471007227897644 - acc:  86.718750%\n","Training - running batch 175/176 of epoch 7/20 - loss: 0.6089573502540588 - acc:  80.645161%\n","Testing - acc:  0.672711%\n","Training - running batch 0/176 of epoch 8/20 - loss: 0.3746836185455322 - acc:  94.531250%\n","Training - running batch 1/176 of epoch 8/20 - loss: 0.6764001846313477 - acc:  78.125000%\n","Training - running batch 2/176 of epoch 8/20 - loss: 0.7673105001449585 - acc:  74.218750%\n","Training - running batch 3/176 of epoch 8/20 - loss: 0.7582939863204956 - acc:  75.781250%\n","Training - running batch 4/176 of epoch 8/20 - loss: 0.41371941566467285 - acc:  90.625000%\n","Training - running batch 5/176 of epoch 8/20 - loss: 0.7982435822486877 - acc:  74.218750%\n","Training - running batch 6/176 of epoch 8/20 - loss: 0.7404931783676147 - acc:  76.562500%\n","Training - running batch 7/176 of epoch 8/20 - loss: 0.3966881036758423 - acc:  94.531250%\n","Training - running batch 8/176 of epoch 8/20 - loss: 0.40795063972473145 - acc:  92.968750%\n","Training - running batch 9/176 of epoch 8/20 - loss: 0.7350324988365173 - acc:  86.718750%\n","Training - running batch 10/176 of epoch 8/20 - loss: 0.7342631816864014 - acc:  75.781250%\n","Training - running batch 11/176 of epoch 8/20 - loss: 0.3549087345600128 - acc:  92.968750%\n","Training - running batch 12/176 of epoch 8/20 - loss: 0.7873134016990662 - acc:  71.875000%\n","Training - running batch 13/176 of epoch 8/20 - loss: 0.8372783660888672 - acc:  71.875000%\n","Training - running batch 14/176 of epoch 8/20 - loss: 0.8749898076057434 - acc:  71.875000%\n","Training - running batch 15/176 of epoch 8/20 - loss: 0.9092332124710083 - acc:  75.781250%\n","Training - running batch 16/176 of epoch 8/20 - loss: 0.6848058700561523 - acc:  84.375000%\n","Training - running batch 17/176 of epoch 8/20 - loss: 0.7218298316001892 - acc:  81.250000%\n","Training - running batch 18/176 of epoch 8/20 - loss: 0.6767117977142334 - acc:  79.687500%\n","Training - running batch 19/176 of epoch 8/20 - loss: 0.8697836399078369 - acc:  65.625000%\n","Training - running batch 20/176 of epoch 8/20 - loss: 0.422626793384552 - acc:  93.750000%\n","Training - running batch 21/176 of epoch 8/20 - loss: 0.6299424171447754 - acc:  82.031250%\n","Training - running batch 22/176 of epoch 8/20 - loss: 0.4917694926261902 - acc:  89.843750%\n","Training - running batch 23/176 of epoch 8/20 - loss: 0.8006596565246582 - acc:  75.000000%\n","Training - running batch 24/176 of epoch 8/20 - loss: 0.4557560086250305 - acc:  88.281250%\n","Training - running batch 25/176 of epoch 8/20 - loss: 0.3600271940231323 - acc:  96.875000%\n","Training - running batch 26/176 of epoch 8/20 - loss: 0.7750441431999207 - acc:  78.125000%\n","Training - running batch 27/176 of epoch 8/20 - loss: 0.5835606455802917 - acc:  86.718750%\n","Training - running batch 28/176 of epoch 8/20 - loss: 0.3593243658542633 - acc:  92.968750%\n","Training - running batch 29/176 of epoch 8/20 - loss: 0.39881929755210876 - acc:  92.968750%\n","Training - running batch 30/176 of epoch 8/20 - loss: 0.7843369841575623 - acc:  71.875000%\n","Training - running batch 31/176 of epoch 8/20 - loss: 0.7620315551757812 - acc:  75.781250%\n","Training - running batch 32/176 of epoch 8/20 - loss: 0.7964314222335815 - acc:  74.218750%\n","Training - running batch 33/176 of epoch 8/20 - loss: 0.8371669054031372 - acc:  75.000000%\n","Training - running batch 34/176 of epoch 8/20 - loss: 0.4557567238807678 - acc:  90.625000%\n","Training - running batch 35/176 of epoch 8/20 - loss: 0.7051756978034973 - acc:  75.781250%\n","Training - running batch 36/176 of epoch 8/20 - loss: 0.3460772931575775 - acc:  95.312500%\n","Training - running batch 37/176 of epoch 8/20 - loss: 0.8673151135444641 - acc:  66.406250%\n","Training - running batch 38/176 of epoch 8/20 - loss: 0.4397152066230774 - acc:  92.187500%\n","Training - running batch 39/176 of epoch 8/20 - loss: 0.36854633688926697 - acc:  93.750000%\n","Training - running batch 40/176 of epoch 8/20 - loss: 0.538350522518158 - acc:  92.968750%\n","Training - running batch 41/176 of epoch 8/20 - loss: 0.68341463804245 - acc:  78.906250%\n","Training - running batch 42/176 of epoch 8/20 - loss: 0.5240209698677063 - acc:  91.406250%\n","Training - running batch 43/176 of epoch 8/20 - loss: 0.4343474507331848 - acc:  94.531250%\n","Training - running batch 44/176 of epoch 8/20 - loss: 0.5118926167488098 - acc:  85.937500%\n","Training - running batch 45/176 of epoch 8/20 - loss: 0.42104047536849976 - acc:  92.968750%\n","Training - running batch 46/176 of epoch 8/20 - loss: 0.5688207745552063 - acc:  85.937500%\n","Training - running batch 47/176 of epoch 8/20 - loss: 0.7774602770805359 - acc:  75.000000%\n","Training - running batch 48/176 of epoch 8/20 - loss: 0.7905530333518982 - acc:  75.000000%\n","Training - running batch 49/176 of epoch 8/20 - loss: 0.3162239193916321 - acc:  93.750000%\n","Training - running batch 50/176 of epoch 8/20 - loss: 0.5276626944541931 - acc:  85.156250%\n","Training - running batch 51/176 of epoch 8/20 - loss: 0.6016498804092407 - acc:  82.031250%\n","Training - running batch 52/176 of epoch 8/20 - loss: 0.695304274559021 - acc:  82.031250%\n","Training - running batch 53/176 of epoch 8/20 - loss: 0.4683065116405487 - acc:  92.187500%\n","Training - running batch 54/176 of epoch 8/20 - loss: 0.7849898934364319 - acc:  76.562500%\n","Training - running batch 55/176 of epoch 8/20 - loss: 0.7683166861534119 - acc:  78.906250%\n","Training - running batch 56/176 of epoch 8/20 - loss: 0.7265476584434509 - acc:  74.218750%\n","Training - running batch 57/176 of epoch 8/20 - loss: 0.44281378388404846 - acc:  89.843750%\n","Training - running batch 58/176 of epoch 8/20 - loss: 0.42699772119522095 - acc:  93.750000%\n","Training - running batch 59/176 of epoch 8/20 - loss: 0.7475516200065613 - acc:  78.906250%\n","Training - running batch 60/176 of epoch 8/20 - loss: 0.44078826904296875 - acc:  92.968750%\n","Training - running batch 61/176 of epoch 8/20 - loss: 0.6405530571937561 - acc:  78.125000%\n","Training - running batch 62/176 of epoch 8/20 - loss: 0.4027070105075836 - acc:  92.187500%\n","Training - running batch 63/176 of epoch 8/20 - loss: 0.3501749336719513 - acc:  92.968750%\n","Training - running batch 64/176 of epoch 8/20 - loss: 0.7007007002830505 - acc:  74.218750%\n","Training - running batch 65/176 of epoch 8/20 - loss: 0.3574051558971405 - acc:  95.312500%\n","Training - running batch 66/176 of epoch 8/20 - loss: 0.924630880355835 - acc:  71.093750%\n","Training - running batch 67/176 of epoch 8/20 - loss: 0.3290981650352478 - acc:  92.187500%\n","Training - running batch 68/176 of epoch 8/20 - loss: 0.6625064611434937 - acc:  78.125000%\n","Training - running batch 69/176 of epoch 8/20 - loss: 0.6744991540908813 - acc:  76.562500%\n","Training - running batch 70/176 of epoch 8/20 - loss: 0.4727272689342499 - acc:  86.718750%\n","Training - running batch 71/176 of epoch 8/20 - loss: 0.7333160638809204 - acc:  78.125000%\n","Training - running batch 72/176 of epoch 8/20 - loss: 0.7711108922958374 - acc:  76.562500%\n","Training - running batch 73/176 of epoch 8/20 - loss: 0.46952947974205017 - acc:  86.718750%\n","Training - running batch 74/176 of epoch 8/20 - loss: 0.43489301204681396 - acc:  89.843750%\n","Training - running batch 75/176 of epoch 8/20 - loss: 0.3662430942058563 - acc:  89.843750%\n","Training - running batch 76/176 of epoch 8/20 - loss: 0.7106195688247681 - acc:  80.468750%\n","Training - running batch 77/176 of epoch 8/20 - loss: 0.8503112196922302 - acc:  74.218750%\n","Training - running batch 78/176 of epoch 8/20 - loss: 0.30781516432762146 - acc:  94.531250%\n","Training - running batch 79/176 of epoch 8/20 - loss: 0.5352990627288818 - acc:  87.500000%\n","Training - running batch 80/176 of epoch 8/20 - loss: 0.5501415133476257 - acc:  82.812500%\n","Training - running batch 81/176 of epoch 8/20 - loss: 0.6750808954238892 - acc:  78.906250%\n","Training - running batch 82/176 of epoch 8/20 - loss: 0.8633598685264587 - acc:  77.343750%\n","Training - running batch 83/176 of epoch 8/20 - loss: 0.29241248965263367 - acc:  96.093750%\n","Training - running batch 84/176 of epoch 8/20 - loss: 0.3024543523788452 - acc:  96.093750%\n","Training - running batch 85/176 of epoch 8/20 - loss: 0.4378785789012909 - acc:  91.406250%\n","Training - running batch 86/176 of epoch 8/20 - loss: 0.8548402190208435 - acc:  72.656250%\n","Training - running batch 87/176 of epoch 8/20 - loss: 0.6338433027267456 - acc:  83.593750%\n","Training - running batch 88/176 of epoch 8/20 - loss: 0.4840680658817291 - acc:  89.843750%\n","Training - running batch 89/176 of epoch 8/20 - loss: 0.9221911430358887 - acc:  67.187500%\n","Training - running batch 90/176 of epoch 8/20 - loss: 0.7306966781616211 - acc:  75.781250%\n","Training - running batch 91/176 of epoch 8/20 - loss: 0.8560658693313599 - acc:  75.781250%\n","Training - running batch 92/176 of epoch 8/20 - loss: 0.3989299237728119 - acc:  92.187500%\n","Training - running batch 93/176 of epoch 8/20 - loss: 0.315432608127594 - acc:  96.093750%\n","Training - running batch 94/176 of epoch 8/20 - loss: 0.7198789715766907 - acc:  72.656250%\n","Training - running batch 95/176 of epoch 8/20 - loss: 0.8844549655914307 - acc:  75.000000%\n","Training - running batch 96/176 of epoch 8/20 - loss: 0.860958456993103 - acc:  73.437500%\n","Training - running batch 97/176 of epoch 8/20 - loss: 0.4650923013687134 - acc:  91.406250%\n","Training - running batch 98/176 of epoch 8/20 - loss: 0.3392583727836609 - acc:  92.187500%\n","Training - running batch 99/176 of epoch 8/20 - loss: 0.6518380045890808 - acc:  81.250000%\n","Training - running batch 100/176 of epoch 8/20 - loss: 0.7029145956039429 - acc:  78.125000%\n","Training - running batch 101/176 of epoch 8/20 - loss: 0.4067169427871704 - acc:  87.500000%\n","Training - running batch 102/176 of epoch 8/20 - loss: 0.36955875158309937 - acc:  92.968750%\n","Training - running batch 103/176 of epoch 8/20 - loss: 0.6819970607757568 - acc:  76.562500%\n","Training - running batch 104/176 of epoch 8/20 - loss: 0.4596381187438965 - acc:  91.406250%\n","Training - running batch 105/176 of epoch 8/20 - loss: 0.7559464573860168 - acc:  75.781250%\n","Training - running batch 106/176 of epoch 8/20 - loss: 0.3136886656284332 - acc:  94.531250%\n","Training - running batch 107/176 of epoch 8/20 - loss: 0.76401287317276 - acc:  75.000000%\n","Training - running batch 108/176 of epoch 8/20 - loss: 0.3461417257785797 - acc:  93.750000%\n","Training - running batch 109/176 of epoch 8/20 - loss: 0.9723291993141174 - acc:  74.218750%\n","Training - running batch 110/176 of epoch 8/20 - loss: 0.8308188915252686 - acc:  72.656250%\n","Training - running batch 111/176 of epoch 8/20 - loss: 0.6426739692687988 - acc:  85.156250%\n","Training - running batch 112/176 of epoch 8/20 - loss: 0.40045595169067383 - acc:  93.750000%\n","Training - running batch 113/176 of epoch 8/20 - loss: 0.7727487683296204 - acc:  82.031250%\n","Training - running batch 114/176 of epoch 8/20 - loss: 0.41680458188056946 - acc:  93.750000%\n","Training - running batch 115/176 of epoch 8/20 - loss: 0.8145293593406677 - acc:  75.000000%\n","Training - running batch 116/176 of epoch 8/20 - loss: 0.6827564239501953 - acc:  77.343750%\n","Training - running batch 117/176 of epoch 8/20 - loss: 0.41677340865135193 - acc:  95.312500%\n","Training - running batch 118/176 of epoch 8/20 - loss: 0.38486745953559875 - acc:  95.312500%\n","Training - running batch 119/176 of epoch 8/20 - loss: 0.6075319051742554 - acc:  81.250000%\n","Training - running batch 120/176 of epoch 8/20 - loss: 0.7512247562408447 - acc:  74.218750%\n","Training - running batch 121/176 of epoch 8/20 - loss: 0.48600879311561584 - acc:  91.406250%\n","Training - running batch 122/176 of epoch 8/20 - loss: 0.6684545278549194 - acc:  75.000000%\n","Training - running batch 123/176 of epoch 8/20 - loss: 0.6090559363365173 - acc:  81.250000%\n","Training - running batch 124/176 of epoch 8/20 - loss: 0.502209484577179 - acc:  85.937500%\n","Training - running batch 125/176 of epoch 8/20 - loss: 0.45496755838394165 - acc:  93.750000%\n","Training - running batch 126/176 of epoch 8/20 - loss: 0.6425862908363342 - acc:  89.062500%\n","Training - running batch 127/176 of epoch 8/20 - loss: 0.7445585131645203 - acc:  76.562500%\n","Training - running batch 128/176 of epoch 8/20 - loss: 0.4116981029510498 - acc:  91.406250%\n","Training - running batch 129/176 of epoch 8/20 - loss: 0.8069287538528442 - acc:  78.906250%\n","Training - running batch 130/176 of epoch 8/20 - loss: 0.5630422830581665 - acc:  83.593750%\n","Training - running batch 131/176 of epoch 8/20 - loss: 0.4320503771305084 - acc:  89.843750%\n","Training - running batch 132/176 of epoch 8/20 - loss: 0.40331536531448364 - acc:  91.406250%\n","Training - running batch 133/176 of epoch 8/20 - loss: 0.3992190361022949 - acc:  91.406250%\n","Training - running batch 134/176 of epoch 8/20 - loss: 0.5185686945915222 - acc:  89.062500%\n","Training - running batch 135/176 of epoch 8/20 - loss: 0.725383460521698 - acc:  77.343750%\n","Training - running batch 136/176 of epoch 8/20 - loss: 0.6047075986862183 - acc:  91.406250%\n","Training - running batch 137/176 of epoch 8/20 - loss: 0.5578055381774902 - acc:  89.843750%\n","Training - running batch 138/176 of epoch 8/20 - loss: 0.4942610263824463 - acc:  86.718750%\n","Training - running batch 139/176 of epoch 8/20 - loss: 0.38410842418670654 - acc:  92.968750%\n","Training - running batch 140/176 of epoch 8/20 - loss: 0.4864843189716339 - acc:  89.062500%\n","Training - running batch 141/176 of epoch 8/20 - loss: 0.623386561870575 - acc:  78.125000%\n","Training - running batch 142/176 of epoch 8/20 - loss: 0.6789642572402954 - acc:  78.125000%\n","Training - running batch 143/176 of epoch 8/20 - loss: 0.5027071237564087 - acc:  89.843750%\n","Training - running batch 144/176 of epoch 8/20 - loss: 0.7313007712364197 - acc:  75.781250%\n","Training - running batch 145/176 of epoch 8/20 - loss: 0.3616064190864563 - acc:  92.187500%\n","Training - running batch 146/176 of epoch 8/20 - loss: 0.7260915040969849 - acc:  75.781250%\n","Training - running batch 147/176 of epoch 8/20 - loss: 0.6257787346839905 - acc:  85.937500%\n","Training - running batch 148/176 of epoch 8/20 - loss: 0.7498422265052795 - acc:  75.781250%\n","Training - running batch 149/176 of epoch 8/20 - loss: 0.44351398944854736 - acc:  87.500000%\n","Training - running batch 150/176 of epoch 8/20 - loss: 0.7397672533988953 - acc:  72.656250%\n","Training - running batch 151/176 of epoch 8/20 - loss: 0.9280118942260742 - acc:  70.312500%\n","Training - running batch 152/176 of epoch 8/20 - loss: 0.4513358771800995 - acc:  91.406250%\n","Training - running batch 153/176 of epoch 8/20 - loss: 0.42412489652633667 - acc:  92.968750%\n","Training - running batch 154/176 of epoch 8/20 - loss: 0.7209200859069824 - acc:  75.000000%\n","Training - running batch 155/176 of epoch 8/20 - loss: 0.521238386631012 - acc:  86.718750%\n","Training - running batch 156/176 of epoch 8/20 - loss: 0.771064043045044 - acc:  75.000000%\n","Training - running batch 157/176 of epoch 8/20 - loss: 0.2691437602043152 - acc:  98.437500%\n","Training - running batch 158/176 of epoch 8/20 - loss: 0.5002551674842834 - acc:  88.281250%\n","Training - running batch 159/176 of epoch 8/20 - loss: 0.8137232065200806 - acc:  74.218750%\n","Training - running batch 160/176 of epoch 8/20 - loss: 0.7585213780403137 - acc:  78.125000%\n","Training - running batch 161/176 of epoch 8/20 - loss: 0.8529888987541199 - acc:  71.093750%\n","Training - running batch 162/176 of epoch 8/20 - loss: 0.6748215556144714 - acc:  78.125000%\n","Training - running batch 163/176 of epoch 8/20 - loss: 0.2879868447780609 - acc:  91.406250%\n","Training - running batch 164/176 of epoch 8/20 - loss: 0.8606671094894409 - acc:  69.531250%\n","Training - running batch 165/176 of epoch 8/20 - loss: 0.5420863628387451 - acc:  88.281250%\n","Training - running batch 166/176 of epoch 8/20 - loss: 0.5576481223106384 - acc:  89.843750%\n","Training - running batch 167/176 of epoch 8/20 - loss: 0.869550883769989 - acc:  70.312500%\n","Training - running batch 168/176 of epoch 8/20 - loss: 0.8411705493927002 - acc:  66.406250%\n","Training - running batch 169/176 of epoch 8/20 - loss: 0.6384738087654114 - acc:  76.562500%\n","Training - running batch 170/176 of epoch 8/20 - loss: 0.47304391860961914 - acc:  91.406250%\n","Training - running batch 171/176 of epoch 8/20 - loss: 0.665066659450531 - acc:  79.687500%\n","Training - running batch 172/176 of epoch 8/20 - loss: 0.400505930185318 - acc:  92.968750%\n","Training - running batch 173/176 of epoch 8/20 - loss: 0.7377224564552307 - acc:  84.375000%\n","Training - running batch 174/176 of epoch 8/20 - loss: 0.6991704702377319 - acc:  78.125000%\n","Training - running batch 175/176 of epoch 8/20 - loss: 0.7795725464820862 - acc:  82.258065%\n","Testing - acc:  0.685334%\n","Training - running batch 0/176 of epoch 9/20 - loss: 0.4061347246170044 - acc:  91.406250%\n","Training - running batch 1/176 of epoch 9/20 - loss: 0.8027858138084412 - acc:  71.093750%\n","Training - running batch 2/176 of epoch 9/20 - loss: 0.7045266032218933 - acc:  76.562500%\n","Training - running batch 3/176 of epoch 9/20 - loss: 0.38856515288352966 - acc:  94.531250%\n","Training - running batch 4/176 of epoch 9/20 - loss: 0.5601169466972351 - acc:  86.718750%\n","Training - running batch 5/176 of epoch 9/20 - loss: 0.7226385474205017 - acc:  77.343750%\n","Training - running batch 6/176 of epoch 9/20 - loss: 0.40978553891181946 - acc:  90.625000%\n","Training - running batch 7/176 of epoch 9/20 - loss: 0.32936257123947144 - acc:  96.875000%\n","Training - running batch 8/176 of epoch 9/20 - loss: 0.3153180480003357 - acc:  94.531250%\n","Training - running batch 9/176 of epoch 9/20 - loss: 0.3637293875217438 - acc:  94.531250%\n","Training - running batch 10/176 of epoch 9/20 - loss: 0.6299939155578613 - acc:  78.125000%\n","Training - running batch 11/176 of epoch 9/20 - loss: 0.35235825181007385 - acc:  93.750000%\n","Training - running batch 12/176 of epoch 9/20 - loss: 0.6693369150161743 - acc:  79.687500%\n","Training - running batch 13/176 of epoch 9/20 - loss: 0.8438430428504944 - acc:  69.531250%\n","Training - running batch 14/176 of epoch 9/20 - loss: 0.2814137637615204 - acc:  95.312500%\n","Training - running batch 15/176 of epoch 9/20 - loss: 0.6358551383018494 - acc:  75.000000%\n","Training - running batch 16/176 of epoch 9/20 - loss: 0.30669718980789185 - acc:  93.750000%\n","Training - running batch 17/176 of epoch 9/20 - loss: 0.6094610095024109 - acc:  82.812500%\n","Training - running batch 18/176 of epoch 9/20 - loss: 0.485779732465744 - acc:  86.718750%\n","Training - running batch 19/176 of epoch 9/20 - loss: 0.605276346206665 - acc:  82.812500%\n","Training - running batch 20/176 of epoch 9/20 - loss: 0.7418380975723267 - acc:  76.562500%\n","Training - running batch 21/176 of epoch 9/20 - loss: 0.6736837029457092 - acc:  78.125000%\n","Training - running batch 22/176 of epoch 9/20 - loss: 0.6379721760749817 - acc:  76.562500%\n","Training - running batch 23/176 of epoch 9/20 - loss: 0.6125544309616089 - acc:  82.031250%\n","Training - running batch 24/176 of epoch 9/20 - loss: 0.6349793672561646 - acc:  82.031250%\n","Training - running batch 25/176 of epoch 9/20 - loss: 0.7142483592033386 - acc:  82.812500%\n","Training - running batch 26/176 of epoch 9/20 - loss: 0.30517300963401794 - acc:  96.093750%\n","Training - running batch 27/176 of epoch 9/20 - loss: 0.659002423286438 - acc:  78.906250%\n","Training - running batch 28/176 of epoch 9/20 - loss: 0.37730729579925537 - acc:  89.843750%\n","Training - running batch 29/176 of epoch 9/20 - loss: 0.5919578671455383 - acc:  82.812500%\n","Training - running batch 30/176 of epoch 9/20 - loss: 0.33812215924263 - acc:  93.750000%\n","Training - running batch 31/176 of epoch 9/20 - loss: 0.41147714853286743 - acc:  94.531250%\n","Training - running batch 32/176 of epoch 9/20 - loss: 0.7249563336372375 - acc:  74.218750%\n","Training - running batch 33/176 of epoch 9/20 - loss: 0.6316516995429993 - acc:  85.937500%\n","Training - running batch 34/176 of epoch 9/20 - loss: 0.4367271363735199 - acc:  93.750000%\n","Training - running batch 35/176 of epoch 9/20 - loss: 0.6428784132003784 - acc:  79.687500%\n","Training - running batch 36/176 of epoch 9/20 - loss: 0.584189236164093 - acc:  83.593750%\n","Training - running batch 37/176 of epoch 9/20 - loss: 0.7281742691993713 - acc:  74.218750%\n","Training - running batch 38/176 of epoch 9/20 - loss: 0.711154580116272 - acc:  71.875000%\n","Training - running batch 39/176 of epoch 9/20 - loss: 0.3328374922275543 - acc:  93.750000%\n","Training - running batch 40/176 of epoch 9/20 - loss: 0.5056407451629639 - acc:  93.750000%\n","Training - running batch 41/176 of epoch 9/20 - loss: 0.7107577323913574 - acc:  78.906250%\n","Training - running batch 42/176 of epoch 9/20 - loss: 0.43955910205841064 - acc:  90.625000%\n","Training - running batch 43/176 of epoch 9/20 - loss: 0.7316377758979797 - acc:  75.781250%\n","Training - running batch 44/176 of epoch 9/20 - loss: 0.7505894303321838 - acc:  75.781250%\n","Training - running batch 45/176 of epoch 9/20 - loss: 0.2835035026073456 - acc:  97.656250%\n","Training - running batch 46/176 of epoch 9/20 - loss: 0.31898078322410583 - acc:  93.750000%\n","Training - running batch 47/176 of epoch 9/20 - loss: 0.3920416235923767 - acc:  94.531250%\n","Training - running batch 48/176 of epoch 9/20 - loss: 0.6943302154541016 - acc:  75.000000%\n","Training - running batch 49/176 of epoch 9/20 - loss: 0.42659783363342285 - acc:  88.281250%\n","Training - running batch 50/176 of epoch 9/20 - loss: 0.3049808442592621 - acc:  92.187500%\n","Training - running batch 51/176 of epoch 9/20 - loss: 0.7026662230491638 - acc:  77.343750%\n","Training - running batch 52/176 of epoch 9/20 - loss: 0.43728265166282654 - acc:  92.968750%\n","Training - running batch 53/176 of epoch 9/20 - loss: 0.32925325632095337 - acc:  92.187500%\n","Training - running batch 54/176 of epoch 9/20 - loss: 0.4371328353881836 - acc:  92.187500%\n","Training - running batch 55/176 of epoch 9/20 - loss: 0.3628675639629364 - acc:  90.625000%\n","Training - running batch 56/176 of epoch 9/20 - loss: 0.28079256415367126 - acc:  92.968750%\n","Training - running batch 57/176 of epoch 9/20 - loss: 0.7329263091087341 - acc:  79.687500%\n","Training - running batch 58/176 of epoch 9/20 - loss: 0.3014450669288635 - acc:  96.093750%\n","Training - running batch 59/176 of epoch 9/20 - loss: 0.24372947216033936 - acc:  99.218750%\n","Training - running batch 60/176 of epoch 9/20 - loss: 0.2991941273212433 - acc:  96.093750%\n","Training - running batch 61/176 of epoch 9/20 - loss: 0.6706994771957397 - acc:  81.250000%\n","Training - running batch 62/176 of epoch 9/20 - loss: 0.6449787020683289 - acc:  82.812500%\n","Training - running batch 63/176 of epoch 9/20 - loss: 0.6388093829154968 - acc:  78.125000%\n","Training - running batch 64/176 of epoch 9/20 - loss: 0.6907851099967957 - acc:  77.343750%\n","Training - running batch 65/176 of epoch 9/20 - loss: 0.34879404306411743 - acc:  95.312500%\n","Training - running batch 66/176 of epoch 9/20 - loss: 0.6268331408500671 - acc:  79.687500%\n","Training - running batch 67/176 of epoch 9/20 - loss: 0.2923009991645813 - acc:  96.093750%\n","Training - running batch 68/176 of epoch 9/20 - loss: 0.7816112041473389 - acc:  76.562500%\n","Training - running batch 69/176 of epoch 9/20 - loss: 0.269907683134079 - acc:  96.093750%\n","Training - running batch 70/176 of epoch 9/20 - loss: 0.6361782550811768 - acc:  80.468750%\n","Training - running batch 71/176 of epoch 9/20 - loss: 0.3770822584629059 - acc:  90.625000%\n","Training - running batch 72/176 of epoch 9/20 - loss: 0.31781312823295593 - acc:  94.531250%\n","Training - running batch 73/176 of epoch 9/20 - loss: 0.6938514709472656 - acc:  75.000000%\n","Training - running batch 74/176 of epoch 9/20 - loss: 0.5565124154090881 - acc:  85.156250%\n","Training - running batch 75/176 of epoch 9/20 - loss: 0.6487756967544556 - acc:  80.468750%\n","Training - running batch 76/176 of epoch 9/20 - loss: 0.648958683013916 - acc:  80.468750%\n","Training - running batch 77/176 of epoch 9/20 - loss: 0.6403025984764099 - acc:  83.593750%\n","Training - running batch 78/176 of epoch 9/20 - loss: 0.3256676495075226 - acc:  95.312500%\n","Training - running batch 79/176 of epoch 9/20 - loss: 0.4109848439693451 - acc:  89.843750%\n","Training - running batch 80/176 of epoch 9/20 - loss: 0.5510385036468506 - acc:  88.281250%\n","Training - running batch 81/176 of epoch 9/20 - loss: 0.359657883644104 - acc:  91.406250%\n","Training - running batch 82/176 of epoch 9/20 - loss: 0.6872732639312744 - acc:  74.218750%\n","Training - running batch 83/176 of epoch 9/20 - loss: 0.4287111759185791 - acc:  89.062500%\n","Training - running batch 84/176 of epoch 9/20 - loss: 0.39731383323669434 - acc:  92.968750%\n","Training - running batch 85/176 of epoch 9/20 - loss: 0.2659667730331421 - acc:  98.437500%\n","Training - running batch 86/176 of epoch 9/20 - loss: 0.8794699907302856 - acc:  75.781250%\n","Training - running batch 87/176 of epoch 9/20 - loss: 0.7314794659614563 - acc:  78.906250%\n","Training - running batch 88/176 of epoch 9/20 - loss: 0.5580688714981079 - acc:  85.156250%\n","Training - running batch 89/176 of epoch 9/20 - loss: 0.3076132535934448 - acc:  92.968750%\n","Training - running batch 90/176 of epoch 9/20 - loss: 0.5587584972381592 - acc:  81.250000%\n","Training - running batch 91/176 of epoch 9/20 - loss: 0.6121951341629028 - acc:  82.031250%\n","Training - running batch 92/176 of epoch 9/20 - loss: 0.28484100103378296 - acc:  95.312500%\n","Training - running batch 93/176 of epoch 9/20 - loss: 0.6407980918884277 - acc:  80.468750%\n","Training - running batch 94/176 of epoch 9/20 - loss: 0.23108941316604614 - acc:  97.656250%\n","Training - running batch 95/176 of epoch 9/20 - loss: 0.4176059067249298 - acc:  91.406250%\n","Training - running batch 96/176 of epoch 9/20 - loss: 0.2952176034450531 - acc:  93.750000%\n","Training - running batch 97/176 of epoch 9/20 - loss: 0.3282063603401184 - acc:  93.750000%\n","Training - running batch 98/176 of epoch 9/20 - loss: 0.5987474322319031 - acc:  82.812500%\n","Training - running batch 99/176 of epoch 9/20 - loss: 0.29872387647628784 - acc:  93.750000%\n","Training - running batch 100/176 of epoch 9/20 - loss: 0.3085743486881256 - acc:  94.531250%\n","Training - running batch 101/176 of epoch 9/20 - loss: 0.7680339813232422 - acc:  75.000000%\n","Training - running batch 102/176 of epoch 9/20 - loss: 0.6438772082328796 - acc:  81.250000%\n","Training - running batch 103/176 of epoch 9/20 - loss: 0.36212918162345886 - acc:  92.968750%\n","Training - running batch 104/176 of epoch 9/20 - loss: 0.7350404262542725 - acc:  78.125000%\n","Training - running batch 105/176 of epoch 9/20 - loss: 0.2869740426540375 - acc:  93.750000%\n","Training - running batch 106/176 of epoch 9/20 - loss: 0.3317309617996216 - acc:  92.968750%\n","Training - running batch 107/176 of epoch 9/20 - loss: 0.29125991463661194 - acc:  96.875000%\n","Training - running batch 108/176 of epoch 9/20 - loss: 0.2723100483417511 - acc:  93.750000%\n","Training - running batch 109/176 of epoch 9/20 - loss: 0.5059686899185181 - acc:  87.500000%\n","Training - running batch 110/176 of epoch 9/20 - loss: 0.7633772492408752 - acc:  78.125000%\n","Training - running batch 111/176 of epoch 9/20 - loss: 0.4596309959888458 - acc:  91.406250%\n","Training - running batch 112/176 of epoch 9/20 - loss: 0.32921189069747925 - acc:  92.187500%\n","Training - running batch 113/176 of epoch 9/20 - loss: 0.5153712034225464 - acc:  89.062500%\n","Training - running batch 114/176 of epoch 9/20 - loss: 0.3542116582393646 - acc:  94.531250%\n","Training - running batch 115/176 of epoch 9/20 - loss: 0.6517168283462524 - acc:  80.468750%\n","Training - running batch 116/176 of epoch 9/20 - loss: 0.4552443027496338 - acc:  89.062500%\n","Training - running batch 117/176 of epoch 9/20 - loss: 0.3540337383747101 - acc:  89.062500%\n","Training - running batch 118/176 of epoch 9/20 - loss: 0.6302991509437561 - acc:  79.687500%\n","Training - running batch 119/176 of epoch 9/20 - loss: 0.6999315023422241 - acc:  73.437500%\n","Training - running batch 120/176 of epoch 9/20 - loss: 0.3177816867828369 - acc:  92.187500%\n","Training - running batch 121/176 of epoch 9/20 - loss: 0.30706319212913513 - acc:  95.312500%\n","Training - running batch 122/176 of epoch 9/20 - loss: 0.44926562905311584 - acc:  89.062500%\n","Training - running batch 123/176 of epoch 9/20 - loss: 0.33723345398902893 - acc:  94.531250%\n","Training - running batch 124/176 of epoch 9/20 - loss: 0.8281753063201904 - acc:  71.875000%\n","Training - running batch 125/176 of epoch 9/20 - loss: 0.6699257493019104 - acc:  76.562500%\n","Training - running batch 126/176 of epoch 9/20 - loss: 0.4413922429084778 - acc:  88.281250%\n","Training - running batch 127/176 of epoch 9/20 - loss: 0.42788487672805786 - acc:  93.750000%\n","Training - running batch 128/176 of epoch 9/20 - loss: 0.3228316605091095 - acc:  93.750000%\n","Training - running batch 129/176 of epoch 9/20 - loss: 0.3512805104255676 - acc:  95.312500%\n","Training - running batch 130/176 of epoch 9/20 - loss: 0.5981757044792175 - acc:  80.468750%\n","Training - running batch 131/176 of epoch 9/20 - loss: 0.32394084334373474 - acc:  93.750000%\n","Training - running batch 132/176 of epoch 9/20 - loss: 0.38430577516555786 - acc:  87.500000%\n","Training - running batch 133/176 of epoch 9/20 - loss: 0.40106791257858276 - acc:  90.625000%\n","Training - running batch 134/176 of epoch 9/20 - loss: 0.33667880296707153 - acc:  92.968750%\n","Training - running batch 135/176 of epoch 9/20 - loss: 0.713585615158081 - acc:  76.562500%\n","Training - running batch 136/176 of epoch 9/20 - loss: 0.41054046154022217 - acc:  92.968750%\n","Training - running batch 137/176 of epoch 9/20 - loss: 0.4918648600578308 - acc:  89.062500%\n","Training - running batch 138/176 of epoch 9/20 - loss: 0.6983218193054199 - acc:  82.031250%\n","Training - running batch 139/176 of epoch 9/20 - loss: 0.28585144877433777 - acc:  92.187500%\n","Training - running batch 140/176 of epoch 9/20 - loss: 0.6859790086746216 - acc:  79.687500%\n","Training - running batch 141/176 of epoch 9/20 - loss: 0.6503568887710571 - acc:  75.000000%\n","Training - running batch 142/176 of epoch 9/20 - loss: 0.2915593981742859 - acc:  96.875000%\n","Training - running batch 143/176 of epoch 9/20 - loss: 0.8491494655609131 - acc:  68.750000%\n","Training - running batch 144/176 of epoch 9/20 - loss: 0.3512508273124695 - acc:  92.187500%\n","Training - running batch 145/176 of epoch 9/20 - loss: 0.2922961711883545 - acc:  96.875000%\n","Training - running batch 146/176 of epoch 9/20 - loss: 0.25390830636024475 - acc:  95.312500%\n","Training - running batch 147/176 of epoch 9/20 - loss: 0.7626584768295288 - acc:  77.343750%\n","Training - running batch 148/176 of epoch 9/20 - loss: 0.40078675746917725 - acc:  88.281250%\n","Training - running batch 149/176 of epoch 9/20 - loss: 0.30899763107299805 - acc:  92.968750%\n","Training - running batch 150/176 of epoch 9/20 - loss: 0.40260761976242065 - acc:  90.625000%\n","Training - running batch 151/176 of epoch 9/20 - loss: 0.5933670997619629 - acc:  82.031250%\n","Training - running batch 152/176 of epoch 9/20 - loss: 0.7082105875015259 - acc:  78.906250%\n","Training - running batch 153/176 of epoch 9/20 - loss: 0.8027300834655762 - acc:  75.781250%\n","Training - running batch 154/176 of epoch 9/20 - loss: 0.6738940477371216 - acc:  84.375000%\n","Training - running batch 155/176 of epoch 9/20 - loss: 0.3245178759098053 - acc:  96.093750%\n","Training - running batch 156/176 of epoch 9/20 - loss: 0.34272301197052 - acc:  92.968750%\n","Training - running batch 157/176 of epoch 9/20 - loss: 0.2287634313106537 - acc:  96.875000%\n","Training - running batch 158/176 of epoch 9/20 - loss: 0.9423696398735046 - acc:  68.750000%\n","Training - running batch 159/176 of epoch 9/20 - loss: 0.8233616948127747 - acc:  77.343750%\n","Training - running batch 160/176 of epoch 9/20 - loss: 0.38670477271080017 - acc:  92.187500%\n","Training - running batch 161/176 of epoch 9/20 - loss: 0.2775464951992035 - acc:  93.750000%\n","Training - running batch 162/176 of epoch 9/20 - loss: 0.6110115051269531 - acc:  83.593750%\n","Training - running batch 163/176 of epoch 9/20 - loss: 0.34270212054252625 - acc:  94.531250%\n","Training - running batch 164/176 of epoch 9/20 - loss: 0.36423611640930176 - acc:  91.406250%\n","Training - running batch 165/176 of epoch 9/20 - loss: 0.22495120763778687 - acc:  93.750000%\n","Training - running batch 166/176 of epoch 9/20 - loss: 0.5734325647354126 - acc:  82.031250%\n","Training - running batch 167/176 of epoch 9/20 - loss: 0.36238595843315125 - acc:  89.843750%\n","Training - running batch 168/176 of epoch 9/20 - loss: 0.4761543273925781 - acc:  86.718750%\n","Training - running batch 169/176 of epoch 9/20 - loss: 0.3618149161338806 - acc:  93.750000%\n","Training - running batch 170/176 of epoch 9/20 - loss: 0.41352349519729614 - acc:  90.625000%\n","Training - running batch 171/176 of epoch 9/20 - loss: 0.538327157497406 - acc:  87.500000%\n","Training - running batch 172/176 of epoch 9/20 - loss: 0.6230396032333374 - acc:  77.343750%\n","Training - running batch 173/176 of epoch 9/20 - loss: 0.7100886702537537 - acc:  75.781250%\n","Training - running batch 174/176 of epoch 9/20 - loss: 0.31690871715545654 - acc:  98.437500%\n","Training - running batch 175/176 of epoch 9/20 - loss: 0.7251514196395874 - acc:  79.032258%\n","Testing - acc:  0.688547%\n","Training - running batch 0/176 of epoch 10/20 - loss: 0.6864532232284546 - acc:  82.812500%\n","Training - running batch 1/176 of epoch 10/20 - loss: 0.6946472525596619 - acc:  81.250000%\n","Training - running batch 2/176 of epoch 10/20 - loss: 0.7114921808242798 - acc:  81.250000%\n","Training - running batch 3/176 of epoch 10/20 - loss: 0.4380107820034027 - acc:  91.406250%\n","Training - running batch 4/176 of epoch 10/20 - loss: 0.4320409297943115 - acc:  92.968750%\n","Training - running batch 5/176 of epoch 10/20 - loss: 0.8015201687812805 - acc:  75.781250%\n","Training - running batch 6/176 of epoch 10/20 - loss: 0.8388227224349976 - acc:  75.781250%\n","Training - running batch 7/176 of epoch 10/20 - loss: 0.34432879090309143 - acc:  93.750000%\n","Training - running batch 8/176 of epoch 10/20 - loss: 0.8944059014320374 - acc:  80.468750%\n","Training - running batch 9/176 of epoch 10/20 - loss: 0.5462819933891296 - acc:  87.500000%\n","Training - running batch 10/176 of epoch 10/20 - loss: 0.2886388599872589 - acc:  96.875000%\n","Training - running batch 11/176 of epoch 10/20 - loss: 0.25981661677360535 - acc:  98.437500%\n","Training - running batch 12/176 of epoch 10/20 - loss: 0.33346566557884216 - acc:  95.312500%\n","Training - running batch 13/176 of epoch 10/20 - loss: 0.3537885546684265 - acc:  93.750000%\n","Training - running batch 14/176 of epoch 10/20 - loss: 0.4654366075992584 - acc:  91.406250%\n","Training - running batch 15/176 of epoch 10/20 - loss: 0.43215134739875793 - acc:  92.187500%\n","Training - running batch 16/176 of epoch 10/20 - loss: 0.3849034309387207 - acc:  92.187500%\n","Training - running batch 17/176 of epoch 10/20 - loss: 0.654249370098114 - acc:  84.375000%\n","Training - running batch 18/176 of epoch 10/20 - loss: 0.48812729120254517 - acc:  92.187500%\n","Training - running batch 19/176 of epoch 10/20 - loss: 0.2979266047477722 - acc:  95.312500%\n","Training - running batch 20/176 of epoch 10/20 - loss: 0.315787672996521 - acc:  96.875000%\n","Training - running batch 21/176 of epoch 10/20 - loss: 0.3534352481365204 - acc:  94.531250%\n","Training - running batch 22/176 of epoch 10/20 - loss: 1.023592472076416 - acc:  71.875000%\n","Training - running batch 23/176 of epoch 10/20 - loss: 0.2760877013206482 - acc:  96.093750%\n","Training - running batch 24/176 of epoch 10/20 - loss: 0.37073710560798645 - acc:  92.968750%\n","Training - running batch 25/176 of epoch 10/20 - loss: 0.6413201093673706 - acc:  84.375000%\n","Training - running batch 26/176 of epoch 10/20 - loss: 0.3353990316390991 - acc:  92.187500%\n","Training - running batch 27/176 of epoch 10/20 - loss: 0.32909804582595825 - acc:  93.750000%\n","Training - running batch 28/176 of epoch 10/20 - loss: 0.797177255153656 - acc:  75.781250%\n","Training - running batch 29/176 of epoch 10/20 - loss: 0.24515964090824127 - acc:  96.875000%\n","Training - running batch 30/176 of epoch 10/20 - loss: 0.7188096642494202 - acc:  82.812500%\n","Training - running batch 31/176 of epoch 10/20 - loss: 0.2775667905807495 - acc:  95.312500%\n","Training - running batch 32/176 of epoch 10/20 - loss: 0.7432755827903748 - acc:  77.343750%\n","Training - running batch 33/176 of epoch 10/20 - loss: 0.24941886961460114 - acc:  97.656250%\n","Training - running batch 34/176 of epoch 10/20 - loss: 0.27648138999938965 - acc:  93.750000%\n","Training - running batch 35/176 of epoch 10/20 - loss: 0.24979077279567719 - acc:  96.093750%\n","Training - running batch 36/176 of epoch 10/20 - loss: 0.30049028992652893 - acc:  95.312500%\n","Training - running batch 37/176 of epoch 10/20 - loss: 0.7493550777435303 - acc:  77.343750%\n","Training - running batch 38/176 of epoch 10/20 - loss: 0.28942862153053284 - acc:  95.312500%\n","Training - running batch 39/176 of epoch 10/20 - loss: 0.6196749210357666 - acc:  84.375000%\n","Training - running batch 40/176 of epoch 10/20 - loss: 0.3689501881599426 - acc:  91.406250%\n","Training - running batch 41/176 of epoch 10/20 - loss: 0.9150736927986145 - acc:  74.218750%\n","Training - running batch 42/176 of epoch 10/20 - loss: 0.740388035774231 - acc:  80.468750%\n","Training - running batch 43/176 of epoch 10/20 - loss: 0.7147433757781982 - acc:  85.156250%\n","Training - running batch 44/176 of epoch 10/20 - loss: 0.24197664856910706 - acc:  100.000000%\n","Training - running batch 45/176 of epoch 10/20 - loss: 0.6406038403511047 - acc:  86.718750%\n","Training - running batch 46/176 of epoch 10/20 - loss: 0.8170164823532104 - acc:  69.531250%\n","Training - running batch 47/176 of epoch 10/20 - loss: 0.6098725199699402 - acc:  87.500000%\n","Training - running batch 48/176 of epoch 10/20 - loss: 0.3189896047115326 - acc:  96.875000%\n","Training - running batch 49/176 of epoch 10/20 - loss: 0.38344451785087585 - acc:  90.625000%\n","Training - running batch 50/176 of epoch 10/20 - loss: 0.7165421843528748 - acc:  79.687500%\n","Training - running batch 51/176 of epoch 10/20 - loss: 0.3205429017543793 - acc:  94.531250%\n","Training - running batch 52/176 of epoch 10/20 - loss: 0.6145145297050476 - acc:  85.156250%\n","Training - running batch 53/176 of epoch 10/20 - loss: 0.8327765464782715 - acc:  78.906250%\n","Training - running batch 54/176 of epoch 10/20 - loss: 0.7070016860961914 - acc:  78.125000%\n","Training - running batch 55/176 of epoch 10/20 - loss: 0.6127137541770935 - acc:  83.593750%\n","Training - running batch 56/176 of epoch 10/20 - loss: 0.561724066734314 - acc:  83.593750%\n","Training - running batch 57/176 of epoch 10/20 - loss: 0.5856549143791199 - acc:  84.375000%\n","Training - running batch 58/176 of epoch 10/20 - loss: 0.34902223944664 - acc:  95.312500%\n","Training - running batch 59/176 of epoch 10/20 - loss: 0.27024680376052856 - acc:  99.218750%\n","Training - running batch 60/176 of epoch 10/20 - loss: 0.6358715295791626 - acc:  76.562500%\n","Training - running batch 61/176 of epoch 10/20 - loss: 0.5954328775405884 - acc:  85.156250%\n","Training - running batch 62/176 of epoch 10/20 - loss: 0.6839418411254883 - acc:  80.468750%\n","Training - running batch 63/176 of epoch 10/20 - loss: 0.8647403120994568 - acc:  74.218750%\n","Training - running batch 64/176 of epoch 10/20 - loss: 0.5544506311416626 - acc:  86.718750%\n","Training - running batch 65/176 of epoch 10/20 - loss: 0.6690765619277954 - acc:  89.062500%\n","Training - running batch 66/176 of epoch 10/20 - loss: 0.410535991191864 - acc:  96.875000%\n","Training - running batch 67/176 of epoch 10/20 - loss: 0.29301050305366516 - acc:  99.218750%\n","Training - running batch 68/176 of epoch 10/20 - loss: 0.45439207553863525 - acc:  90.625000%\n","Training - running batch 69/176 of epoch 10/20 - loss: 0.3478298783302307 - acc:  92.968750%\n","Training - running batch 70/176 of epoch 10/20 - loss: 0.32229143381118774 - acc:  96.093750%\n","Training - running batch 71/176 of epoch 10/20 - loss: 0.6715579032897949 - acc:  84.375000%\n","Training - running batch 72/176 of epoch 10/20 - loss: 0.20599162578582764 - acc:  98.437500%\n","Training - running batch 73/176 of epoch 10/20 - loss: 0.6383278965950012 - acc:  82.031250%\n","Training - running batch 74/176 of epoch 10/20 - loss: 0.6105183959007263 - acc:  82.812500%\n","Training - running batch 75/176 of epoch 10/20 - loss: 0.3066105544567108 - acc:  96.093750%\n","Training - running batch 76/176 of epoch 10/20 - loss: 0.363709032535553 - acc:  92.968750%\n","Training - running batch 77/176 of epoch 10/20 - loss: 0.3836546838283539 - acc:  93.750000%\n","Training - running batch 78/176 of epoch 10/20 - loss: 0.7368720769882202 - acc:  79.687500%\n","Training - running batch 79/176 of epoch 10/20 - loss: 0.7464220523834229 - acc:  80.468750%\n","Training - running batch 80/176 of epoch 10/20 - loss: 0.6945172548294067 - acc:  75.781250%\n","Training - running batch 81/176 of epoch 10/20 - loss: 0.5090615153312683 - acc:  91.406250%\n","Training - running batch 82/176 of epoch 10/20 - loss: 0.5543212890625 - acc:  89.843750%\n","Training - running batch 83/176 of epoch 10/20 - loss: 0.32280781865119934 - acc:  98.437500%\n","Training - running batch 84/176 of epoch 10/20 - loss: 0.3263413608074188 - acc:  93.750000%\n","Training - running batch 85/176 of epoch 10/20 - loss: 0.4851134121417999 - acc:  89.062500%\n","Training - running batch 86/176 of epoch 10/20 - loss: 0.3197358548641205 - acc:  96.093750%\n","Training - running batch 87/176 of epoch 10/20 - loss: 0.6543052196502686 - acc:  84.375000%\n","Training - running batch 88/176 of epoch 10/20 - loss: 0.2878466248512268 - acc:  97.656250%\n","Training - running batch 89/176 of epoch 10/20 - loss: 0.25231462717056274 - acc:  97.656250%\n","Training - running batch 90/176 of epoch 10/20 - loss: 0.3855421841144562 - acc:  94.531250%\n","Training - running batch 91/176 of epoch 10/20 - loss: 0.6025381684303284 - acc:  85.156250%\n","Training - running batch 92/176 of epoch 10/20 - loss: 0.659828782081604 - acc:  81.250000%\n","Training - running batch 93/176 of epoch 10/20 - loss: 0.3200247883796692 - acc:  95.312500%\n","Training - running batch 94/176 of epoch 10/20 - loss: 0.4924083948135376 - acc:  89.062500%\n","Training - running batch 95/176 of epoch 10/20 - loss: 0.7045203447341919 - acc:  81.250000%\n","Training - running batch 96/176 of epoch 10/20 - loss: 0.364714652299881 - acc:  94.531250%\n","Training - running batch 97/176 of epoch 10/20 - loss: 0.22370557487010956 - acc:  97.656250%\n","Training - running batch 98/176 of epoch 10/20 - loss: 0.7240630388259888 - acc:  81.250000%\n","Training - running batch 99/176 of epoch 10/20 - loss: 0.28378742933273315 - acc:  96.875000%\n","Training - running batch 100/176 of epoch 10/20 - loss: 0.6124969720840454 - acc:  84.375000%\n","Training - running batch 101/176 of epoch 10/20 - loss: 0.6475107073783875 - acc:  83.593750%\n","Training - running batch 102/176 of epoch 10/20 - loss: 0.5050797462463379 - acc:  89.062500%\n","Training - running batch 103/176 of epoch 10/20 - loss: 0.760425329208374 - acc:  80.468750%\n","Training - running batch 104/176 of epoch 10/20 - loss: 0.5408660173416138 - acc:  87.500000%\n","Training - running batch 105/176 of epoch 10/20 - loss: 0.39432188868522644 - acc:  94.531250%\n","Training - running batch 106/176 of epoch 10/20 - loss: 0.6579211950302124 - acc:  85.156250%\n","Training - running batch 107/176 of epoch 10/20 - loss: 0.43894684314727783 - acc:  90.625000%\n","Training - running batch 108/176 of epoch 10/20 - loss: 0.6757298707962036 - acc:  83.593750%\n","Training - running batch 109/176 of epoch 10/20 - loss: 0.35482197999954224 - acc:  92.968750%\n","Training - running batch 110/176 of epoch 10/20 - loss: 1.0254459381103516 - acc:  71.093750%\n","Training - running batch 111/176 of epoch 10/20 - loss: 0.5427337884902954 - acc:  89.062500%\n","Training - running batch 112/176 of epoch 10/20 - loss: 0.8098737597465515 - acc:  78.125000%\n","Training - running batch 113/176 of epoch 10/20 - loss: 0.766235888004303 - acc:  75.000000%\n","Training - running batch 114/176 of epoch 10/20 - loss: 0.7079344987869263 - acc:  80.468750%\n","Training - running batch 115/176 of epoch 10/20 - loss: 0.5063855051994324 - acc:  86.718750%\n","Training - running batch 116/176 of epoch 10/20 - loss: 0.5263862013816833 - acc:  92.187500%\n","Training - running batch 117/176 of epoch 10/20 - loss: 0.5195383429527283 - acc:  90.625000%\n","Training - running batch 118/176 of epoch 10/20 - loss: 0.7654401659965515 - acc:  78.125000%\n","Training - running batch 119/176 of epoch 10/20 - loss: 0.7700052261352539 - acc:  76.562500%\n","Training - running batch 120/176 of epoch 10/20 - loss: 0.3278750777244568 - acc:  96.875000%\n","Training - running batch 121/176 of epoch 10/20 - loss: 0.2640393376350403 - acc:  96.875000%\n","Training - running batch 122/176 of epoch 10/20 - loss: 0.5726406574249268 - acc:  83.593750%\n","Training - running batch 123/176 of epoch 10/20 - loss: 0.706076979637146 - acc:  83.593750%\n","Training - running batch 124/176 of epoch 10/20 - loss: 0.6468899846076965 - acc:  84.375000%\n","Training - running batch 125/176 of epoch 10/20 - loss: 0.32631561160087585 - acc:  92.968750%\n","Training - running batch 126/176 of epoch 10/20 - loss: 0.7136080265045166 - acc:  81.250000%\n","Training - running batch 127/176 of epoch 10/20 - loss: 0.3545677065849304 - acc:  94.531250%\n","Training - running batch 128/176 of epoch 10/20 - loss: 0.3935049772262573 - acc:  92.968750%\n","Training - running batch 129/176 of epoch 10/20 - loss: 0.3999951183795929 - acc:  97.656250%\n","Training - running batch 130/176 of epoch 10/20 - loss: 0.4329419732093811 - acc:  92.968750%\n","Training - running batch 131/176 of epoch 10/20 - loss: 0.8247506022453308 - acc:  80.468750%\n","Training - running batch 132/176 of epoch 10/20 - loss: 0.6116912364959717 - acc:  83.593750%\n","Training - running batch 133/176 of epoch 10/20 - loss: 0.35707321763038635 - acc:  92.968750%\n","Training - running batch 134/176 of epoch 10/20 - loss: 0.4412080943584442 - acc:  92.187500%\n","Training - running batch 135/176 of epoch 10/20 - loss: 0.3269498944282532 - acc:  91.406250%\n","Training - running batch 136/176 of epoch 10/20 - loss: 0.2937217652797699 - acc:  96.093750%\n","Training - running batch 137/176 of epoch 10/20 - loss: 0.7378694415092468 - acc:  77.343750%\n","Training - running batch 138/176 of epoch 10/20 - loss: 0.5613582134246826 - acc:  84.375000%\n","Training - running batch 139/176 of epoch 10/20 - loss: 0.3434990346431732 - acc:  92.968750%\n","Training - running batch 140/176 of epoch 10/20 - loss: 0.6118252873420715 - acc:  92.187500%\n","Training - running batch 141/176 of epoch 10/20 - loss: 0.7477213144302368 - acc:  79.687500%\n","Training - running batch 142/176 of epoch 10/20 - loss: 0.745906412601471 - acc:  75.000000%\n","Training - running batch 143/176 of epoch 10/20 - loss: 0.9393782019615173 - acc:  77.343750%\n","Training - running batch 144/176 of epoch 10/20 - loss: 0.48905444145202637 - acc:  92.187500%\n","Training - running batch 145/176 of epoch 10/20 - loss: 0.39096006751060486 - acc:  90.625000%\n","Training - running batch 146/176 of epoch 10/20 - loss: 0.3820938766002655 - acc:  91.406250%\n","Training - running batch 147/176 of epoch 10/20 - loss: 0.4407678246498108 - acc:  88.281250%\n","Training - running batch 148/176 of epoch 10/20 - loss: 0.7872232794761658 - acc:  77.343750%\n","Training - running batch 149/176 of epoch 10/20 - loss: 0.7348541617393494 - acc:  79.687500%\n","Training - running batch 150/176 of epoch 10/20 - loss: 0.7495914697647095 - acc:  78.906250%\n","Training - running batch 151/176 of epoch 10/20 - loss: 0.40658533573150635 - acc:  89.062500%\n","Training - running batch 152/176 of epoch 10/20 - loss: 0.35373419523239136 - acc:  92.187500%\n","Training - running batch 153/176 of epoch 10/20 - loss: 0.7507973313331604 - acc:  80.468750%\n","Training - running batch 154/176 of epoch 10/20 - loss: 0.4066508114337921 - acc:  92.187500%\n","Training - running batch 155/176 of epoch 10/20 - loss: 0.905687689781189 - acc:  72.656250%\n","Training - running batch 156/176 of epoch 10/20 - loss: 0.7998084425926208 - acc:  75.000000%\n","Training - running batch 157/176 of epoch 10/20 - loss: 0.5640490651130676 - acc:  86.718750%\n","Training - running batch 158/176 of epoch 10/20 - loss: 0.7753179669380188 - acc:  70.312500%\n","Training - running batch 159/176 of epoch 10/20 - loss: 0.3185131549835205 - acc:  96.093750%\n","Training - running batch 160/176 of epoch 10/20 - loss: 0.2786332368850708 - acc:  96.875000%\n","Training - running batch 161/176 of epoch 10/20 - loss: 0.8009795546531677 - acc:  75.000000%\n","Training - running batch 162/176 of epoch 10/20 - loss: 0.6326583027839661 - acc:  80.468750%\n","Training - running batch 163/176 of epoch 10/20 - loss: 0.6923533082008362 - acc:  76.562500%\n","Training - running batch 164/176 of epoch 10/20 - loss: 0.31708037853240967 - acc:  94.531250%\n","Training - running batch 165/176 of epoch 10/20 - loss: 0.6569439768791199 - acc:  85.156250%\n","Training - running batch 166/176 of epoch 10/20 - loss: 0.6905847787857056 - acc:  81.250000%\n","Training - running batch 167/176 of epoch 10/20 - loss: 0.48988738656044006 - acc:  91.406250%\n","Training - running batch 168/176 of epoch 10/20 - loss: 0.4701847434043884 - acc:  90.625000%\n","Training - running batch 169/176 of epoch 10/20 - loss: 0.7235963344573975 - acc:  80.468750%\n","Training - running batch 170/176 of epoch 10/20 - loss: 0.6213924288749695 - acc:  80.468750%\n","Training - running batch 171/176 of epoch 10/20 - loss: 0.4769026041030884 - acc:  90.625000%\n","Training - running batch 172/176 of epoch 10/20 - loss: 0.5663095712661743 - acc:  84.375000%\n","Training - running batch 173/176 of epoch 10/20 - loss: 0.29835012555122375 - acc:  96.875000%\n","Training - running batch 174/176 of epoch 10/20 - loss: 0.5997186303138733 - acc:  82.812500%\n","Training - running batch 175/176 of epoch 10/20 - loss: 0.9276545643806458 - acc:  75.806452%\n","Testing - acc:  0.683498%\n","Training - running batch 0/176 of epoch 11/20 - loss: 0.5300437211990356 - acc:  88.281250%\n","Training - running batch 1/176 of epoch 11/20 - loss: 0.2698826789855957 - acc:  97.656250%\n","Training - running batch 2/176 of epoch 11/20 - loss: 0.24503956735134125 - acc:  97.656250%\n","Training - running batch 3/176 of epoch 11/20 - loss: 0.2741415798664093 - acc:  98.437500%\n","Training - running batch 4/176 of epoch 11/20 - loss: 0.28800588846206665 - acc:  96.093750%\n","Training - running batch 5/176 of epoch 11/20 - loss: 0.4040312170982361 - acc:  92.968750%\n","Training - running batch 6/176 of epoch 11/20 - loss: 0.39383918046951294 - acc:  89.062500%\n","Training - running batch 7/176 of epoch 11/20 - loss: 0.7070823907852173 - acc:  74.218750%\n","Training - running batch 8/176 of epoch 11/20 - loss: 0.3952249586582184 - acc:  89.843750%\n","Training - running batch 9/176 of epoch 11/20 - loss: 0.5750255584716797 - acc:  82.812500%\n","Training - running batch 10/176 of epoch 11/20 - loss: 0.20271150767803192 - acc:  96.093750%\n","Training - running batch 11/176 of epoch 11/20 - loss: 0.5485711693763733 - acc:  85.156250%\n","Training - running batch 12/176 of epoch 11/20 - loss: 0.38747626543045044 - acc:  93.750000%\n","Training - running batch 13/176 of epoch 11/20 - loss: 0.2721106708049774 - acc:  97.656250%\n","Training - running batch 14/176 of epoch 11/20 - loss: 0.2214655876159668 - acc:  99.218750%\n","Training - running batch 15/176 of epoch 11/20 - loss: 0.3189052939414978 - acc:  94.531250%\n","Training - running batch 16/176 of epoch 11/20 - loss: 0.7064144015312195 - acc:  85.937500%\n","Training - running batch 17/176 of epoch 11/20 - loss: 0.266815721988678 - acc:  96.093750%\n","Training - running batch 18/176 of epoch 11/20 - loss: 0.634406328201294 - acc:  82.812500%\n","Training - running batch 19/176 of epoch 11/20 - loss: 0.2627222239971161 - acc:  96.093750%\n","Training - running batch 20/176 of epoch 11/20 - loss: 0.6536464691162109 - acc:  85.156250%\n","Training - running batch 21/176 of epoch 11/20 - loss: 0.3733251094818115 - acc:  92.187500%\n","Training - running batch 22/176 of epoch 11/20 - loss: 0.6536283493041992 - acc:  78.906250%\n","Training - running batch 23/176 of epoch 11/20 - loss: 0.2810981869697571 - acc:  94.531250%\n","Training - running batch 24/176 of epoch 11/20 - loss: 0.2521344721317291 - acc:  96.093750%\n","Training - running batch 25/176 of epoch 11/20 - loss: 0.19881722331047058 - acc:  98.437500%\n","Training - running batch 26/176 of epoch 11/20 - loss: 0.3688030540943146 - acc:  91.406250%\n","Training - running batch 27/176 of epoch 11/20 - loss: 0.37124350666999817 - acc:  92.187500%\n","Training - running batch 28/176 of epoch 11/20 - loss: 0.2687288224697113 - acc:  98.437500%\n","Training - running batch 29/176 of epoch 11/20 - loss: 0.4145476520061493 - acc:  90.625000%\n","Training - running batch 30/176 of epoch 11/20 - loss: 0.3768274188041687 - acc:  91.406250%\n","Training - running batch 31/176 of epoch 11/20 - loss: 0.4402258098125458 - acc:  93.750000%\n","Training - running batch 32/176 of epoch 11/20 - loss: 0.2522377371788025 - acc:  94.531250%\n","Training - running batch 33/176 of epoch 11/20 - loss: 0.6902431845664978 - acc:  80.468750%\n","Training - running batch 34/176 of epoch 11/20 - loss: 0.7695490717887878 - acc:  75.781250%\n","Training - running batch 35/176 of epoch 11/20 - loss: 0.42138752341270447 - acc:  92.968750%\n","Training - running batch 36/176 of epoch 11/20 - loss: 0.5553989410400391 - acc:  85.937500%\n","Training - running batch 37/176 of epoch 11/20 - loss: 0.6435768008232117 - acc:  81.250000%\n","Training - running batch 38/176 of epoch 11/20 - loss: 0.34225979447364807 - acc:  91.406250%\n","Training - running batch 39/176 of epoch 11/20 - loss: 0.7218905091285706 - acc:  82.812500%\n","Training - running batch 40/176 of epoch 11/20 - loss: 0.2229834496974945 - acc:  95.312500%\n","Training - running batch 41/176 of epoch 11/20 - loss: 0.5945158004760742 - acc:  85.937500%\n","Training - running batch 42/176 of epoch 11/20 - loss: 0.5419650673866272 - acc:  89.062500%\n","Training - running batch 43/176 of epoch 11/20 - loss: 0.2267715483903885 - acc:  97.656250%\n","Training - running batch 44/176 of epoch 11/20 - loss: 0.21816405653953552 - acc:  97.656250%\n","Training - running batch 45/176 of epoch 11/20 - loss: 0.3139372766017914 - acc:  92.187500%\n","Training - running batch 46/176 of epoch 11/20 - loss: 0.46733757853507996 - acc:  89.843750%\n","Training - running batch 47/176 of epoch 11/20 - loss: 0.3329484462738037 - acc:  92.968750%\n","Training - running batch 48/176 of epoch 11/20 - loss: 0.7394821047782898 - acc:  76.562500%\n","Training - running batch 49/176 of epoch 11/20 - loss: 0.8513321280479431 - acc:  75.000000%\n","Training - running batch 50/176 of epoch 11/20 - loss: 0.7591432332992554 - acc:  75.781250%\n","Training - running batch 51/176 of epoch 11/20 - loss: 0.4901449382305145 - acc:  86.718750%\n","Training - running batch 52/176 of epoch 11/20 - loss: 0.2954712510108948 - acc:  96.875000%\n","Training - running batch 53/176 of epoch 11/20 - loss: 0.26384228467941284 - acc:  96.093750%\n","Training - running batch 54/176 of epoch 11/20 - loss: 0.5463125109672546 - acc:  87.500000%\n","Training - running batch 55/176 of epoch 11/20 - loss: 0.21369795501232147 - acc:  96.875000%\n","Training - running batch 56/176 of epoch 11/20 - loss: 0.22390006482601166 - acc:  96.093750%\n","Training - running batch 57/176 of epoch 11/20 - loss: 0.4300850033760071 - acc:  89.062500%\n","Training - running batch 58/176 of epoch 11/20 - loss: 0.8599681854248047 - acc:  74.218750%\n","Training - running batch 59/176 of epoch 11/20 - loss: 0.4030834436416626 - acc:  96.093750%\n","Training - running batch 60/176 of epoch 11/20 - loss: 0.2842305600643158 - acc:  96.093750%\n","Training - running batch 61/176 of epoch 11/20 - loss: 0.33621180057525635 - acc:  89.843750%\n","Training - running batch 62/176 of epoch 11/20 - loss: 0.4540882110595703 - acc:  89.062500%\n","Training - running batch 63/176 of epoch 11/20 - loss: 0.2826188802719116 - acc:  95.312500%\n","Training - running batch 64/176 of epoch 11/20 - loss: 0.48114579916000366 - acc:  87.500000%\n","Training - running batch 65/176 of epoch 11/20 - loss: 0.6936420798301697 - acc:  79.687500%\n","Training - running batch 66/176 of epoch 11/20 - loss: 0.26668602228164673 - acc:  98.437500%\n","Training - running batch 67/176 of epoch 11/20 - loss: 0.1993691474199295 - acc:  99.218750%\n","Training - running batch 68/176 of epoch 11/20 - loss: 0.5441266298294067 - acc:  85.937500%\n","Training - running batch 69/176 of epoch 11/20 - loss: 0.2862876057624817 - acc:  97.656250%\n","Training - running batch 70/176 of epoch 11/20 - loss: 0.29575297236442566 - acc:  92.187500%\n","Training - running batch 71/176 of epoch 11/20 - loss: 0.37479764223098755 - acc:  92.187500%\n","Training - running batch 72/176 of epoch 11/20 - loss: 0.38846707344055176 - acc:  92.968750%\n","Training - running batch 73/176 of epoch 11/20 - loss: 0.2583405375480652 - acc:  93.750000%\n","Training - running batch 74/176 of epoch 11/20 - loss: 0.42071354389190674 - acc:  91.406250%\n","Training - running batch 75/176 of epoch 11/20 - loss: 0.21475012600421906 - acc:  95.312500%\n","Training - running batch 76/176 of epoch 11/20 - loss: 0.5999366044998169 - acc:  82.031250%\n","Training - running batch 77/176 of epoch 11/20 - loss: 0.26351702213287354 - acc:  93.750000%\n","Training - running batch 78/176 of epoch 11/20 - loss: 0.7992608547210693 - acc:  82.031250%\n","Training - running batch 79/176 of epoch 11/20 - loss: 0.3721415102481842 - acc:  94.531250%\n","Training - running batch 80/176 of epoch 11/20 - loss: 0.6017729640007019 - acc:  85.156250%\n","Training - running batch 81/176 of epoch 11/20 - loss: 0.47940582036972046 - acc:  91.406250%\n","Training - running batch 82/176 of epoch 11/20 - loss: 0.2917766869068146 - acc:  95.312500%\n","Training - running batch 83/176 of epoch 11/20 - loss: 0.5553244352340698 - acc:  82.812500%\n","Training - running batch 84/176 of epoch 11/20 - loss: 0.23572534322738647 - acc:  94.531250%\n","Training - running batch 85/176 of epoch 11/20 - loss: 0.6264755725860596 - acc:  85.156250%\n","Training - running batch 86/176 of epoch 11/20 - loss: 0.19242355227470398 - acc:  97.656250%\n","Training - running batch 87/176 of epoch 11/20 - loss: 0.24483811855316162 - acc:  96.093750%\n","Training - running batch 88/176 of epoch 11/20 - loss: 0.2040339857339859 - acc:  97.656250%\n","Training - running batch 89/176 of epoch 11/20 - loss: 0.30556392669677734 - acc:  97.656250%\n","Training - running batch 90/176 of epoch 11/20 - loss: 0.5748834609985352 - acc:  82.812500%\n","Training - running batch 91/176 of epoch 11/20 - loss: 0.24533617496490479 - acc:  94.531250%\n","Training - running batch 92/176 of epoch 11/20 - loss: 0.45077455043792725 - acc:  89.062500%\n","Training - running batch 93/176 of epoch 11/20 - loss: 0.5045686364173889 - acc:  89.843750%\n","Training - running batch 94/176 of epoch 11/20 - loss: 0.2147524505853653 - acc:  98.437500%\n","Training - running batch 95/176 of epoch 11/20 - loss: 0.6139236688613892 - acc:  84.375000%\n","Training - running batch 96/176 of epoch 11/20 - loss: 0.5004473924636841 - acc:  90.625000%\n","Training - running batch 97/176 of epoch 11/20 - loss: 0.6876766681671143 - acc:  81.250000%\n","Training - running batch 98/176 of epoch 11/20 - loss: 0.7284013628959656 - acc:  76.562500%\n","Training - running batch 99/176 of epoch 11/20 - loss: 0.17194747924804688 - acc:  97.656250%\n","Training - running batch 100/176 of epoch 11/20 - loss: 0.4072785973548889 - acc:  94.531250%\n","Training - running batch 101/176 of epoch 11/20 - loss: 0.6219335794448853 - acc:  85.156250%\n","Training - running batch 102/176 of epoch 11/20 - loss: 0.5245036482810974 - acc:  88.281250%\n","Training - running batch 103/176 of epoch 11/20 - loss: 0.2756102383136749 - acc:  94.531250%\n","Training - running batch 104/176 of epoch 11/20 - loss: 0.3814104199409485 - acc:  94.531250%\n","Training - running batch 105/176 of epoch 11/20 - loss: 0.38242313265800476 - acc:  92.187500%\n","Training - running batch 106/176 of epoch 11/20 - loss: 0.6043170094490051 - acc:  82.031250%\n","Training - running batch 107/176 of epoch 11/20 - loss: 0.7222048044204712 - acc:  80.468750%\n","Training - running batch 108/176 of epoch 11/20 - loss: 0.26174893975257874 - acc:  96.093750%\n","Training - running batch 109/176 of epoch 11/20 - loss: 0.33097416162490845 - acc:  93.750000%\n","Training - running batch 110/176 of epoch 11/20 - loss: 0.3786827623844147 - acc:  94.531250%\n","Training - running batch 111/176 of epoch 11/20 - loss: 0.2759936451911926 - acc:  92.187500%\n","Training - running batch 112/176 of epoch 11/20 - loss: 0.23621726036071777 - acc:  96.875000%\n","Training - running batch 113/176 of epoch 11/20 - loss: 0.2350999265909195 - acc:  96.875000%\n","Training - running batch 114/176 of epoch 11/20 - loss: 0.7519083619117737 - acc:  78.906250%\n","Training - running batch 115/176 of epoch 11/20 - loss: 0.734660267829895 - acc:  77.343750%\n","Training - running batch 116/176 of epoch 11/20 - loss: 0.4135805368423462 - acc:  90.625000%\n","Training - running batch 117/176 of epoch 11/20 - loss: 0.3500261902809143 - acc:  92.187500%\n","Training - running batch 118/176 of epoch 11/20 - loss: 0.6062531471252441 - acc:  82.812500%\n","Training - running batch 119/176 of epoch 11/20 - loss: 0.8417689204216003 - acc:  78.125000%\n","Training - running batch 120/176 of epoch 11/20 - loss: 0.3304782807826996 - acc:  94.531250%\n","Training - running batch 121/176 of epoch 11/20 - loss: 0.2957598865032196 - acc:  93.750000%\n","Training - running batch 122/176 of epoch 11/20 - loss: 0.3300222158432007 - acc:  90.625000%\n","Training - running batch 123/176 of epoch 11/20 - loss: 0.3479984998703003 - acc:  89.843750%\n","Training - running batch 124/176 of epoch 11/20 - loss: 0.442585825920105 - acc:  90.625000%\n","Training - running batch 125/176 of epoch 11/20 - loss: 0.687788724899292 - acc:  80.468750%\n","Training - running batch 126/176 of epoch 11/20 - loss: 0.7101126909255981 - acc:  72.656250%\n","Training - running batch 127/176 of epoch 11/20 - loss: 0.39012211561203003 - acc:  94.531250%\n","Training - running batch 128/176 of epoch 11/20 - loss: 0.3396291136741638 - acc:  92.187500%\n","Training - running batch 129/176 of epoch 11/20 - loss: 0.3521425724029541 - acc:  92.968750%\n","Training - running batch 130/176 of epoch 11/20 - loss: 0.25364527106285095 - acc:  96.093750%\n","Training - running batch 131/176 of epoch 11/20 - loss: 0.8835698962211609 - acc:  74.218750%\n","Training - running batch 132/176 of epoch 11/20 - loss: 0.5566954612731934 - acc:  81.250000%\n","Training - running batch 133/176 of epoch 11/20 - loss: 0.21531178057193756 - acc:  96.093750%\n","Training - running batch 134/176 of epoch 11/20 - loss: 0.38990357518196106 - acc:  93.750000%\n","Training - running batch 135/176 of epoch 11/20 - loss: 0.761932373046875 - acc:  79.687500%\n","Training - running batch 136/176 of epoch 11/20 - loss: 0.6099553108215332 - acc:  82.812500%\n","Training - running batch 137/176 of epoch 11/20 - loss: 0.46780309081077576 - acc:  87.500000%\n","Training - running batch 138/176 of epoch 11/20 - loss: 0.6994098424911499 - acc:  85.156250%\n","Training - running batch 139/176 of epoch 11/20 - loss: 0.5526438355445862 - acc:  87.500000%\n","Training - running batch 140/176 of epoch 11/20 - loss: 0.5804958343505859 - acc:  86.718750%\n","Training - running batch 141/176 of epoch 11/20 - loss: 0.371753066778183 - acc:  92.968750%\n","Training - running batch 142/176 of epoch 11/20 - loss: 0.37024593353271484 - acc:  93.750000%\n","Training - running batch 143/176 of epoch 11/20 - loss: 0.5879380702972412 - acc:  85.937500%\n","Training - running batch 144/176 of epoch 11/20 - loss: 0.2813625633716583 - acc:  96.093750%\n","Training - running batch 145/176 of epoch 11/20 - loss: 0.5287978649139404 - acc:  85.937500%\n","Training - running batch 146/176 of epoch 11/20 - loss: 0.31490010023117065 - acc:  96.093750%\n","Training - running batch 147/176 of epoch 11/20 - loss: 0.3834823966026306 - acc:  94.531250%\n","Training - running batch 148/176 of epoch 11/20 - loss: 0.4037597179412842 - acc:  90.625000%\n","Training - running batch 149/176 of epoch 11/20 - loss: 0.7155958414077759 - acc:  78.906250%\n","Training - running batch 150/176 of epoch 11/20 - loss: 0.4548763930797577 - acc:  92.187500%\n","Training - running batch 151/176 of epoch 11/20 - loss: 0.45741477608680725 - acc:  90.625000%\n","Training - running batch 152/176 of epoch 11/20 - loss: 0.2959221601486206 - acc:  92.968750%\n","Training - running batch 153/176 of epoch 11/20 - loss: 0.6519794464111328 - acc:  84.375000%\n","Training - running batch 154/176 of epoch 11/20 - loss: 0.7271237969398499 - acc:  80.468750%\n","Training - running batch 155/176 of epoch 11/20 - loss: 0.6256026029586792 - acc:  81.250000%\n","Training - running batch 156/176 of epoch 11/20 - loss: 0.4933777451515198 - acc:  88.281250%\n","Training - running batch 157/176 of epoch 11/20 - loss: 0.5542814135551453 - acc:  82.031250%\n","Training - running batch 158/176 of epoch 11/20 - loss: 0.552125871181488 - acc:  85.937500%\n","Training - running batch 159/176 of epoch 11/20 - loss: 0.565125048160553 - acc:  86.718750%\n","Training - running batch 160/176 of epoch 11/20 - loss: 0.47350871562957764 - acc:  90.625000%\n","Training - running batch 161/176 of epoch 11/20 - loss: 0.29324328899383545 - acc:  93.750000%\n","Training - running batch 162/176 of epoch 11/20 - loss: 0.282352477312088 - acc:  95.312500%\n","Training - running batch 163/176 of epoch 11/20 - loss: 0.6372518539428711 - acc:  82.812500%\n","Training - running batch 164/176 of epoch 11/20 - loss: 0.7606149911880493 - acc:  80.468750%\n","Training - running batch 165/176 of epoch 11/20 - loss: 0.45202183723449707 - acc:  88.281250%\n","Training - running batch 166/176 of epoch 11/20 - loss: 0.3377954661846161 - acc:  93.750000%\n","Training - running batch 167/176 of epoch 11/20 - loss: 0.29381686449050903 - acc:  98.437500%\n","Training - running batch 168/176 of epoch 11/20 - loss: 0.5591464042663574 - acc:  87.500000%\n","Training - running batch 169/176 of epoch 11/20 - loss: 0.4042305648326874 - acc:  89.843750%\n","Training - running batch 170/176 of epoch 11/20 - loss: 0.26686713099479675 - acc:  94.531250%\n","Training - running batch 171/176 of epoch 11/20 - loss: 0.41589048504829407 - acc:  91.406250%\n","Training - running batch 172/176 of epoch 11/20 - loss: 0.29763486981391907 - acc:  94.531250%\n","Training - running batch 173/176 of epoch 11/20 - loss: 0.41439056396484375 - acc:  91.406250%\n","Training - running batch 174/176 of epoch 11/20 - loss: 0.677083432674408 - acc:  82.031250%\n","Training - running batch 175/176 of epoch 11/20 - loss: 0.44642966985702515 - acc:  91.935484%\n","Testing - acc:  0.664907%\n","Training - running batch 0/176 of epoch 12/20 - loss: 0.39136195182800293 - acc:  89.062500%\n","Training - running batch 1/176 of epoch 12/20 - loss: 0.3275529742240906 - acc:  92.187500%\n","Training - running batch 2/176 of epoch 12/20 - loss: 0.4764801859855652 - acc:  89.062500%\n","Training - running batch 3/176 of epoch 12/20 - loss: 0.4176398813724518 - acc:  90.625000%\n","Training - running batch 4/176 of epoch 12/20 - loss: 0.5050263404846191 - acc:  87.500000%\n","Training - running batch 5/176 of epoch 12/20 - loss: 0.3055467903614044 - acc:  94.531250%\n","Training - running batch 6/176 of epoch 12/20 - loss: 0.5043331384658813 - acc:  85.156250%\n","Training - running batch 7/176 of epoch 12/20 - loss: 0.42543426156044006 - acc:  91.406250%\n","Training - running batch 8/176 of epoch 12/20 - loss: 0.2292506992816925 - acc:  97.656250%\n","Training - running batch 9/176 of epoch 12/20 - loss: 0.43439462780952454 - acc:  90.625000%\n","Training - running batch 10/176 of epoch 12/20 - loss: 0.26291781663894653 - acc:  94.531250%\n","Training - running batch 11/176 of epoch 12/20 - loss: 0.4203273355960846 - acc:  88.281250%\n","Training - running batch 12/176 of epoch 12/20 - loss: 0.3852732181549072 - acc:  91.406250%\n","Training - running batch 13/176 of epoch 12/20 - loss: 0.2568802833557129 - acc:  96.875000%\n","Training - running batch 14/176 of epoch 12/20 - loss: 0.3377832770347595 - acc:  93.750000%\n","Training - running batch 15/176 of epoch 12/20 - loss: 0.2544434368610382 - acc:  95.312500%\n","Training - running batch 16/176 of epoch 12/20 - loss: 0.5125691890716553 - acc:  85.156250%\n","Training - running batch 17/176 of epoch 12/20 - loss: 0.2166929394006729 - acc:  96.875000%\n","Training - running batch 18/176 of epoch 12/20 - loss: 0.3649049401283264 - acc:  92.968750%\n","Training - running batch 19/176 of epoch 12/20 - loss: 0.4218234121799469 - acc:  86.718750%\n","Training - running batch 20/176 of epoch 12/20 - loss: 0.3323180079460144 - acc:  91.406250%\n","Training - running batch 21/176 of epoch 12/20 - loss: 0.2159162312746048 - acc:  96.875000%\n","Training - running batch 22/176 of epoch 12/20 - loss: 0.2548752427101135 - acc:  95.312500%\n","Training - running batch 23/176 of epoch 12/20 - loss: 0.5499606728553772 - acc:  89.843750%\n","Training - running batch 24/176 of epoch 12/20 - loss: 0.33890461921691895 - acc:  93.750000%\n","Training - running batch 25/176 of epoch 12/20 - loss: 0.35414114594459534 - acc:  92.968750%\n","Training - running batch 26/176 of epoch 12/20 - loss: 0.24337832629680634 - acc:  97.656250%\n","Training - running batch 27/176 of epoch 12/20 - loss: 0.27248451113700867 - acc:  98.437500%\n","Training - running batch 28/176 of epoch 12/20 - loss: 0.5448002219200134 - acc:  86.718750%\n","Training - running batch 29/176 of epoch 12/20 - loss: 0.36833569407463074 - acc:  92.968750%\n","Training - running batch 30/176 of epoch 12/20 - loss: 0.5950462818145752 - acc:  85.937500%\n","Training - running batch 31/176 of epoch 12/20 - loss: 0.2733207643032074 - acc:  93.750000%\n","Training - running batch 32/176 of epoch 12/20 - loss: 0.3886740803718567 - acc:  92.187500%\n","Training - running batch 33/176 of epoch 12/20 - loss: 0.254321426153183 - acc:  96.093750%\n","Training - running batch 34/176 of epoch 12/20 - loss: 0.21264679729938507 - acc:  96.875000%\n","Training - running batch 35/176 of epoch 12/20 - loss: 0.4713457226753235 - acc:  89.843750%\n","Training - running batch 36/176 of epoch 12/20 - loss: 0.21744851768016815 - acc:  96.875000%\n","Training - running batch 37/176 of epoch 12/20 - loss: 0.19314825534820557 - acc:  96.875000%\n","Training - running batch 38/176 of epoch 12/20 - loss: 0.1628359705209732 - acc:  98.437500%\n","Training - running batch 39/176 of epoch 12/20 - loss: 0.20968152582645416 - acc:  97.656250%\n","Training - running batch 40/176 of epoch 12/20 - loss: 0.5256986618041992 - acc:  89.062500%\n","Training - running batch 41/176 of epoch 12/20 - loss: 0.5775020122528076 - acc:  85.937500%\n","Training - running batch 42/176 of epoch 12/20 - loss: 0.46991094946861267 - acc:  89.062500%\n","Training - running batch 43/176 of epoch 12/20 - loss: 0.4287013113498688 - acc:  90.625000%\n","Training - running batch 44/176 of epoch 12/20 - loss: 0.28755712509155273 - acc:  95.312500%\n","Training - running batch 45/176 of epoch 12/20 - loss: 0.3296692967414856 - acc:  91.406250%\n","Training - running batch 46/176 of epoch 12/20 - loss: 0.2919982671737671 - acc:  92.968750%\n","Training - running batch 47/176 of epoch 12/20 - loss: 0.212656170129776 - acc:  94.531250%\n","Training - running batch 48/176 of epoch 12/20 - loss: 0.4196181893348694 - acc:  90.625000%\n","Training - running batch 49/176 of epoch 12/20 - loss: 0.2499275803565979 - acc:  95.312500%\n","Training - running batch 50/176 of epoch 12/20 - loss: 0.19898833334445953 - acc:  96.875000%\n","Training - running batch 51/176 of epoch 12/20 - loss: 0.23468667268753052 - acc:  95.312500%\n","Training - running batch 52/176 of epoch 12/20 - loss: 0.10386825352907181 - acc:  99.218750%\n","Training - running batch 53/176 of epoch 12/20 - loss: 0.2755025327205658 - acc:  96.093750%\n","Training - running batch 54/176 of epoch 12/20 - loss: 0.640177309513092 - acc:  84.375000%\n","Training - running batch 55/176 of epoch 12/20 - loss: 0.40096214413642883 - acc:  88.281250%\n","Training - running batch 56/176 of epoch 12/20 - loss: 0.44657155871391296 - acc:  86.718750%\n","Training - running batch 57/176 of epoch 12/20 - loss: 0.24200962483882904 - acc:  97.656250%\n","Training - running batch 58/176 of epoch 12/20 - loss: 0.24210461974143982 - acc:  98.437500%\n","Training - running batch 59/176 of epoch 12/20 - loss: 0.43939444422721863 - acc:  89.843750%\n","Training - running batch 60/176 of epoch 12/20 - loss: 0.17314743995666504 - acc:  96.875000%\n","Training - running batch 61/176 of epoch 12/20 - loss: 0.15470527112483978 - acc:  98.437500%\n","Training - running batch 62/176 of epoch 12/20 - loss: 0.5471292734146118 - acc:  85.937500%\n","Training - running batch 63/176 of epoch 12/20 - loss: 0.4728870391845703 - acc:  88.281250%\n","Training - running batch 64/176 of epoch 12/20 - loss: 0.174950510263443 - acc:  97.656250%\n","Training - running batch 65/176 of epoch 12/20 - loss: 0.23876039683818817 - acc:  98.437500%\n","Training - running batch 66/176 of epoch 12/20 - loss: 0.2924547493457794 - acc:  92.187500%\n","Training - running batch 67/176 of epoch 12/20 - loss: 0.5196120738983154 - acc:  87.500000%\n","Training - running batch 68/176 of epoch 12/20 - loss: 0.3346409499645233 - acc:  94.531250%\n","Training - running batch 69/176 of epoch 12/20 - loss: 0.23460465669631958 - acc:  94.531250%\n","Training - running batch 70/176 of epoch 12/20 - loss: 0.22014246881008148 - acc:  96.093750%\n","Training - running batch 71/176 of epoch 12/20 - loss: 0.22046539187431335 - acc:  96.875000%\n","Training - running batch 72/176 of epoch 12/20 - loss: 0.5414979457855225 - acc:  88.281250%\n","Training - running batch 73/176 of epoch 12/20 - loss: 0.2959666848182678 - acc:  92.187500%\n","Training - running batch 74/176 of epoch 12/20 - loss: 0.1943872720003128 - acc:  97.656250%\n","Training - running batch 75/176 of epoch 12/20 - loss: 0.38130298256874084 - acc:  92.187500%\n","Training - running batch 76/176 of epoch 12/20 - loss: 0.4903874397277832 - acc:  88.281250%\n","Training - running batch 77/176 of epoch 12/20 - loss: 0.3529863953590393 - acc:  93.750000%\n","Training - running batch 78/176 of epoch 12/20 - loss: 0.1797962337732315 - acc:  96.875000%\n","Training - running batch 79/176 of epoch 12/20 - loss: 0.28618323802948 - acc:  93.750000%\n","Training - running batch 80/176 of epoch 12/20 - loss: 0.23277553915977478 - acc:  97.656250%\n","Training - running batch 81/176 of epoch 12/20 - loss: 0.2661948502063751 - acc:  95.312500%\n","Training - running batch 82/176 of epoch 12/20 - loss: 0.23819252848625183 - acc:  94.531250%\n","Training - running batch 83/176 of epoch 12/20 - loss: 0.2635127007961273 - acc:  95.312500%\n","Training - running batch 84/176 of epoch 12/20 - loss: 0.11568829417228699 - acc:  99.218750%\n","Training - running batch 85/176 of epoch 12/20 - loss: 0.5619005560874939 - acc:  84.375000%\n","Training - running batch 86/176 of epoch 12/20 - loss: 0.3268851041793823 - acc:  95.312500%\n","Training - running batch 87/176 of epoch 12/20 - loss: 0.4360603094100952 - acc:  88.281250%\n","Training - running batch 88/176 of epoch 12/20 - loss: 0.18694926798343658 - acc:  96.875000%\n","Training - running batch 89/176 of epoch 12/20 - loss: 0.16794507205486298 - acc:  97.656250%\n","Training - running batch 90/176 of epoch 12/20 - loss: 0.170613095164299 - acc:  99.218750%\n","Training - running batch 91/176 of epoch 12/20 - loss: 0.23513944447040558 - acc:  95.312500%\n","Training - running batch 92/176 of epoch 12/20 - loss: 0.4528902769088745 - acc:  89.843750%\n","Training - running batch 93/176 of epoch 12/20 - loss: 0.1316835731267929 - acc:  99.218750%\n","Training - running batch 94/176 of epoch 12/20 - loss: 0.16550728678703308 - acc:  96.093750%\n","Training - running batch 95/176 of epoch 12/20 - loss: 0.2940680980682373 - acc:  91.406250%\n","Training - running batch 96/176 of epoch 12/20 - loss: 0.2448355257511139 - acc:  95.312500%\n","Training - running batch 97/176 of epoch 12/20 - loss: 0.24911098182201385 - acc:  97.656250%\n","Training - running batch 98/176 of epoch 12/20 - loss: 0.38661137223243713 - acc:  91.406250%\n","Training - running batch 99/176 of epoch 12/20 - loss: 0.3714742660522461 - acc:  93.750000%\n","Training - running batch 100/176 of epoch 12/20 - loss: 0.24107283353805542 - acc:  96.093750%\n","Training - running batch 101/176 of epoch 12/20 - loss: 0.31436070799827576 - acc:  96.875000%\n","Training - running batch 102/176 of epoch 12/20 - loss: 0.17870314419269562 - acc:  98.437500%\n","Training - running batch 103/176 of epoch 12/20 - loss: 0.33177119493484497 - acc:  92.968750%\n","Training - running batch 104/176 of epoch 12/20 - loss: 0.5004546642303467 - acc:  85.937500%\n","Training - running batch 105/176 of epoch 12/20 - loss: 0.506517231464386 - acc:  86.718750%\n","Training - running batch 106/176 of epoch 12/20 - loss: 0.21474458277225494 - acc:  96.093750%\n","Training - running batch 107/176 of epoch 12/20 - loss: 0.5699211359024048 - acc:  88.281250%\n","Training - running batch 108/176 of epoch 12/20 - loss: 0.4555922746658325 - acc:  88.281250%\n","Training - running batch 109/176 of epoch 12/20 - loss: 0.1523061841726303 - acc:  97.656250%\n","Training - running batch 110/176 of epoch 12/20 - loss: 0.2720879912376404 - acc:  94.531250%\n","Training - running batch 111/176 of epoch 12/20 - loss: 0.18871143460273743 - acc:  97.656250%\n","Training - running batch 112/176 of epoch 12/20 - loss: 0.1747456043958664 - acc:  96.093750%\n","Training - running batch 113/176 of epoch 12/20 - loss: 0.2193012237548828 - acc:  94.531250%\n","Training - running batch 114/176 of epoch 12/20 - loss: 0.25895214080810547 - acc:  95.312500%\n","Training - running batch 115/176 of epoch 12/20 - loss: 0.283921480178833 - acc:  96.093750%\n","Training - running batch 116/176 of epoch 12/20 - loss: 0.3106215298175812 - acc:  96.875000%\n","Training - running batch 117/176 of epoch 12/20 - loss: 0.6975418925285339 - acc:  79.687500%\n","Training - running batch 118/176 of epoch 12/20 - loss: 0.31388479471206665 - acc:  91.406250%\n","Training - running batch 119/176 of epoch 12/20 - loss: 0.2270527333021164 - acc:  93.750000%\n","Training - running batch 120/176 of epoch 12/20 - loss: 0.2957451343536377 - acc:  96.875000%\n","Training - running batch 121/176 of epoch 12/20 - loss: 0.1455237716436386 - acc:  97.656250%\n","Training - running batch 122/176 of epoch 12/20 - loss: 0.600896418094635 - acc:  82.812500%\n","Training - running batch 123/176 of epoch 12/20 - loss: 0.28503701090812683 - acc:  93.750000%\n","Training - running batch 124/176 of epoch 12/20 - loss: 0.28396931290626526 - acc:  94.531250%\n","Training - running batch 125/176 of epoch 12/20 - loss: 0.20390594005584717 - acc:  94.531250%\n","Training - running batch 126/176 of epoch 12/20 - loss: 0.30075833201408386 - acc:  92.187500%\n","Training - running batch 127/176 of epoch 12/20 - loss: 0.4415145516395569 - acc:  89.062500%\n","Training - running batch 128/176 of epoch 12/20 - loss: 0.5493265390396118 - acc:  85.156250%\n","Training - running batch 129/176 of epoch 12/20 - loss: 0.5600367784500122 - acc:  83.593750%\n","Training - running batch 130/176 of epoch 12/20 - loss: 0.20572762191295624 - acc:  96.093750%\n","Training - running batch 131/176 of epoch 12/20 - loss: 0.5305845141410828 - acc:  89.843750%\n","Training - running batch 132/176 of epoch 12/20 - loss: 0.2624245285987854 - acc:  94.531250%\n","Training - running batch 133/176 of epoch 12/20 - loss: 0.4756213128566742 - acc:  90.625000%\n","Training - running batch 134/176 of epoch 12/20 - loss: 0.2050638049840927 - acc:  96.093750%\n","Training - running batch 135/176 of epoch 12/20 - loss: 0.5682910680770874 - acc:  85.156250%\n","Training - running batch 136/176 of epoch 12/20 - loss: 0.18241968750953674 - acc:  96.875000%\n","Training - running batch 137/176 of epoch 12/20 - loss: 0.4622540771961212 - acc:  84.375000%\n","Training - running batch 138/176 of epoch 12/20 - loss: 0.21431520581245422 - acc:  95.312500%\n","Training - running batch 139/176 of epoch 12/20 - loss: 0.46206414699554443 - acc:  86.718750%\n","Training - running batch 140/176 of epoch 12/20 - loss: 0.48629677295684814 - acc:  87.500000%\n","Training - running batch 141/176 of epoch 12/20 - loss: 0.32226064801216125 - acc:  92.968750%\n","Training - running batch 142/176 of epoch 12/20 - loss: 0.2753809690475464 - acc:  96.875000%\n","Training - running batch 143/176 of epoch 12/20 - loss: 0.3297930955886841 - acc:  92.968750%\n","Training - running batch 144/176 of epoch 12/20 - loss: 0.45983266830444336 - acc:  87.500000%\n","Training - running batch 145/176 of epoch 12/20 - loss: 0.18217062950134277 - acc:  99.218750%\n","Training - running batch 146/176 of epoch 12/20 - loss: 0.5138485431671143 - acc:  83.593750%\n","Training - running batch 147/176 of epoch 12/20 - loss: 0.19378799200057983 - acc:  96.093750%\n","Training - running batch 148/176 of epoch 12/20 - loss: 0.5461801886558533 - acc:  84.375000%\n","Training - running batch 149/176 of epoch 12/20 - loss: 0.3175240457057953 - acc:  95.312500%\n","Training - running batch 150/176 of epoch 12/20 - loss: 0.4052179157733917 - acc:  90.625000%\n","Training - running batch 151/176 of epoch 12/20 - loss: 0.24074223637580872 - acc:  95.312500%\n","Training - running batch 152/176 of epoch 12/20 - loss: 0.5082407593727112 - acc:  85.937500%\n","Training - running batch 153/176 of epoch 12/20 - loss: 0.46564456820487976 - acc:  86.718750%\n","Training - running batch 154/176 of epoch 12/20 - loss: 0.24482163786888123 - acc:  93.750000%\n","Training - running batch 155/176 of epoch 12/20 - loss: 0.6985412836074829 - acc:  82.031250%\n","Training - running batch 156/176 of epoch 12/20 - loss: 0.33979764580726624 - acc:  95.312500%\n","Training - running batch 157/176 of epoch 12/20 - loss: 0.3417380154132843 - acc:  92.968750%\n","Training - running batch 158/176 of epoch 12/20 - loss: 0.24322770535945892 - acc:  95.312500%\n","Training - running batch 159/176 of epoch 12/20 - loss: 0.377013236284256 - acc:  92.187500%\n","Training - running batch 160/176 of epoch 12/20 - loss: 0.5143738389015198 - acc:  82.812500%\n","Training - running batch 161/176 of epoch 12/20 - loss: 0.24676205217838287 - acc:  93.750000%\n","Training - running batch 162/176 of epoch 12/20 - loss: 0.31738582253456116 - acc:  97.656250%\n","Training - running batch 163/176 of epoch 12/20 - loss: 0.2751229703426361 - acc:  96.875000%\n","Training - running batch 164/176 of epoch 12/20 - loss: 0.20423445105552673 - acc:  95.312500%\n","Training - running batch 165/176 of epoch 12/20 - loss: 0.3122181296348572 - acc:  91.406250%\n","Training - running batch 166/176 of epoch 12/20 - loss: 0.5949565768241882 - acc:  86.718750%\n","Training - running batch 167/176 of epoch 12/20 - loss: 0.4358048141002655 - acc:  86.718750%\n","Training - running batch 168/176 of epoch 12/20 - loss: 0.44368448853492737 - acc:  88.281250%\n","Training - running batch 169/176 of epoch 12/20 - loss: 0.261257141828537 - acc:  96.875000%\n","Training - running batch 170/176 of epoch 12/20 - loss: 0.28340205550193787 - acc:  92.187500%\n","Training - running batch 171/176 of epoch 12/20 - loss: 0.29620638489723206 - acc:  93.750000%\n","Training - running batch 172/176 of epoch 12/20 - loss: 0.474556565284729 - acc:  88.281250%\n","Training - running batch 173/176 of epoch 12/20 - loss: 0.5885156393051147 - acc:  85.156250%\n","Training - running batch 174/176 of epoch 12/20 - loss: 0.1727728545665741 - acc:  95.312500%\n","Training - running batch 175/176 of epoch 12/20 - loss: 0.45907214283943176 - acc:  93.548387%\n","Testing - acc:  0.659628%\n","Training - running batch 0/176 of epoch 13/20 - loss: 0.17948375642299652 - acc:  96.875000%\n","Training - running batch 1/176 of epoch 13/20 - loss: 0.21795383095741272 - acc:  96.093750%\n","Training - running batch 2/176 of epoch 13/20 - loss: 0.17763228714466095 - acc:  98.437500%\n","Training - running batch 3/176 of epoch 13/20 - loss: 0.20482060313224792 - acc:  96.093750%\n","Training - running batch 4/176 of epoch 13/20 - loss: 0.2532431185245514 - acc:  93.750000%\n","Training - running batch 5/176 of epoch 13/20 - loss: 0.4262939691543579 - acc:  92.968750%\n","Training - running batch 6/176 of epoch 13/20 - loss: 0.4276529550552368 - acc:  90.625000%\n","Training - running batch 7/176 of epoch 13/20 - loss: 0.4386158287525177 - acc:  87.500000%\n","Training - running batch 8/176 of epoch 13/20 - loss: 0.15550920367240906 - acc:  96.875000%\n","Training - running batch 9/176 of epoch 13/20 - loss: 0.4023236334323883 - acc:  89.062500%\n","Training - running batch 10/176 of epoch 13/20 - loss: 0.2531129717826843 - acc:  95.312500%\n","Training - running batch 11/176 of epoch 13/20 - loss: 0.37946245074272156 - acc:  92.968750%\n","Training - running batch 12/176 of epoch 13/20 - loss: 0.4819796681404114 - acc:  86.718750%\n","Training - running batch 13/176 of epoch 13/20 - loss: 0.17051374912261963 - acc:  96.875000%\n","Training - running batch 14/176 of epoch 13/20 - loss: 0.33648109436035156 - acc:  91.406250%\n","Training - running batch 15/176 of epoch 13/20 - loss: 0.15581265091896057 - acc:  98.437500%\n","Training - running batch 16/176 of epoch 13/20 - loss: 0.19416387379169464 - acc:  94.531250%\n","Training - running batch 17/176 of epoch 13/20 - loss: 0.3487873077392578 - acc:  92.968750%\n","Training - running batch 18/176 of epoch 13/20 - loss: 0.3259115517139435 - acc:  92.968750%\n","Training - running batch 19/176 of epoch 13/20 - loss: 0.23773303627967834 - acc:  95.312500%\n","Training - running batch 20/176 of epoch 13/20 - loss: 0.5065115690231323 - acc:  88.281250%\n","Training - running batch 21/176 of epoch 13/20 - loss: 0.3569588363170624 - acc:  94.531250%\n","Training - running batch 22/176 of epoch 13/20 - loss: 0.28726422786712646 - acc:  93.750000%\n","Training - running batch 23/176 of epoch 13/20 - loss: 0.5041704177856445 - acc:  85.156250%\n","Training - running batch 24/176 of epoch 13/20 - loss: 0.5162874460220337 - acc:  86.718750%\n","Training - running batch 25/176 of epoch 13/20 - loss: 0.18819834291934967 - acc:  96.093750%\n","Training - running batch 26/176 of epoch 13/20 - loss: 0.609445333480835 - acc:  86.718750%\n","Training - running batch 27/176 of epoch 13/20 - loss: 0.2700966000556946 - acc:  93.750000%\n","Training - running batch 28/176 of epoch 13/20 - loss: 0.3452916443347931 - acc:  94.531250%\n","Training - running batch 29/176 of epoch 13/20 - loss: 0.16543057560920715 - acc:  98.437500%\n","Training - running batch 30/176 of epoch 13/20 - loss: 0.3633420765399933 - acc:  92.968750%\n","Training - running batch 31/176 of epoch 13/20 - loss: 0.3665226995944977 - acc:  92.187500%\n","Training - running batch 32/176 of epoch 13/20 - loss: 0.42628929018974304 - acc:  89.062500%\n","Training - running batch 33/176 of epoch 13/20 - loss: 0.17503592371940613 - acc:  95.312500%\n","Training - running batch 34/176 of epoch 13/20 - loss: 0.33701497316360474 - acc:  92.187500%\n","Training - running batch 35/176 of epoch 13/20 - loss: 0.5605543255805969 - acc:  81.250000%\n","Training - running batch 36/176 of epoch 13/20 - loss: 0.1821412742137909 - acc:  95.312500%\n","Training - running batch 37/176 of epoch 13/20 - loss: 0.15385422110557556 - acc:  96.875000%\n","Training - running batch 38/176 of epoch 13/20 - loss: 0.429555207490921 - acc:  87.500000%\n","Training - running batch 39/176 of epoch 13/20 - loss: 0.38914191722869873 - acc:  91.406250%\n","Training - running batch 40/176 of epoch 13/20 - loss: 0.42852166295051575 - acc:  90.625000%\n","Training - running batch 41/176 of epoch 13/20 - loss: 0.4140700101852417 - acc:  89.062500%\n","Training - running batch 42/176 of epoch 13/20 - loss: 0.42420631647109985 - acc:  86.718750%\n","Training - running batch 43/176 of epoch 13/20 - loss: 0.3398975133895874 - acc:  91.406250%\n","Training - running batch 44/176 of epoch 13/20 - loss: 0.534976065158844 - acc:  83.593750%\n","Training - running batch 45/176 of epoch 13/20 - loss: 0.3482475280761719 - acc:  95.312500%\n","Training - running batch 46/176 of epoch 13/20 - loss: 0.16818107664585114 - acc:  98.437500%\n","Training - running batch 47/176 of epoch 13/20 - loss: 0.19498561322689056 - acc:  98.437500%\n","Training - running batch 48/176 of epoch 13/20 - loss: 0.21323946118354797 - acc:  97.656250%\n","Training - running batch 49/176 of epoch 13/20 - loss: 0.3985584080219269 - acc:  90.625000%\n","Training - running batch 50/176 of epoch 13/20 - loss: 0.42747968435287476 - acc:  91.406250%\n","Training - running batch 51/176 of epoch 13/20 - loss: 0.15445618331432343 - acc:  99.218750%\n","Training - running batch 52/176 of epoch 13/20 - loss: 0.2131088376045227 - acc:  97.656250%\n","Training - running batch 53/176 of epoch 13/20 - loss: 0.15632367134094238 - acc:  97.656250%\n","Training - running batch 54/176 of epoch 13/20 - loss: 0.20293216407299042 - acc:  96.875000%\n","Training - running batch 55/176 of epoch 13/20 - loss: 0.313930481672287 - acc:  96.093750%\n","Training - running batch 56/176 of epoch 13/20 - loss: 0.36608654260635376 - acc:  91.406250%\n","Training - running batch 57/176 of epoch 13/20 - loss: 0.20943929255008698 - acc:  98.437500%\n","Training - running batch 58/176 of epoch 13/20 - loss: 0.528599739074707 - acc:  85.156250%\n","Training - running batch 59/176 of epoch 13/20 - loss: 0.1474432647228241 - acc:  97.656250%\n","Training - running batch 60/176 of epoch 13/20 - loss: 0.38561007380485535 - acc:  93.750000%\n","Training - running batch 61/176 of epoch 13/20 - loss: 0.23761995136737823 - acc:  96.093750%\n","Training - running batch 62/176 of epoch 13/20 - loss: 0.1524651199579239 - acc:  99.218750%\n","Training - running batch 63/176 of epoch 13/20 - loss: 0.4347350001335144 - acc:  89.843750%\n","Training - running batch 64/176 of epoch 13/20 - loss: 0.241660937666893 - acc:  96.093750%\n","Training - running batch 65/176 of epoch 13/20 - loss: 0.26975494623184204 - acc:  94.531250%\n","Training - running batch 66/176 of epoch 13/20 - loss: 0.22500421106815338 - acc:  94.531250%\n","Training - running batch 67/176 of epoch 13/20 - loss: 0.35089200735092163 - acc:  88.281250%\n","Training - running batch 68/176 of epoch 13/20 - loss: 0.33112797141075134 - acc:  89.843750%\n","Training - running batch 69/176 of epoch 13/20 - loss: 0.31270909309387207 - acc:  95.312500%\n","Training - running batch 70/176 of epoch 13/20 - loss: 0.33018621802330017 - acc:  92.187500%\n","Training - running batch 71/176 of epoch 13/20 - loss: 0.17405451834201813 - acc:  96.875000%\n","Training - running batch 72/176 of epoch 13/20 - loss: 0.2039148211479187 - acc:  96.093750%\n","Training - running batch 73/176 of epoch 13/20 - loss: 0.2788413166999817 - acc:  94.531250%\n","Training - running batch 74/176 of epoch 13/20 - loss: 0.37751519680023193 - acc:  90.625000%\n","Training - running batch 75/176 of epoch 13/20 - loss: 0.18461874127388 - acc:  96.093750%\n","Training - running batch 76/176 of epoch 13/20 - loss: 0.3779119551181793 - acc:  93.750000%\n","Training - running batch 77/176 of epoch 13/20 - loss: 0.29393118619918823 - acc:  93.750000%\n","Training - running batch 78/176 of epoch 13/20 - loss: 0.35268110036849976 - acc:  91.406250%\n","Training - running batch 79/176 of epoch 13/20 - loss: 0.18597285449504852 - acc:  96.093750%\n","Training - running batch 80/176 of epoch 13/20 - loss: 0.21917782723903656 - acc:  96.875000%\n","Training - running batch 81/176 of epoch 13/20 - loss: 0.35101738572120667 - acc:  90.625000%\n","Training - running batch 82/176 of epoch 13/20 - loss: 0.32825127243995667 - acc:  96.093750%\n","Training - running batch 83/176 of epoch 13/20 - loss: 0.32821547985076904 - acc:  91.406250%\n","Training - running batch 84/176 of epoch 13/20 - loss: 0.21501928567886353 - acc:  96.093750%\n","Training - running batch 85/176 of epoch 13/20 - loss: 0.20336444675922394 - acc:  95.312500%\n","Training - running batch 86/176 of epoch 13/20 - loss: 0.23016585409641266 - acc:  94.531250%\n","Training - running batch 87/176 of epoch 13/20 - loss: 0.3890248239040375 - acc:  89.843750%\n","Training - running batch 88/176 of epoch 13/20 - loss: 0.37011218070983887 - acc:  89.062500%\n","Training - running batch 89/176 of epoch 13/20 - loss: 0.35427388548851013 - acc:  88.281250%\n","Training - running batch 90/176 of epoch 13/20 - loss: 0.3224135935306549 - acc:  92.187500%\n","Training - running batch 91/176 of epoch 13/20 - loss: 0.28140535950660706 - acc:  93.750000%\n","Training - running batch 92/176 of epoch 13/20 - loss: 0.31156685948371887 - acc:  92.187500%\n","Training - running batch 93/176 of epoch 13/20 - loss: 0.30723071098327637 - acc:  94.531250%\n","Training - running batch 94/176 of epoch 13/20 - loss: 0.28415846824645996 - acc:  94.531250%\n","Training - running batch 95/176 of epoch 13/20 - loss: 0.4255605936050415 - acc:  90.625000%\n","Training - running batch 96/176 of epoch 13/20 - loss: 0.33712658286094666 - acc:  91.406250%\n","Training - running batch 97/176 of epoch 13/20 - loss: 0.28944188356399536 - acc:  92.968750%\n","Training - running batch 98/176 of epoch 13/20 - loss: 0.20687571167945862 - acc:  96.875000%\n","Training - running batch 99/176 of epoch 13/20 - loss: 0.5113426446914673 - acc:  85.937500%\n","Training - running batch 100/176 of epoch 13/20 - loss: 0.3201335072517395 - acc:  94.531250%\n","Training - running batch 101/176 of epoch 13/20 - loss: 0.41857144236564636 - acc:  89.843750%\n","Training - running batch 102/176 of epoch 13/20 - loss: 0.2990797162055969 - acc:  95.312500%\n","Training - running batch 103/176 of epoch 13/20 - loss: 0.21885421872138977 - acc:  96.875000%\n","Training - running batch 104/176 of epoch 13/20 - loss: 0.1472945511341095 - acc:  96.875000%\n","Training - running batch 105/176 of epoch 13/20 - loss: 0.2608988285064697 - acc:  96.093750%\n","Training - running batch 106/176 of epoch 13/20 - loss: 0.1774733066558838 - acc:  96.875000%\n","Training - running batch 107/176 of epoch 13/20 - loss: 0.4438888728618622 - acc:  85.937500%\n","Training - running batch 108/176 of epoch 13/20 - loss: 0.5453798770904541 - acc:  83.593750%\n","Training - running batch 109/176 of epoch 13/20 - loss: 0.5171879529953003 - acc:  82.031250%\n","Training - running batch 110/176 of epoch 13/20 - loss: 0.3500601649284363 - acc:  92.968750%\n","Training - running batch 111/176 of epoch 13/20 - loss: 0.11348026990890503 - acc:  99.218750%\n","Training - running batch 112/176 of epoch 13/20 - loss: 0.18647556006908417 - acc:  98.437500%\n","Training - running batch 113/176 of epoch 13/20 - loss: 0.16231834888458252 - acc:  98.437500%\n","Training - running batch 114/176 of epoch 13/20 - loss: 0.17518432438373566 - acc:  97.656250%\n","Training - running batch 115/176 of epoch 13/20 - loss: 0.4439845085144043 - acc:  88.281250%\n","Training - running batch 116/176 of epoch 13/20 - loss: 0.15666356682777405 - acc:  99.218750%\n","Training - running batch 117/176 of epoch 13/20 - loss: 0.16176153719425201 - acc:  98.437500%\n","Training - running batch 118/176 of epoch 13/20 - loss: 0.35899707674980164 - acc:  90.625000%\n","Training - running batch 119/176 of epoch 13/20 - loss: 0.185157909989357 - acc:  96.875000%\n","Training - running batch 120/176 of epoch 13/20 - loss: 0.1712011694908142 - acc:  97.656250%\n","Training - running batch 121/176 of epoch 13/20 - loss: 0.3145565092563629 - acc:  94.531250%\n","Training - running batch 122/176 of epoch 13/20 - loss: 0.15854202210903168 - acc:  97.656250%\n","Training - running batch 123/176 of epoch 13/20 - loss: 0.2749485671520233 - acc:  91.406250%\n","Training - running batch 124/176 of epoch 13/20 - loss: 0.15040871500968933 - acc:  98.437500%\n","Training - running batch 125/176 of epoch 13/20 - loss: 0.29596227407455444 - acc:  95.312500%\n","Training - running batch 126/176 of epoch 13/20 - loss: 0.3324911594390869 - acc:  90.625000%\n","Training - running batch 127/176 of epoch 13/20 - loss: 0.2870364189147949 - acc:  92.968750%\n","Training - running batch 128/176 of epoch 13/20 - loss: 0.5111081600189209 - acc:  88.281250%\n","Training - running batch 129/176 of epoch 13/20 - loss: 0.21968628466129303 - acc:  95.312500%\n","Training - running batch 130/176 of epoch 13/20 - loss: 0.36786895990371704 - acc:  91.406250%\n","Training - running batch 131/176 of epoch 13/20 - loss: 0.4284626543521881 - acc:  90.625000%\n","Training - running batch 132/176 of epoch 13/20 - loss: 0.5639249682426453 - acc:  86.718750%\n","Training - running batch 133/176 of epoch 13/20 - loss: 0.19498960673809052 - acc:  96.875000%\n","Training - running batch 134/176 of epoch 13/20 - loss: 0.6714798212051392 - acc:  85.156250%\n","Training - running batch 135/176 of epoch 13/20 - loss: 0.3840785622596741 - acc:  91.406250%\n","Training - running batch 136/176 of epoch 13/20 - loss: 0.24752701818943024 - acc:  95.312500%\n","Training - running batch 137/176 of epoch 13/20 - loss: 0.49918854236602783 - acc:  87.500000%\n","Training - running batch 138/176 of epoch 13/20 - loss: 0.25915607810020447 - acc:  94.531250%\n","Training - running batch 139/176 of epoch 13/20 - loss: 0.24754191935062408 - acc:  95.312500%\n","Training - running batch 140/176 of epoch 13/20 - loss: 0.3762694299221039 - acc:  92.187500%\n","Training - running batch 141/176 of epoch 13/20 - loss: 0.33911654353141785 - acc:  92.187500%\n","Training - running batch 142/176 of epoch 13/20 - loss: 0.3996374309062958 - acc:  87.500000%\n","Training - running batch 143/176 of epoch 13/20 - loss: 0.1615590900182724 - acc:  97.656250%\n","Training - running batch 144/176 of epoch 13/20 - loss: 0.33036914467811584 - acc:  92.187500%\n","Training - running batch 145/176 of epoch 13/20 - loss: 0.25327688455581665 - acc:  96.093750%\n","Training - running batch 146/176 of epoch 13/20 - loss: 0.4608668386936188 - acc:  89.843750%\n","Training - running batch 147/176 of epoch 13/20 - loss: 0.3638318181037903 - acc:  90.625000%\n","Training - running batch 148/176 of epoch 13/20 - loss: 0.1338624805212021 - acc:  97.656250%\n","Training - running batch 149/176 of epoch 13/20 - loss: 0.2627077102661133 - acc:  96.093750%\n","Training - running batch 150/176 of epoch 13/20 - loss: 0.46951040625572205 - acc:  89.843750%\n","Training - running batch 151/176 of epoch 13/20 - loss: 0.343619704246521 - acc:  91.406250%\n","Training - running batch 152/176 of epoch 13/20 - loss: 0.19287431240081787 - acc:  96.093750%\n","Training - running batch 153/176 of epoch 13/20 - loss: 0.4606850743293762 - acc:  89.062500%\n","Training - running batch 154/176 of epoch 13/20 - loss: 0.46936747431755066 - acc:  86.718750%\n","Training - running batch 155/176 of epoch 13/20 - loss: 0.31404659152030945 - acc:  93.750000%\n","Training - running batch 156/176 of epoch 13/20 - loss: 0.20479191839694977 - acc:  96.875000%\n","Training - running batch 157/176 of epoch 13/20 - loss: 0.221041738986969 - acc:  97.656250%\n","Training - running batch 158/176 of epoch 13/20 - loss: 0.3739250898361206 - acc:  89.062500%\n","Training - running batch 159/176 of epoch 13/20 - loss: 0.2159915268421173 - acc:  96.093750%\n","Training - running batch 160/176 of epoch 13/20 - loss: 0.4630529284477234 - acc:  89.843750%\n","Training - running batch 161/176 of epoch 13/20 - loss: 0.4839102625846863 - acc:  89.843750%\n","Training - running batch 162/176 of epoch 13/20 - loss: 0.21669955551624298 - acc:  97.656250%\n","Training - running batch 163/176 of epoch 13/20 - loss: 0.3721816837787628 - acc:  89.843750%\n","Training - running batch 164/176 of epoch 13/20 - loss: 0.24726489186286926 - acc:  92.187500%\n","Training - running batch 165/176 of epoch 13/20 - loss: 0.5053927898406982 - acc:  86.718750%\n","Training - running batch 166/176 of epoch 13/20 - loss: 0.3872831165790558 - acc:  92.968750%\n","Training - running batch 167/176 of epoch 13/20 - loss: 0.3423866033554077 - acc:  89.062500%\n","Training - running batch 168/176 of epoch 13/20 - loss: 0.559664785861969 - acc:  85.937500%\n","Training - running batch 169/176 of epoch 13/20 - loss: 0.4898650348186493 - acc:  85.156250%\n","Training - running batch 170/176 of epoch 13/20 - loss: 0.11781928688287735 - acc:  98.437500%\n","Training - running batch 171/176 of epoch 13/20 - loss: 0.339328795671463 - acc:  92.187500%\n","Training - running batch 172/176 of epoch 13/20 - loss: 0.4692588150501251 - acc:  84.375000%\n","Training - running batch 173/176 of epoch 13/20 - loss: 0.22675342857837677 - acc:  94.531250%\n","Training - running batch 174/176 of epoch 13/20 - loss: 0.41087865829467773 - acc:  91.406250%\n","Training - running batch 175/176 of epoch 13/20 - loss: 0.2289389967918396 - acc:  91.935484%\n","Testing - acc:  0.678678%\n","Training - running batch 0/176 of epoch 14/20 - loss: 0.3465098440647125 - acc:  90.625000%\n","Training - running batch 1/176 of epoch 14/20 - loss: 0.19580940902233124 - acc:  97.656250%\n","Training - running batch 2/176 of epoch 14/20 - loss: 0.3180488348007202 - acc:  92.968750%\n","Training - running batch 3/176 of epoch 14/20 - loss: 0.3389054238796234 - acc:  92.187500%\n","Training - running batch 4/176 of epoch 14/20 - loss: 0.15720871090888977 - acc:  99.218750%\n","Training - running batch 5/176 of epoch 14/20 - loss: 0.36283671855926514 - acc:  93.750000%\n","Training - running batch 6/176 of epoch 14/20 - loss: 0.16357538104057312 - acc:  99.218750%\n","Training - running batch 7/176 of epoch 14/20 - loss: 0.3409547209739685 - acc:  93.750000%\n","Training - running batch 8/176 of epoch 14/20 - loss: 0.2187565267086029 - acc:  96.093750%\n","Training - running batch 9/176 of epoch 14/20 - loss: 0.28343665599823 - acc:  96.875000%\n","Training - running batch 10/176 of epoch 14/20 - loss: 0.19661809504032135 - acc:  96.093750%\n","Training - running batch 11/176 of epoch 14/20 - loss: 0.41298210620880127 - acc:  88.281250%\n","Training - running batch 12/176 of epoch 14/20 - loss: 0.2973639667034149 - acc:  92.968750%\n","Training - running batch 13/176 of epoch 14/20 - loss: 0.35116252303123474 - acc:  92.187500%\n","Training - running batch 14/176 of epoch 14/20 - loss: 0.27654916048049927 - acc:  92.187500%\n","Training - running batch 15/176 of epoch 14/20 - loss: 0.27517321705818176 - acc:  94.531250%\n","Training - running batch 16/176 of epoch 14/20 - loss: 0.47144630551338196 - acc:  89.062500%\n","Training - running batch 17/176 of epoch 14/20 - loss: 0.32870131731033325 - acc:  94.531250%\n","Training - running batch 18/176 of epoch 14/20 - loss: 0.22918649017810822 - acc:  96.093750%\n","Training - running batch 19/176 of epoch 14/20 - loss: 0.21024686098098755 - acc:  95.312500%\n","Training - running batch 20/176 of epoch 14/20 - loss: 0.2799471318721771 - acc:  92.187500%\n","Training - running batch 21/176 of epoch 14/20 - loss: 0.21396225690841675 - acc:  95.312500%\n","Training - running batch 22/176 of epoch 14/20 - loss: 0.3376833498477936 - acc:  92.187500%\n","Training - running batch 23/176 of epoch 14/20 - loss: 0.20334576070308685 - acc:  96.093750%\n","Training - running batch 24/176 of epoch 14/20 - loss: 0.15995639562606812 - acc:  98.437500%\n","Training - running batch 25/176 of epoch 14/20 - loss: 0.256965309381485 - acc:  96.093750%\n","Training - running batch 26/176 of epoch 14/20 - loss: 0.1914212703704834 - acc:  95.312500%\n","Training - running batch 27/176 of epoch 14/20 - loss: 0.173773854970932 - acc:  96.875000%\n","Training - running batch 28/176 of epoch 14/20 - loss: 0.5381958484649658 - acc:  85.156250%\n","Training - running batch 29/176 of epoch 14/20 - loss: 0.18473288416862488 - acc:  96.875000%\n","Training - running batch 30/176 of epoch 14/20 - loss: 0.2248610556125641 - acc:  95.312500%\n","Training - running batch 31/176 of epoch 14/20 - loss: 0.37398380041122437 - acc:  90.625000%\n","Training - running batch 32/176 of epoch 14/20 - loss: 0.30769574642181396 - acc:  94.531250%\n","Training - running batch 33/176 of epoch 14/20 - loss: 0.3232108950614929 - acc:  92.968750%\n","Training - running batch 34/176 of epoch 14/20 - loss: 0.28824761509895325 - acc:  93.750000%\n","Training - running batch 35/176 of epoch 14/20 - loss: 0.17891575396060944 - acc:  96.093750%\n","Training - running batch 36/176 of epoch 14/20 - loss: 0.130854532122612 - acc:  100.000000%\n","Training - running batch 37/176 of epoch 14/20 - loss: 0.3311370015144348 - acc:  92.187500%\n","Training - running batch 38/176 of epoch 14/20 - loss: 0.3512342572212219 - acc:  89.062500%\n","Training - running batch 39/176 of epoch 14/20 - loss: 0.3361702263355255 - acc:  91.406250%\n","Training - running batch 40/176 of epoch 14/20 - loss: 0.2521166205406189 - acc:  92.968750%\n","Training - running batch 41/176 of epoch 14/20 - loss: 0.14161935448646545 - acc:  98.437500%\n","Training - running batch 42/176 of epoch 14/20 - loss: 0.1915392279624939 - acc:  93.750000%\n","Training - running batch 43/176 of epoch 14/20 - loss: 0.31830936670303345 - acc:  93.750000%\n","Training - running batch 44/176 of epoch 14/20 - loss: 0.16696304082870483 - acc:  97.656250%\n","Training - running batch 45/176 of epoch 14/20 - loss: 0.23858819901943207 - acc:  95.312500%\n","Training - running batch 46/176 of epoch 14/20 - loss: 0.3222239315509796 - acc:  92.187500%\n","Training - running batch 47/176 of epoch 14/20 - loss: 0.304023414850235 - acc:  90.625000%\n","Training - running batch 48/176 of epoch 14/20 - loss: 0.3484658896923065 - acc:  91.406250%\n","Training - running batch 49/176 of epoch 14/20 - loss: 0.14471767842769623 - acc:  97.656250%\n","Training - running batch 50/176 of epoch 14/20 - loss: 0.28637468814849854 - acc:  92.187500%\n","Training - running batch 51/176 of epoch 14/20 - loss: 0.25453904271125793 - acc:  94.531250%\n","Training - running batch 52/176 of epoch 14/20 - loss: 0.2115560919046402 - acc:  96.875000%\n","Training - running batch 53/176 of epoch 14/20 - loss: 0.3930501937866211 - acc:  87.500000%\n","Training - running batch 54/176 of epoch 14/20 - loss: 0.28521135449409485 - acc:  94.531250%\n","Training - running batch 55/176 of epoch 14/20 - loss: 0.3137366771697998 - acc:  89.843750%\n","Training - running batch 56/176 of epoch 14/20 - loss: 0.3682333528995514 - acc:  87.500000%\n","Training - running batch 57/176 of epoch 14/20 - loss: 0.3033449053764343 - acc:  92.187500%\n","Training - running batch 58/176 of epoch 14/20 - loss: 0.17198605835437775 - acc:  96.093750%\n","Training - running batch 59/176 of epoch 14/20 - loss: 0.34310245513916016 - acc:  90.625000%\n","Training - running batch 60/176 of epoch 14/20 - loss: 0.34790703654289246 - acc:  96.093750%\n","Training - running batch 61/176 of epoch 14/20 - loss: 0.19092769920825958 - acc:  96.093750%\n","Training - running batch 62/176 of epoch 14/20 - loss: 0.11557839065790176 - acc:  100.000000%\n","Training - running batch 63/176 of epoch 14/20 - loss: 0.3166918456554413 - acc:  92.968750%\n","Training - running batch 64/176 of epoch 14/20 - loss: 0.2660064995288849 - acc:  93.750000%\n","Training - running batch 65/176 of epoch 14/20 - loss: 0.43824303150177 - acc:  89.062500%\n","Training - running batch 66/176 of epoch 14/20 - loss: 0.30101513862609863 - acc:  90.625000%\n","Training - running batch 67/176 of epoch 14/20 - loss: 0.37515151500701904 - acc:  89.843750%\n","Training - running batch 68/176 of epoch 14/20 - loss: 0.20507793128490448 - acc:  97.656250%\n","Training - running batch 69/176 of epoch 14/20 - loss: 0.1751818209886551 - acc:  100.000000%\n","Training - running batch 70/176 of epoch 14/20 - loss: 0.22951041162014008 - acc:  98.437500%\n","Training - running batch 71/176 of epoch 14/20 - loss: 0.1887371987104416 - acc:  97.656250%\n","Training - running batch 72/176 of epoch 14/20 - loss: 0.31988924741744995 - acc:  92.187500%\n","Training - running batch 73/176 of epoch 14/20 - loss: 0.17390386760234833 - acc:  96.093750%\n","Training - running batch 74/176 of epoch 14/20 - loss: 0.26953041553497314 - acc:  92.968750%\n","Training - running batch 75/176 of epoch 14/20 - loss: 0.12907171249389648 - acc:  97.656250%\n","Training - running batch 76/176 of epoch 14/20 - loss: 0.3464903235435486 - acc:  91.406250%\n","Training - running batch 77/176 of epoch 14/20 - loss: 0.45017337799072266 - acc:  92.187500%\n","Training - running batch 78/176 of epoch 14/20 - loss: 0.23621436953544617 - acc:  94.531250%\n","Training - running batch 79/176 of epoch 14/20 - loss: 0.23752273619174957 - acc:  96.875000%\n","Training - running batch 80/176 of epoch 14/20 - loss: 0.27996259927749634 - acc:  93.750000%\n","Training - running batch 81/176 of epoch 14/20 - loss: 0.425137996673584 - acc:  88.281250%\n","Training - running batch 82/176 of epoch 14/20 - loss: 0.27434664964675903 - acc:  95.312500%\n","Training - running batch 83/176 of epoch 14/20 - loss: 0.17957328259944916 - acc:  98.437500%\n","Training - running batch 84/176 of epoch 14/20 - loss: 0.2716751992702484 - acc:  95.312500%\n","Training - running batch 85/176 of epoch 14/20 - loss: 0.34511664509773254 - acc:  94.531250%\n","Training - running batch 86/176 of epoch 14/20 - loss: 0.3554791510105133 - acc:  89.843750%\n","Training - running batch 87/176 of epoch 14/20 - loss: 0.1609501838684082 - acc:  98.437500%\n","Training - running batch 88/176 of epoch 14/20 - loss: 0.35117727518081665 - acc:  91.406250%\n","Training - running batch 89/176 of epoch 14/20 - loss: 0.24808372557163239 - acc:  94.531250%\n","Training - running batch 90/176 of epoch 14/20 - loss: 0.14949697256088257 - acc:  96.875000%\n","Training - running batch 91/176 of epoch 14/20 - loss: 0.3601326644420624 - acc:  89.062500%\n","Training - running batch 92/176 of epoch 14/20 - loss: 0.3698199689388275 - acc:  85.937500%\n","Training - running batch 93/176 of epoch 14/20 - loss: 0.529212474822998 - acc:  85.937500%\n","Training - running batch 94/176 of epoch 14/20 - loss: 0.28503432869911194 - acc:  92.187500%\n","Training - running batch 95/176 of epoch 14/20 - loss: 0.3229619264602661 - acc:  92.968750%\n","Training - running batch 96/176 of epoch 14/20 - loss: 0.32170361280441284 - acc:  91.406250%\n","Training - running batch 97/176 of epoch 14/20 - loss: 0.1954139918088913 - acc:  98.437500%\n","Training - running batch 98/176 of epoch 14/20 - loss: 0.32231029868125916 - acc:  91.406250%\n","Training - running batch 99/176 of epoch 14/20 - loss: 0.3520703911781311 - acc:  92.187500%\n","Training - running batch 100/176 of epoch 14/20 - loss: 0.17353419959545135 - acc:  97.656250%\n","Training - running batch 101/176 of epoch 14/20 - loss: 0.3908161222934723 - acc:  88.281250%\n","Training - running batch 102/176 of epoch 14/20 - loss: 0.19571687281131744 - acc:  96.875000%\n","Training - running batch 103/176 of epoch 14/20 - loss: 0.39499104022979736 - acc:  91.406250%\n","Training - running batch 104/176 of epoch 14/20 - loss: 0.23352757096290588 - acc:  92.968750%\n","Training - running batch 105/176 of epoch 14/20 - loss: 0.2344360202550888 - acc:  98.437500%\n","Training - running batch 106/176 of epoch 14/20 - loss: 0.3137582242488861 - acc:  89.843750%\n","Training - running batch 107/176 of epoch 14/20 - loss: 0.21426290273666382 - acc:  94.531250%\n","Training - running batch 108/176 of epoch 14/20 - loss: 0.34977760910987854 - acc:  92.187500%\n","Training - running batch 109/176 of epoch 14/20 - loss: 0.2435256689786911 - acc:  93.750000%\n","Training - running batch 110/176 of epoch 14/20 - loss: 0.20718160271644592 - acc:  96.875000%\n","Training - running batch 111/176 of epoch 14/20 - loss: 0.3081514835357666 - acc:  93.750000%\n","Training - running batch 112/176 of epoch 14/20 - loss: 0.22055602073669434 - acc:  92.187500%\n","Training - running batch 113/176 of epoch 14/20 - loss: 0.2812528908252716 - acc:  93.750000%\n","Training - running batch 114/176 of epoch 14/20 - loss: 0.3763885200023651 - acc:  89.843750%\n","Training - running batch 115/176 of epoch 14/20 - loss: 0.24737878143787384 - acc:  98.437500%\n","Training - running batch 116/176 of epoch 14/20 - loss: 0.3096417188644409 - acc:  93.750000%\n","Training - running batch 117/176 of epoch 14/20 - loss: 0.18245960772037506 - acc:  96.093750%\n","Training - running batch 118/176 of epoch 14/20 - loss: 0.22458826005458832 - acc:  95.312500%\n","Training - running batch 119/176 of epoch 14/20 - loss: 0.33899474143981934 - acc:  91.406250%\n","Training - running batch 120/176 of epoch 14/20 - loss: 0.16175970435142517 - acc:  98.437500%\n","Training - running batch 121/176 of epoch 14/20 - loss: 0.3662840723991394 - acc:  93.750000%\n","Training - running batch 122/176 of epoch 14/20 - loss: 0.4562683403491974 - acc:  87.500000%\n","Training - running batch 123/176 of epoch 14/20 - loss: 0.4656648635864258 - acc:  85.937500%\n","Training - running batch 124/176 of epoch 14/20 - loss: 0.36634179949760437 - acc:  90.625000%\n","Training - running batch 125/176 of epoch 14/20 - loss: 0.2971027195453644 - acc:  94.531250%\n","Training - running batch 126/176 of epoch 14/20 - loss: 0.42276814579963684 - acc:  90.625000%\n","Training - running batch 127/176 of epoch 14/20 - loss: 0.35154709219932556 - acc:  91.406250%\n","Training - running batch 128/176 of epoch 14/20 - loss: 0.3160020709037781 - acc:  92.968750%\n","Training - running batch 129/176 of epoch 14/20 - loss: 0.5333421230316162 - acc:  84.375000%\n","Training - running batch 130/176 of epoch 14/20 - loss: 0.30920401215553284 - acc:  92.968750%\n","Training - running batch 131/176 of epoch 14/20 - loss: 0.20103274285793304 - acc:  96.093750%\n","Training - running batch 132/176 of epoch 14/20 - loss: 0.5351656079292297 - acc:  85.937500%\n","Training - running batch 133/176 of epoch 14/20 - loss: 0.19632494449615479 - acc:  94.531250%\n","Training - running batch 134/176 of epoch 14/20 - loss: 0.16799229383468628 - acc:  98.437500%\n","Training - running batch 135/176 of epoch 14/20 - loss: 0.1899298131465912 - acc:  96.093750%\n","Training - running batch 136/176 of epoch 14/20 - loss: 0.18451645970344543 - acc:  99.218750%\n","Training - running batch 137/176 of epoch 14/20 - loss: 0.29136955738067627 - acc:  95.312500%\n","Training - running batch 138/176 of epoch 14/20 - loss: 0.5252057909965515 - acc:  87.500000%\n","Training - running batch 139/176 of epoch 14/20 - loss: 0.3805559575557709 - acc:  83.593750%\n","Training - running batch 140/176 of epoch 14/20 - loss: 0.2508693337440491 - acc:  95.312500%\n","Training - running batch 141/176 of epoch 14/20 - loss: 0.39117297530174255 - acc:  88.281250%\n","Training - running batch 142/176 of epoch 14/20 - loss: 0.21159638464450836 - acc:  96.093750%\n","Training - running batch 143/176 of epoch 14/20 - loss: 0.4559505581855774 - acc:  92.968750%\n","Training - running batch 144/176 of epoch 14/20 - loss: 0.14371086657047272 - acc:  99.218750%\n","Training - running batch 145/176 of epoch 14/20 - loss: 0.16735851764678955 - acc:  96.875000%\n","Training - running batch 146/176 of epoch 14/20 - loss: 0.4073106050491333 - acc:  89.062500%\n","Training - running batch 147/176 of epoch 14/20 - loss: 0.20669473707675934 - acc:  98.437500%\n","Training - running batch 148/176 of epoch 14/20 - loss: 0.1404460072517395 - acc:  96.875000%\n","Training - running batch 149/176 of epoch 14/20 - loss: 0.2681831121444702 - acc:  93.750000%\n","Training - running batch 150/176 of epoch 14/20 - loss: 0.3933085501194 - acc:  86.718750%\n","Training - running batch 151/176 of epoch 14/20 - loss: 0.20290717482566833 - acc:  97.656250%\n","Training - running batch 152/176 of epoch 14/20 - loss: 0.2084219604730606 - acc:  96.875000%\n","Training - running batch 153/176 of epoch 14/20 - loss: 0.2725357711315155 - acc:  92.968750%\n","Training - running batch 154/176 of epoch 14/20 - loss: 0.20246171951293945 - acc:  95.312500%\n","Training - running batch 155/176 of epoch 14/20 - loss: 0.3601531684398651 - acc:  88.281250%\n","Training - running batch 156/176 of epoch 14/20 - loss: 0.3734287917613983 - acc:  89.062500%\n","Training - running batch 157/176 of epoch 14/20 - loss: 0.42867910861968994 - acc:  85.156250%\n","Training - running batch 158/176 of epoch 14/20 - loss: 0.4186946153640747 - acc:  85.937500%\n","Training - running batch 159/176 of epoch 14/20 - loss: 0.15934792160987854 - acc:  96.875000%\n","Training - running batch 160/176 of epoch 14/20 - loss: 0.4347141981124878 - acc:  89.062500%\n","Training - running batch 161/176 of epoch 14/20 - loss: 0.11814137548208237 - acc:  99.218750%\n","Training - running batch 162/176 of epoch 14/20 - loss: 0.34468889236450195 - acc:  89.062500%\n","Training - running batch 163/176 of epoch 14/20 - loss: 0.1808205097913742 - acc:  96.875000%\n","Training - running batch 164/176 of epoch 14/20 - loss: 0.412540465593338 - acc:  90.625000%\n","Training - running batch 165/176 of epoch 14/20 - loss: 0.15022128820419312 - acc:  96.093750%\n","Training - running batch 166/176 of epoch 14/20 - loss: 0.27034202218055725 - acc:  93.750000%\n","Training - running batch 167/176 of epoch 14/20 - loss: 0.2593708634376526 - acc:  95.312500%\n","Training - running batch 168/176 of epoch 14/20 - loss: 0.407789021730423 - acc:  92.187500%\n","Training - running batch 169/176 of epoch 14/20 - loss: 0.13126574456691742 - acc:  99.218750%\n","Training - running batch 170/176 of epoch 14/20 - loss: 0.2500922381877899 - acc:  93.750000%\n","Training - running batch 171/176 of epoch 14/20 - loss: 0.146123468875885 - acc:  99.218750%\n","Training - running batch 172/176 of epoch 14/20 - loss: 0.41216394305229187 - acc:  84.375000%\n","Training - running batch 173/176 of epoch 14/20 - loss: 0.37979039549827576 - acc:  89.843750%\n","Training - running batch 174/176 of epoch 14/20 - loss: 0.3387225568294525 - acc:  87.500000%\n","Training - running batch 175/176 of epoch 14/20 - loss: 0.17325608432292938 - acc:  98.387097%\n","Testing - acc:  0.680285%\n","Training - running batch 0/176 of epoch 15/20 - loss: 0.19598565995693207 - acc:  96.875000%\n","Training - running batch 1/176 of epoch 15/20 - loss: 0.4207700192928314 - acc:  89.062500%\n","Training - running batch 2/176 of epoch 15/20 - loss: 0.3758227229118347 - acc:  87.500000%\n","Training - running batch 3/176 of epoch 15/20 - loss: 0.34144675731658936 - acc:  87.500000%\n","Training - running batch 4/176 of epoch 15/20 - loss: 0.23045577108860016 - acc:  95.312500%\n","Training - running batch 5/176 of epoch 15/20 - loss: 0.12658853828907013 - acc:  97.656250%\n","Training - running batch 6/176 of epoch 15/20 - loss: 0.2612806558609009 - acc:  96.093750%\n","Training - running batch 7/176 of epoch 15/20 - loss: 0.18341752886772156 - acc:  98.437500%\n","Training - running batch 8/176 of epoch 15/20 - loss: 0.22012865543365479 - acc:  96.875000%\n","Training - running batch 9/176 of epoch 15/20 - loss: 0.11191952228546143 - acc:  100.000000%\n","Training - running batch 10/176 of epoch 15/20 - loss: 0.2414921522140503 - acc:  94.531250%\n","Training - running batch 11/176 of epoch 15/20 - loss: 0.4166547954082489 - acc:  87.500000%\n","Training - running batch 12/176 of epoch 15/20 - loss: 0.1962321698665619 - acc:  96.875000%\n","Training - running batch 13/176 of epoch 15/20 - loss: 0.2609940469264984 - acc:  95.312500%\n","Training - running batch 14/176 of epoch 15/20 - loss: 0.215080127120018 - acc:  95.312500%\n","Training - running batch 15/176 of epoch 15/20 - loss: 0.0895480290055275 - acc:  96.875000%\n","Training - running batch 16/176 of epoch 15/20 - loss: 0.08993924409151077 - acc:  96.875000%\n","Training - running batch 17/176 of epoch 15/20 - loss: 0.25715911388397217 - acc:  92.968750%\n","Training - running batch 18/176 of epoch 15/20 - loss: 0.11304143071174622 - acc:  98.437500%\n","Training - running batch 19/176 of epoch 15/20 - loss: 0.2343718409538269 - acc:  97.656250%\n","Training - running batch 20/176 of epoch 15/20 - loss: 0.17388999462127686 - acc:  97.656250%\n","Training - running batch 21/176 of epoch 15/20 - loss: 0.2836158871650696 - acc:  90.625000%\n","Training - running batch 22/176 of epoch 15/20 - loss: 0.22644224762916565 - acc:  94.531250%\n","Training - running batch 23/176 of epoch 15/20 - loss: 0.3144695460796356 - acc:  92.187500%\n","Training - running batch 24/176 of epoch 15/20 - loss: 0.1391148567199707 - acc:  98.437500%\n","Training - running batch 25/176 of epoch 15/20 - loss: 0.11666020005941391 - acc:  99.218750%\n","Training - running batch 26/176 of epoch 15/20 - loss: 0.26781192421913147 - acc:  93.750000%\n","Training - running batch 27/176 of epoch 15/20 - loss: 0.0748056173324585 - acc:  98.437500%\n","Training - running batch 28/176 of epoch 15/20 - loss: 0.23139075934886932 - acc:  93.750000%\n","Training - running batch 29/176 of epoch 15/20 - loss: 0.10642562061548233 - acc:  99.218750%\n","Training - running batch 30/176 of epoch 15/20 - loss: 0.2614893317222595 - acc:  95.312500%\n","Training - running batch 31/176 of epoch 15/20 - loss: 0.1822669506072998 - acc:  96.875000%\n","Training - running batch 32/176 of epoch 15/20 - loss: 0.24727553129196167 - acc:  92.968750%\n","Training - running batch 33/176 of epoch 15/20 - loss: 0.1273811310529709 - acc:  99.218750%\n","Training - running batch 34/176 of epoch 15/20 - loss: 0.170670747756958 - acc:  96.875000%\n","Training - running batch 35/176 of epoch 15/20 - loss: 0.11176503449678421 - acc:  99.218750%\n","Training - running batch 36/176 of epoch 15/20 - loss: 0.1752149760723114 - acc:  96.875000%\n","Training - running batch 37/176 of epoch 15/20 - loss: 0.20993094146251678 - acc:  96.093750%\n","Training - running batch 38/176 of epoch 15/20 - loss: 0.28521162271499634 - acc:  94.531250%\n","Training - running batch 39/176 of epoch 15/20 - loss: 0.07780005037784576 - acc:  100.000000%\n","Training - running batch 40/176 of epoch 15/20 - loss: 0.18524931371212006 - acc:  93.750000%\n","Training - running batch 41/176 of epoch 15/20 - loss: 0.1137363389134407 - acc:  99.218750%\n","Training - running batch 42/176 of epoch 15/20 - loss: 0.18750564754009247 - acc:  98.437500%\n","Training - running batch 43/176 of epoch 15/20 - loss: 0.13456571102142334 - acc:  97.656250%\n","Training - running batch 44/176 of epoch 15/20 - loss: 0.08100150525569916 - acc:  100.000000%\n","Training - running batch 45/176 of epoch 15/20 - loss: 0.32253405451774597 - acc:  94.531250%\n","Training - running batch 46/176 of epoch 15/20 - loss: 0.17407485842704773 - acc:  99.218750%\n","Training - running batch 47/176 of epoch 15/20 - loss: 0.1358235478401184 - acc:  98.437500%\n","Training - running batch 48/176 of epoch 15/20 - loss: 0.12479085475206375 - acc:  97.656250%\n","Training - running batch 49/176 of epoch 15/20 - loss: 0.1358329802751541 - acc:  98.437500%\n","Training - running batch 50/176 of epoch 15/20 - loss: 0.2448863685131073 - acc:  95.312500%\n","Training - running batch 51/176 of epoch 15/20 - loss: 0.19784854352474213 - acc:  96.093750%\n","Training - running batch 52/176 of epoch 15/20 - loss: 0.16474103927612305 - acc:  98.437500%\n","Training - running batch 53/176 of epoch 15/20 - loss: 0.22253936529159546 - acc:  96.093750%\n","Training - running batch 54/176 of epoch 15/20 - loss: 0.14385691285133362 - acc:  97.656250%\n","Training - running batch 55/176 of epoch 15/20 - loss: 0.09494995325803757 - acc:  100.000000%\n","Training - running batch 56/176 of epoch 15/20 - loss: 0.25427496433258057 - acc:  93.750000%\n","Training - running batch 57/176 of epoch 15/20 - loss: 0.06193450838327408 - acc:  100.000000%\n","Training - running batch 58/176 of epoch 15/20 - loss: 0.1077919527888298 - acc:  100.000000%\n","Training - running batch 59/176 of epoch 15/20 - loss: 0.3165341019630432 - acc:  94.531250%\n","Training - running batch 60/176 of epoch 15/20 - loss: 0.18970297276973724 - acc:  97.656250%\n","Training - running batch 61/176 of epoch 15/20 - loss: 0.22147487103939056 - acc:  95.312500%\n","Training - running batch 62/176 of epoch 15/20 - loss: 0.20684219896793365 - acc:  97.656250%\n","Training - running batch 63/176 of epoch 15/20 - loss: 0.37533038854599 - acc:  92.187500%\n","Training - running batch 64/176 of epoch 15/20 - loss: 0.07882319390773773 - acc:  98.437500%\n","Training - running batch 65/176 of epoch 15/20 - loss: 0.22081828117370605 - acc:  96.093750%\n","Training - running batch 66/176 of epoch 15/20 - loss: 0.25331035256385803 - acc:  92.968750%\n","Training - running batch 67/176 of epoch 15/20 - loss: 0.1582939773797989 - acc:  96.093750%\n","Training - running batch 68/176 of epoch 15/20 - loss: 0.16400855779647827 - acc:  99.218750%\n","Training - running batch 69/176 of epoch 15/20 - loss: 0.10798119008541107 - acc:  99.218750%\n","Training - running batch 70/176 of epoch 15/20 - loss: 0.1038493812084198 - acc:  100.000000%\n","Training - running batch 71/176 of epoch 15/20 - loss: 0.23778259754180908 - acc:  96.093750%\n","Training - running batch 72/176 of epoch 15/20 - loss: 0.2956698536872864 - acc:  93.750000%\n","Training - running batch 73/176 of epoch 15/20 - loss: 0.19001717865467072 - acc:  95.312500%\n","Training - running batch 74/176 of epoch 15/20 - loss: 0.1593223661184311 - acc:  96.875000%\n","Training - running batch 75/176 of epoch 15/20 - loss: 0.13613104820251465 - acc:  92.187500%\n","Training - running batch 76/176 of epoch 15/20 - loss: 0.15464696288108826 - acc:  100.000000%\n","Training - running batch 77/176 of epoch 15/20 - loss: 0.147095188498497 - acc:  96.875000%\n","Training - running batch 78/176 of epoch 15/20 - loss: 0.1332506686449051 - acc:  97.656250%\n","Training - running batch 79/176 of epoch 15/20 - loss: 0.05434201657772064 - acc:  100.000000%\n","Training - running batch 80/176 of epoch 15/20 - loss: 0.11766346544027328 - acc:  97.656250%\n","Training - running batch 81/176 of epoch 15/20 - loss: 0.07586847990751266 - acc:  100.000000%\n","Training - running batch 82/176 of epoch 15/20 - loss: 0.12789441645145416 - acc:  97.656250%\n","Training - running batch 83/176 of epoch 15/20 - loss: 0.15448999404907227 - acc:  96.093750%\n","Training - running batch 84/176 of epoch 15/20 - loss: 0.20571282505989075 - acc:  96.875000%\n","Training - running batch 85/176 of epoch 15/20 - loss: 0.14214597642421722 - acc:  96.093750%\n","Training - running batch 86/176 of epoch 15/20 - loss: 0.3061003088951111 - acc:  92.187500%\n","Training - running batch 87/176 of epoch 15/20 - loss: 0.2531925439834595 - acc:  93.750000%\n","Training - running batch 88/176 of epoch 15/20 - loss: 0.11201267689466476 - acc:  98.437500%\n","Training - running batch 89/176 of epoch 15/20 - loss: 0.12988711893558502 - acc:  97.656250%\n","Training - running batch 90/176 of epoch 15/20 - loss: 0.06844650954008102 - acc:  100.000000%\n","Training - running batch 91/176 of epoch 15/20 - loss: 0.07130467146635056 - acc:  100.000000%\n","Training - running batch 92/176 of epoch 15/20 - loss: 0.1468529850244522 - acc:  96.875000%\n","Training - running batch 93/176 of epoch 15/20 - loss: 0.10998548567295074 - acc:  98.437500%\n","Training - running batch 94/176 of epoch 15/20 - loss: 0.2971513569355011 - acc:  94.531250%\n","Training - running batch 95/176 of epoch 15/20 - loss: 0.09433121979236603 - acc:  100.000000%\n","Training - running batch 96/176 of epoch 15/20 - loss: 0.0747998058795929 - acc:  98.437500%\n","Training - running batch 97/176 of epoch 15/20 - loss: 0.0895710438489914 - acc:  100.000000%\n","Training - running batch 98/176 of epoch 15/20 - loss: 0.16701801121234894 - acc:  98.437500%\n","Training - running batch 99/176 of epoch 15/20 - loss: 0.14552980661392212 - acc:  98.437500%\n","Training - running batch 100/176 of epoch 15/20 - loss: 0.10279888659715652 - acc:  100.000000%\n","Training - running batch 101/176 of epoch 15/20 - loss: 0.2585909962654114 - acc:  93.750000%\n","Training - running batch 102/176 of epoch 15/20 - loss: 0.10574672371149063 - acc:  98.437500%\n","Training - running batch 103/176 of epoch 15/20 - loss: 0.09779943525791168 - acc:  100.000000%\n","Training - running batch 104/176 of epoch 15/20 - loss: 0.07779714465141296 - acc:  100.000000%\n","Training - running batch 105/176 of epoch 15/20 - loss: 0.13134510815143585 - acc:  98.437500%\n","Training - running batch 106/176 of epoch 15/20 - loss: 0.1429760903120041 - acc:  96.093750%\n","Training - running batch 107/176 of epoch 15/20 - loss: 0.09938082098960876 - acc:  100.000000%\n","Training - running batch 108/176 of epoch 15/20 - loss: 0.12156534940004349 - acc:  98.437500%\n","Training - running batch 109/176 of epoch 15/20 - loss: 0.2395235151052475 - acc:  96.875000%\n","Training - running batch 110/176 of epoch 15/20 - loss: 0.10390456020832062 - acc:  98.437500%\n","Training - running batch 111/176 of epoch 15/20 - loss: 0.12303178757429123 - acc:  100.000000%\n","Training - running batch 112/176 of epoch 15/20 - loss: 0.1679227203130722 - acc:  96.093750%\n","Training - running batch 113/176 of epoch 15/20 - loss: 0.15940794348716736 - acc:  97.656250%\n","Training - running batch 114/176 of epoch 15/20 - loss: 0.21096709370613098 - acc:  95.312500%\n","Training - running batch 115/176 of epoch 15/20 - loss: 0.1977251023054123 - acc:  96.875000%\n","Training - running batch 116/176 of epoch 15/20 - loss: 0.1814565807580948 - acc:  97.656250%\n","Training - running batch 117/176 of epoch 15/20 - loss: 0.48901164531707764 - acc:  91.406250%\n","Training - running batch 118/176 of epoch 15/20 - loss: 0.23582999408245087 - acc:  96.875000%\n","Training - running batch 119/176 of epoch 15/20 - loss: 0.1206023171544075 - acc:  97.656250%\n","Training - running batch 120/176 of epoch 15/20 - loss: 0.13325414061546326 - acc:  97.656250%\n","Training - running batch 121/176 of epoch 15/20 - loss: 0.15287785232067108 - acc:  95.312500%\n","Training - running batch 122/176 of epoch 15/20 - loss: 0.04775279387831688 - acc:  100.000000%\n","Training - running batch 123/176 of epoch 15/20 - loss: 0.1553708016872406 - acc:  96.093750%\n","Training - running batch 124/176 of epoch 15/20 - loss: 0.209661602973938 - acc:  94.531250%\n","Training - running batch 125/176 of epoch 15/20 - loss: 0.09530604630708694 - acc:  99.218750%\n","Training - running batch 126/176 of epoch 15/20 - loss: 0.11391747742891312 - acc:  99.218750%\n","Training - running batch 127/176 of epoch 15/20 - loss: 0.17732281982898712 - acc:  96.093750%\n","Training - running batch 128/176 of epoch 15/20 - loss: 0.1598655730485916 - acc:  96.093750%\n","Training - running batch 129/176 of epoch 15/20 - loss: 0.24354292452335358 - acc:  93.750000%\n","Training - running batch 130/176 of epoch 15/20 - loss: 0.06840967386960983 - acc:  100.000000%\n","Training - running batch 131/176 of epoch 15/20 - loss: 0.5584779381752014 - acc:  86.718750%\n","Training - running batch 132/176 of epoch 15/20 - loss: 0.14801248908042908 - acc:  100.000000%\n","Training - running batch 133/176 of epoch 15/20 - loss: 0.20961742103099823 - acc:  94.531250%\n","Training - running batch 134/176 of epoch 15/20 - loss: 0.20175041258335114 - acc:  95.312500%\n","Training - running batch 135/176 of epoch 15/20 - loss: 0.1702742725610733 - acc:  99.218750%\n","Training - running batch 136/176 of epoch 15/20 - loss: 0.17327114939689636 - acc:  96.875000%\n","Training - running batch 137/176 of epoch 15/20 - loss: 0.20722420513629913 - acc:  96.093750%\n","Training - running batch 138/176 of epoch 15/20 - loss: 0.12171068787574768 - acc:  96.875000%\n","Training - running batch 139/176 of epoch 15/20 - loss: 0.11139209568500519 - acc:  97.656250%\n","Training - running batch 140/176 of epoch 15/20 - loss: 0.1260054111480713 - acc:  96.875000%\n","Training - running batch 141/176 of epoch 15/20 - loss: 0.2053123414516449 - acc:  96.875000%\n","Training - running batch 142/176 of epoch 15/20 - loss: 0.10548262298107147 - acc:  98.437500%\n","Training - running batch 143/176 of epoch 15/20 - loss: 0.14998137950897217 - acc:  94.531250%\n","Training - running batch 144/176 of epoch 15/20 - loss: 0.11412759870290756 - acc:  99.218750%\n","Training - running batch 145/176 of epoch 15/20 - loss: 0.22043997049331665 - acc:  94.531250%\n","Training - running batch 146/176 of epoch 15/20 - loss: 0.1516338586807251 - acc:  96.875000%\n","Training - running batch 147/176 of epoch 15/20 - loss: 0.1627321094274521 - acc:  98.437500%\n","Training - running batch 148/176 of epoch 15/20 - loss: 0.0900825709104538 - acc:  99.218750%\n","Training - running batch 149/176 of epoch 15/20 - loss: 0.21601742506027222 - acc:  96.093750%\n","Training - running batch 150/176 of epoch 15/20 - loss: 0.29336413741111755 - acc:  93.750000%\n","Training - running batch 151/176 of epoch 15/20 - loss: 0.08476417511701584 - acc:  98.437500%\n","Training - running batch 152/176 of epoch 15/20 - loss: 0.07797940075397491 - acc:  98.437500%\n","Training - running batch 153/176 of epoch 15/20 - loss: 0.19398030638694763 - acc:  96.093750%\n","Training - running batch 154/176 of epoch 15/20 - loss: 0.21420007944107056 - acc:  95.312500%\n","Training - running batch 155/176 of epoch 15/20 - loss: 0.07006929814815521 - acc:  100.000000%\n","Training - running batch 156/176 of epoch 15/20 - loss: 0.08451227843761444 - acc:  96.875000%\n","Training - running batch 157/176 of epoch 15/20 - loss: 0.11314963549375534 - acc:  97.656250%\n","Training - running batch 158/176 of epoch 15/20 - loss: 0.23191900551319122 - acc:  96.875000%\n","Training - running batch 159/176 of epoch 15/20 - loss: 0.1319131702184677 - acc:  98.437500%\n","Training - running batch 160/176 of epoch 15/20 - loss: 0.1478981077671051 - acc:  96.875000%\n","Training - running batch 161/176 of epoch 15/20 - loss: 0.296676367521286 - acc:  93.750000%\n","Training - running batch 162/176 of epoch 15/20 - loss: 0.2113579511642456 - acc:  96.875000%\n","Training - running batch 163/176 of epoch 15/20 - loss: 0.05166708678007126 - acc:  100.000000%\n","Training - running batch 164/176 of epoch 15/20 - loss: 0.11823979765176773 - acc:  99.218750%\n","Training - running batch 165/176 of epoch 15/20 - loss: 0.3662874102592468 - acc:  92.187500%\n","Training - running batch 166/176 of epoch 15/20 - loss: 0.18024292588233948 - acc:  97.656250%\n","Training - running batch 167/176 of epoch 15/20 - loss: 0.1281185895204544 - acc:  98.437500%\n","Training - running batch 168/176 of epoch 15/20 - loss: 0.10069921612739563 - acc:  99.218750%\n","Training - running batch 169/176 of epoch 15/20 - loss: 0.15390785038471222 - acc:  99.218750%\n","Training - running batch 170/176 of epoch 15/20 - loss: 0.09670265018939972 - acc:  97.656250%\n","Training - running batch 171/176 of epoch 15/20 - loss: 0.13109953701496124 - acc:  99.218750%\n","Training - running batch 172/176 of epoch 15/20 - loss: 0.1631152480840683 - acc:  96.093750%\n","Training - running batch 173/176 of epoch 15/20 - loss: 0.25739288330078125 - acc:  93.750000%\n","Training - running batch 174/176 of epoch 15/20 - loss: 0.1531660407781601 - acc:  97.656250%\n","Training - running batch 175/176 of epoch 15/20 - loss: 0.38129767775535583 - acc:  88.709677%\n","Testing - acc:  0.724581%\n","Training - running batch 0/176 of epoch 16/20 - loss: 0.0629144087433815 - acc:  100.000000%\n","Training - running batch 1/176 of epoch 16/20 - loss: 0.19845078885555267 - acc:  94.531250%\n","Training - running batch 2/176 of epoch 16/20 - loss: 0.1732897311449051 - acc:  98.437500%\n","Training - running batch 3/176 of epoch 16/20 - loss: 0.16216175258159637 - acc:  96.093750%\n","Training - running batch 4/176 of epoch 16/20 - loss: 0.15101705491542816 - acc:  99.218750%\n","Training - running batch 5/176 of epoch 16/20 - loss: 0.08968441188335419 - acc:  98.437500%\n","Training - running batch 6/176 of epoch 16/20 - loss: 0.07760970294475555 - acc:  98.437500%\n","Training - running batch 7/176 of epoch 16/20 - loss: 0.09046304225921631 - acc:  98.437500%\n","Training - running batch 8/176 of epoch 16/20 - loss: 0.13868622481822968 - acc:  97.656250%\n","Training - running batch 9/176 of epoch 16/20 - loss: 0.17565599083900452 - acc:  98.437500%\n","Training - running batch 10/176 of epoch 16/20 - loss: 0.2722821533679962 - acc:  96.093750%\n","Training - running batch 11/176 of epoch 16/20 - loss: 0.11597704887390137 - acc:  100.000000%\n","Training - running batch 12/176 of epoch 16/20 - loss: 0.17727157473564148 - acc:  97.656250%\n","Training - running batch 13/176 of epoch 16/20 - loss: 0.1270628571510315 - acc:  97.656250%\n","Training - running batch 14/176 of epoch 16/20 - loss: 0.11386732012033463 - acc:  98.437500%\n","Training - running batch 15/176 of epoch 16/20 - loss: 0.07424043118953705 - acc:  100.000000%\n","Training - running batch 16/176 of epoch 16/20 - loss: 0.16399554908275604 - acc:  96.093750%\n","Training - running batch 17/176 of epoch 16/20 - loss: 0.11889432370662689 - acc:  98.437500%\n","Training - running batch 18/176 of epoch 16/20 - loss: 0.10512597113847733 - acc:  98.437500%\n","Training - running batch 19/176 of epoch 16/20 - loss: 0.0922163873910904 - acc:  100.000000%\n","Training - running batch 20/176 of epoch 16/20 - loss: 0.15514077246189117 - acc:  96.875000%\n","Training - running batch 21/176 of epoch 16/20 - loss: 0.12898360192775726 - acc:  98.437500%\n","Training - running batch 22/176 of epoch 16/20 - loss: 0.20673146843910217 - acc:  96.875000%\n","Training - running batch 23/176 of epoch 16/20 - loss: 0.08700551092624664 - acc:  100.000000%\n","Training - running batch 24/176 of epoch 16/20 - loss: 0.054861459881067276 - acc:  100.000000%\n","Training - running batch 25/176 of epoch 16/20 - loss: 0.1486411690711975 - acc:  96.875000%\n","Training - running batch 26/176 of epoch 16/20 - loss: 0.18341389298439026 - acc:  96.875000%\n","Training - running batch 27/176 of epoch 16/20 - loss: 0.06541520357131958 - acc:  100.000000%\n","Training - running batch 28/176 of epoch 16/20 - loss: 0.15870995819568634 - acc:  100.000000%\n","Training - running batch 29/176 of epoch 16/20 - loss: 0.22541654109954834 - acc:  96.093750%\n","Training - running batch 30/176 of epoch 16/20 - loss: 0.06143544241786003 - acc:  100.000000%\n","Training - running batch 31/176 of epoch 16/20 - loss: 0.20120954513549805 - acc:  96.093750%\n","Training - running batch 32/176 of epoch 16/20 - loss: 0.17209427058696747 - acc:  97.656250%\n","Training - running batch 33/176 of epoch 16/20 - loss: 0.087547168135643 - acc:  98.437500%\n","Training - running batch 34/176 of epoch 16/20 - loss: 0.19791202247142792 - acc:  96.093750%\n","Training - running batch 35/176 of epoch 16/20 - loss: 0.10210692137479782 - acc:  99.218750%\n","Training - running batch 36/176 of epoch 16/20 - loss: 0.172286719083786 - acc:  96.875000%\n","Training - running batch 37/176 of epoch 16/20 - loss: 0.14994493126869202 - acc:  96.093750%\n","Training - running batch 38/176 of epoch 16/20 - loss: 0.17085608839988708 - acc:  96.875000%\n","Training - running batch 39/176 of epoch 16/20 - loss: 0.12114406377077103 - acc:  98.437500%\n","Training - running batch 40/176 of epoch 16/20 - loss: 0.20252960920333862 - acc:  96.875000%\n","Training - running batch 41/176 of epoch 16/20 - loss: 0.1379830539226532 - acc:  96.875000%\n","Training - running batch 42/176 of epoch 16/20 - loss: 0.08627146482467651 - acc:  99.218750%\n","Training - running batch 43/176 of epoch 16/20 - loss: 0.1369718611240387 - acc:  97.656250%\n","Training - running batch 44/176 of epoch 16/20 - loss: 0.10984054207801819 - acc:  97.656250%\n","Training - running batch 45/176 of epoch 16/20 - loss: 0.24457712471485138 - acc:  94.531250%\n","Training - running batch 46/176 of epoch 16/20 - loss: 0.15576936304569244 - acc:  99.218750%\n","Training - running batch 47/176 of epoch 16/20 - loss: 0.1651235967874527 - acc:  96.875000%\n","Training - running batch 48/176 of epoch 16/20 - loss: 0.10744653642177582 - acc:  99.218750%\n","Training - running batch 49/176 of epoch 16/20 - loss: 0.06897306442260742 - acc:  100.000000%\n","Training - running batch 50/176 of epoch 16/20 - loss: 0.12124665081501007 - acc:  99.218750%\n","Training - running batch 51/176 of epoch 16/20 - loss: 0.08072711527347565 - acc:  98.437500%\n","Training - running batch 52/176 of epoch 16/20 - loss: 0.11999344825744629 - acc:  98.437500%\n","Training - running batch 53/176 of epoch 16/20 - loss: 0.14318864047527313 - acc:  98.437500%\n","Training - running batch 54/176 of epoch 16/20 - loss: 0.15500155091285706 - acc:  99.218750%\n","Training - running batch 55/176 of epoch 16/20 - loss: 0.10904061794281006 - acc:  99.218750%\n","Training - running batch 56/176 of epoch 16/20 - loss: 0.1600494533777237 - acc:  96.093750%\n","Training - running batch 57/176 of epoch 16/20 - loss: 0.09444916248321533 - acc:  99.218750%\n","Training - running batch 58/176 of epoch 16/20 - loss: 0.055971547961235046 - acc:  100.000000%\n","Training - running batch 59/176 of epoch 16/20 - loss: 0.07976820319890976 - acc:  100.000000%\n","Training - running batch 60/176 of epoch 16/20 - loss: 0.09136547893285751 - acc:  99.218750%\n","Training - running batch 61/176 of epoch 16/20 - loss: 0.2048463076353073 - acc:  95.312500%\n","Training - running batch 62/176 of epoch 16/20 - loss: 0.28597551584243774 - acc:  94.531250%\n","Training - running batch 63/176 of epoch 16/20 - loss: 0.31811660528182983 - acc:  91.406250%\n","Training - running batch 64/176 of epoch 16/20 - loss: 0.21535015106201172 - acc:  96.093750%\n","Training - running batch 65/176 of epoch 16/20 - loss: 0.11002576351165771 - acc:  98.437500%\n","Training - running batch 66/176 of epoch 16/20 - loss: 0.1158604696393013 - acc:  96.875000%\n","Training - running batch 67/176 of epoch 16/20 - loss: 0.20258280634880066 - acc:  96.875000%\n","Training - running batch 68/176 of epoch 16/20 - loss: 0.10945916175842285 - acc:  99.218750%\n","Training - running batch 69/176 of epoch 16/20 - loss: 0.1546943038702011 - acc:  97.656250%\n","Training - running batch 70/176 of epoch 16/20 - loss: 0.10396834462881088 - acc:  97.656250%\n","Training - running batch 71/176 of epoch 16/20 - loss: 0.18726325035095215 - acc:  96.093750%\n","Training - running batch 72/176 of epoch 16/20 - loss: 0.12172619253396988 - acc:  96.875000%\n","Training - running batch 73/176 of epoch 16/20 - loss: 0.09452049434185028 - acc:  99.218750%\n","Training - running batch 74/176 of epoch 16/20 - loss: 0.16891562938690186 - acc:  96.093750%\n","Training - running batch 75/176 of epoch 16/20 - loss: 0.09530183672904968 - acc:  100.000000%\n","Training - running batch 76/176 of epoch 16/20 - loss: 0.13214722275733948 - acc:  100.000000%\n","Training - running batch 77/176 of epoch 16/20 - loss: 0.05751940980553627 - acc:  100.000000%\n","Training - running batch 78/176 of epoch 16/20 - loss: 0.11625339090824127 - acc:  97.656250%\n","Training - running batch 79/176 of epoch 16/20 - loss: 0.22729796171188354 - acc:  95.312500%\n","Training - running batch 80/176 of epoch 16/20 - loss: 0.11975570768117905 - acc:  97.656250%\n","Training - running batch 81/176 of epoch 16/20 - loss: 0.15024641156196594 - acc:  97.656250%\n","Training - running batch 82/176 of epoch 16/20 - loss: 0.11325282603502274 - acc:  96.875000%\n","Training - running batch 83/176 of epoch 16/20 - loss: 0.13851511478424072 - acc:  98.437500%\n","Training - running batch 84/176 of epoch 16/20 - loss: 0.1629963219165802 - acc:  96.875000%\n","Training - running batch 85/176 of epoch 16/20 - loss: 0.0821196511387825 - acc:  100.000000%\n","Training - running batch 86/176 of epoch 16/20 - loss: 0.12880097329616547 - acc:  98.437500%\n","Training - running batch 87/176 of epoch 16/20 - loss: 0.0829416960477829 - acc:  100.000000%\n","Training - running batch 88/176 of epoch 16/20 - loss: 0.058427222073078156 - acc:  100.000000%\n","Training - running batch 89/176 of epoch 16/20 - loss: 0.12449517101049423 - acc:  97.656250%\n","Training - running batch 90/176 of epoch 16/20 - loss: 0.11890042573213577 - acc:  100.000000%\n","Training - running batch 91/176 of epoch 16/20 - loss: 0.19158124923706055 - acc:  96.093750%\n","Training - running batch 92/176 of epoch 16/20 - loss: 0.08440276980400085 - acc:  98.437500%\n","Training - running batch 93/176 of epoch 16/20 - loss: 0.04808450862765312 - acc:  100.000000%\n","Training - running batch 94/176 of epoch 16/20 - loss: 0.1925743818283081 - acc:  96.875000%\n","Training - running batch 95/176 of epoch 16/20 - loss: 0.086506187915802 - acc:  100.000000%\n","Training - running batch 96/176 of epoch 16/20 - loss: 0.11971911787986755 - acc:  99.218750%\n","Training - running batch 97/176 of epoch 16/20 - loss: 0.13615792989730835 - acc:  97.656250%\n","Training - running batch 98/176 of epoch 16/20 - loss: 0.06256738305091858 - acc:  100.000000%\n","Training - running batch 99/176 of epoch 16/20 - loss: 0.13027659058570862 - acc:  97.656250%\n","Training - running batch 100/176 of epoch 16/20 - loss: 0.07517871260643005 - acc:  96.875000%\n","Training - running batch 101/176 of epoch 16/20 - loss: 0.14749570190906525 - acc:  96.093750%\n","Training - running batch 102/176 of epoch 16/20 - loss: 0.18346717953681946 - acc:  96.875000%\n","Training - running batch 103/176 of epoch 16/20 - loss: 0.08097859472036362 - acc:  98.437500%\n","Training - running batch 104/176 of epoch 16/20 - loss: 0.07568821310997009 - acc:  98.437500%\n","Training - running batch 105/176 of epoch 16/20 - loss: 0.16913144290447235 - acc:  96.875000%\n","Training - running batch 106/176 of epoch 16/20 - loss: 0.06124453246593475 - acc:  100.000000%\n","Training - running batch 107/176 of epoch 16/20 - loss: 0.09297960251569748 - acc:  96.875000%\n","Training - running batch 108/176 of epoch 16/20 - loss: 0.05322261527180672 - acc:  100.000000%\n","Training - running batch 109/176 of epoch 16/20 - loss: 0.09993916004896164 - acc:  98.437500%\n","Training - running batch 110/176 of epoch 16/20 - loss: 0.16929224133491516 - acc:  97.656250%\n","Training - running batch 111/176 of epoch 16/20 - loss: 0.14202192425727844 - acc:  97.656250%\n","Training - running batch 112/176 of epoch 16/20 - loss: 0.1600015014410019 - acc:  97.656250%\n","Training - running batch 113/176 of epoch 16/20 - loss: 0.12590298056602478 - acc:  98.437500%\n","Training - running batch 114/176 of epoch 16/20 - loss: 0.059696976095438004 - acc:  98.437500%\n","Training - running batch 115/176 of epoch 16/20 - loss: 0.13629385828971863 - acc:  99.218750%\n","Training - running batch 116/176 of epoch 16/20 - loss: 0.048185329884290695 - acc:  100.000000%\n","Training - running batch 117/176 of epoch 16/20 - loss: 0.14113664627075195 - acc:  97.656250%\n","Training - running batch 118/176 of epoch 16/20 - loss: 0.07195545732975006 - acc:  99.218750%\n","Training - running batch 119/176 of epoch 16/20 - loss: 0.19690299034118652 - acc:  96.093750%\n","Training - running batch 120/176 of epoch 16/20 - loss: 0.2802879214286804 - acc:  93.750000%\n","Training - running batch 121/176 of epoch 16/20 - loss: 0.18708699941635132 - acc:  96.875000%\n","Training - running batch 122/176 of epoch 16/20 - loss: 0.11370733380317688 - acc:  96.875000%\n","Training - running batch 123/176 of epoch 16/20 - loss: 0.15894730389118195 - acc:  95.312500%\n","Training - running batch 124/176 of epoch 16/20 - loss: 0.1918952316045761 - acc:  95.312500%\n","Training - running batch 125/176 of epoch 16/20 - loss: 0.1600664108991623 - acc:  95.312500%\n","Training - running batch 126/176 of epoch 16/20 - loss: 0.0577370822429657 - acc:  100.000000%\n","Training - running batch 127/176 of epoch 16/20 - loss: 0.11139132082462311 - acc:  98.437500%\n","Training - running batch 128/176 of epoch 16/20 - loss: 0.07372332364320755 - acc:  99.218750%\n","Training - running batch 129/176 of epoch 16/20 - loss: 0.14528223872184753 - acc:  96.875000%\n","Training - running batch 130/176 of epoch 16/20 - loss: 0.1521865427494049 - acc:  96.093750%\n","Training - running batch 131/176 of epoch 16/20 - loss: 0.18615186214447021 - acc:  94.531250%\n","Training - running batch 132/176 of epoch 16/20 - loss: 0.09802152216434479 - acc:  100.000000%\n","Training - running batch 133/176 of epoch 16/20 - loss: 0.057988353073596954 - acc:  100.000000%\n","Training - running batch 134/176 of epoch 16/20 - loss: 0.16967839002609253 - acc:  95.312500%\n","Training - running batch 135/176 of epoch 16/20 - loss: 0.1051182746887207 - acc:  99.218750%\n","Training - running batch 136/176 of epoch 16/20 - loss: 0.06644339859485626 - acc:  99.218750%\n","Training - running batch 137/176 of epoch 16/20 - loss: 0.07043884694576263 - acc:  98.437500%\n","Training - running batch 138/176 of epoch 16/20 - loss: 0.17545616626739502 - acc:  96.875000%\n","Training - running batch 139/176 of epoch 16/20 - loss: 0.18557853996753693 - acc:  94.531250%\n","Training - running batch 140/176 of epoch 16/20 - loss: 0.11046683043241501 - acc:  96.875000%\n","Training - running batch 141/176 of epoch 16/20 - loss: 0.05279400944709778 - acc:  100.000000%\n","Training - running batch 142/176 of epoch 16/20 - loss: 0.1304461807012558 - acc:  100.000000%\n","Training - running batch 143/176 of epoch 16/20 - loss: 0.16202157735824585 - acc:  96.093750%\n","Training - running batch 144/176 of epoch 16/20 - loss: 0.18232455849647522 - acc:  95.312500%\n","Training - running batch 145/176 of epoch 16/20 - loss: 0.03565525263547897 - acc:  100.000000%\n","Training - running batch 146/176 of epoch 16/20 - loss: 0.08051713556051254 - acc:  100.000000%\n","Training - running batch 147/176 of epoch 16/20 - loss: 0.0866815522313118 - acc:  100.000000%\n","Training - running batch 148/176 of epoch 16/20 - loss: 0.17473946511745453 - acc:  96.093750%\n","Training - running batch 149/176 of epoch 16/20 - loss: 0.20721735060214996 - acc:  93.750000%\n","Training - running batch 150/176 of epoch 16/20 - loss: 0.18545231223106384 - acc:  96.093750%\n","Training - running batch 151/176 of epoch 16/20 - loss: 0.09244085848331451 - acc:  96.875000%\n","Training - running batch 152/176 of epoch 16/20 - loss: 0.12542319297790527 - acc:  97.656250%\n","Training - running batch 153/176 of epoch 16/20 - loss: 0.2114534229040146 - acc:  96.875000%\n","Training - running batch 154/176 of epoch 16/20 - loss: 0.11600256711244583 - acc:  98.437500%\n","Training - running batch 155/176 of epoch 16/20 - loss: 0.09179022163152695 - acc:  96.875000%\n","Training - running batch 156/176 of epoch 16/20 - loss: 0.19394218921661377 - acc:  95.312500%\n","Training - running batch 157/176 of epoch 16/20 - loss: 0.15393796563148499 - acc:  96.875000%\n","Training - running batch 158/176 of epoch 16/20 - loss: 0.20539604127407074 - acc:  95.312500%\n","Training - running batch 159/176 of epoch 16/20 - loss: 0.08460524678230286 - acc:  99.218750%\n","Training - running batch 160/176 of epoch 16/20 - loss: 0.3282064199447632 - acc:  89.062500%\n","Training - running batch 161/176 of epoch 16/20 - loss: 0.17734815180301666 - acc:  98.437500%\n","Training - running batch 162/176 of epoch 16/20 - loss: 0.17364493012428284 - acc:  96.875000%\n","Training - running batch 163/176 of epoch 16/20 - loss: 0.09380821138620377 - acc:  98.437500%\n","Training - running batch 164/176 of epoch 16/20 - loss: 0.08251215517520905 - acc:  100.000000%\n","Training - running batch 165/176 of epoch 16/20 - loss: 0.21529939770698547 - acc:  95.312500%\n","Training - running batch 166/176 of epoch 16/20 - loss: 0.07826213538646698 - acc:  99.218750%\n","Training - running batch 167/176 of epoch 16/20 - loss: 0.19054540991783142 - acc:  97.656250%\n","Training - running batch 168/176 of epoch 16/20 - loss: 0.14818212389945984 - acc:  95.312500%\n","Training - running batch 169/176 of epoch 16/20 - loss: 0.06644699722528458 - acc:  99.218750%\n","Training - running batch 170/176 of epoch 16/20 - loss: 0.21862633526325226 - acc:  94.531250%\n","Training - running batch 171/176 of epoch 16/20 - loss: 0.13150320947170258 - acc:  97.656250%\n","Training - running batch 172/176 of epoch 16/20 - loss: 0.18853749334812164 - acc:  93.750000%\n","Training - running batch 173/176 of epoch 16/20 - loss: 0.1574956476688385 - acc:  96.093750%\n","Training - running batch 174/176 of epoch 16/20 - loss: 0.11516311764717102 - acc:  98.437500%\n","Training - running batch 175/176 of epoch 16/20 - loss: 0.18192395567893982 - acc:  96.774194%\n","Testing - acc:  0.729171%\n","Training - running batch 0/176 of epoch 17/20 - loss: 0.14794382452964783 - acc:  96.875000%\n","Training - running batch 1/176 of epoch 17/20 - loss: 0.1544765830039978 - acc:  96.875000%\n","Training - running batch 2/176 of epoch 17/20 - loss: 0.16438622772693634 - acc:  97.656250%\n","Training - running batch 3/176 of epoch 17/20 - loss: 0.15101678669452667 - acc:  99.218750%\n","Training - running batch 4/176 of epoch 17/20 - loss: 0.10346580296754837 - acc:  97.656250%\n","Training - running batch 5/176 of epoch 17/20 - loss: 0.08830852806568146 - acc:  100.000000%\n","Training - running batch 6/176 of epoch 17/20 - loss: 0.06901545822620392 - acc:  98.437500%\n","Training - running batch 7/176 of epoch 17/20 - loss: 0.14651091396808624 - acc:  96.875000%\n","Training - running batch 8/176 of epoch 17/20 - loss: 0.050752583891153336 - acc:  100.000000%\n","Training - running batch 9/176 of epoch 17/20 - loss: 0.17272979021072388 - acc:  96.875000%\n","Training - running batch 10/176 of epoch 17/20 - loss: 0.21914099156856537 - acc:  93.750000%\n","Training - running batch 11/176 of epoch 17/20 - loss: 0.16782329976558685 - acc:  98.437500%\n","Training - running batch 12/176 of epoch 17/20 - loss: 0.06677529960870743 - acc:  99.218750%\n","Training - running batch 13/176 of epoch 17/20 - loss: 0.16163332760334015 - acc:  98.437500%\n","Training - running batch 14/176 of epoch 17/20 - loss: 0.054294124245643616 - acc:  99.218750%\n","Training - running batch 15/176 of epoch 17/20 - loss: 0.13435426354408264 - acc:  96.875000%\n","Training - running batch 16/176 of epoch 17/20 - loss: 0.10017599165439606 - acc:  100.000000%\n","Training - running batch 17/176 of epoch 17/20 - loss: 0.07913525402545929 - acc:  100.000000%\n","Training - running batch 18/176 of epoch 17/20 - loss: 0.07129902392625809 - acc:  100.000000%\n","Training - running batch 19/176 of epoch 17/20 - loss: 0.17947882413864136 - acc:  93.750000%\n","Training - running batch 20/176 of epoch 17/20 - loss: 0.2901778817176819 - acc:  91.406250%\n","Training - running batch 21/176 of epoch 17/20 - loss: 0.06258304417133331 - acc:  98.437500%\n","Training - running batch 22/176 of epoch 17/20 - loss: 0.04873025417327881 - acc:  99.218750%\n","Training - running batch 23/176 of epoch 17/20 - loss: 0.041536059230566025 - acc:  100.000000%\n","Training - running batch 24/176 of epoch 17/20 - loss: 0.07394048571586609 - acc:  100.000000%\n","Training - running batch 25/176 of epoch 17/20 - loss: 0.23697742819786072 - acc:  96.875000%\n","Training - running batch 26/176 of epoch 17/20 - loss: 0.10006741434335709 - acc:  98.437500%\n","Training - running batch 27/176 of epoch 17/20 - loss: 0.17920169234275818 - acc:  93.750000%\n","Training - running batch 28/176 of epoch 17/20 - loss: 0.12589237093925476 - acc:  96.875000%\n","Training - running batch 29/176 of epoch 17/20 - loss: 0.10858049243688583 - acc:  98.437500%\n","Training - running batch 30/176 of epoch 17/20 - loss: 0.17485712468624115 - acc:  96.093750%\n","Training - running batch 31/176 of epoch 17/20 - loss: 0.07884909212589264 - acc:  100.000000%\n","Training - running batch 32/176 of epoch 17/20 - loss: 0.1457713097333908 - acc:  100.000000%\n","Training - running batch 33/176 of epoch 17/20 - loss: 0.14324799180030823 - acc:  97.656250%\n","Training - running batch 34/176 of epoch 17/20 - loss: 0.1744259148836136 - acc:  96.093750%\n","Training - running batch 35/176 of epoch 17/20 - loss: 0.22683152556419373 - acc:  94.531250%\n","Training - running batch 36/176 of epoch 17/20 - loss: 0.1855418086051941 - acc:  96.093750%\n","Training - running batch 37/176 of epoch 17/20 - loss: 0.17408044636249542 - acc:  96.093750%\n","Training - running batch 38/176 of epoch 17/20 - loss: 0.06359159201383591 - acc:  100.000000%\n","Training - running batch 39/176 of epoch 17/20 - loss: 0.18509452044963837 - acc:  96.875000%\n","Training - running batch 40/176 of epoch 17/20 - loss: 0.09252246469259262 - acc:  96.875000%\n","Training - running batch 41/176 of epoch 17/20 - loss: 0.03949045389890671 - acc:  100.000000%\n","Training - running batch 42/176 of epoch 17/20 - loss: 0.20458918809890747 - acc:  96.093750%\n","Training - running batch 43/176 of epoch 17/20 - loss: 0.0600791871547699 - acc:  100.000000%\n","Training - running batch 44/176 of epoch 17/20 - loss: 0.04571113735437393 - acc:  100.000000%\n","Training - running batch 45/176 of epoch 17/20 - loss: 0.07347520440816879 - acc:  98.437500%\n","Training - running batch 46/176 of epoch 17/20 - loss: 0.05655480921268463 - acc:  100.000000%\n","Training - running batch 47/176 of epoch 17/20 - loss: 0.12882116436958313 - acc:  97.656250%\n","Training - running batch 48/176 of epoch 17/20 - loss: 0.10446688532829285 - acc:  100.000000%\n","Training - running batch 49/176 of epoch 17/20 - loss: 0.13025566935539246 - acc:  96.875000%\n","Training - running batch 50/176 of epoch 17/20 - loss: 0.11220043897628784 - acc:  99.218750%\n","Training - running batch 51/176 of epoch 17/20 - loss: 0.07673867046833038 - acc:  99.218750%\n","Training - running batch 52/176 of epoch 17/20 - loss: 0.10768161714076996 - acc:  98.437500%\n","Training - running batch 53/176 of epoch 17/20 - loss: 0.1699867993593216 - acc:  96.875000%\n","Training - running batch 54/176 of epoch 17/20 - loss: 0.07180766761302948 - acc:  99.218750%\n","Training - running batch 55/176 of epoch 17/20 - loss: 0.15163780748844147 - acc:  96.875000%\n","Training - running batch 56/176 of epoch 17/20 - loss: 0.10795316100120544 - acc:  99.218750%\n","Training - running batch 57/176 of epoch 17/20 - loss: 0.09723074734210968 - acc:  98.437500%\n","Training - running batch 58/176 of epoch 17/20 - loss: 0.13780611753463745 - acc:  97.656250%\n","Training - running batch 59/176 of epoch 17/20 - loss: 0.10588616132736206 - acc:  98.437500%\n","Training - running batch 60/176 of epoch 17/20 - loss: 0.14218102395534515 - acc:  94.531250%\n","Training - running batch 61/176 of epoch 17/20 - loss: 0.053930334746837616 - acc:  100.000000%\n","Training - running batch 62/176 of epoch 17/20 - loss: 0.10753733664751053 - acc:  99.218750%\n","Training - running batch 63/176 of epoch 17/20 - loss: 0.11767848581075668 - acc:  97.656250%\n","Training - running batch 64/176 of epoch 17/20 - loss: 0.19249479472637177 - acc:  96.875000%\n","Training - running batch 65/176 of epoch 17/20 - loss: 0.1257423758506775 - acc:  97.656250%\n","Training - running batch 66/176 of epoch 17/20 - loss: 0.19406309723854065 - acc:  95.312500%\n","Training - running batch 67/176 of epoch 17/20 - loss: 0.1398976892232895 - acc:  96.875000%\n","Training - running batch 68/176 of epoch 17/20 - loss: 0.11689628660678864 - acc:  97.656250%\n","Training - running batch 69/176 of epoch 17/20 - loss: 0.06773953139781952 - acc:  100.000000%\n","Training - running batch 70/176 of epoch 17/20 - loss: 0.11059077084064484 - acc:  98.437500%\n","Training - running batch 71/176 of epoch 17/20 - loss: 0.11405585706233978 - acc:  98.437500%\n","Training - running batch 72/176 of epoch 17/20 - loss: 0.08538731932640076 - acc:  98.437500%\n","Training - running batch 73/176 of epoch 17/20 - loss: 0.0945330336689949 - acc:  97.656250%\n","Training - running batch 74/176 of epoch 17/20 - loss: 0.08577512204647064 - acc:  97.656250%\n","Training - running batch 75/176 of epoch 17/20 - loss: 0.12277096509933472 - acc:  98.437500%\n","Training - running batch 76/176 of epoch 17/20 - loss: 0.11068926751613617 - acc:  96.875000%\n","Training - running batch 77/176 of epoch 17/20 - loss: 0.13418206572532654 - acc:  96.875000%\n","Training - running batch 78/176 of epoch 17/20 - loss: 0.12522181868553162 - acc:  98.437500%\n","Training - running batch 79/176 of epoch 17/20 - loss: 0.05234597623348236 - acc:  100.000000%\n","Training - running batch 80/176 of epoch 17/20 - loss: 0.15894128382205963 - acc:  96.093750%\n","Training - running batch 81/176 of epoch 17/20 - loss: 0.11146064847707748 - acc:  100.000000%\n","Training - running batch 82/176 of epoch 17/20 - loss: 0.19696125388145447 - acc:  96.875000%\n","Training - running batch 83/176 of epoch 17/20 - loss: 0.06533901393413544 - acc:  100.000000%\n","Training - running batch 84/176 of epoch 17/20 - loss: 0.15046435594558716 - acc:  96.875000%\n","Training - running batch 85/176 of epoch 17/20 - loss: 0.2386312335729599 - acc:  96.875000%\n","Training - running batch 86/176 of epoch 17/20 - loss: 0.09636946022510529 - acc:  98.437500%\n","Training - running batch 87/176 of epoch 17/20 - loss: 0.169319748878479 - acc:  96.093750%\n","Training - running batch 88/176 of epoch 17/20 - loss: 0.24367037415504456 - acc:  90.625000%\n","Training - running batch 89/176 of epoch 17/20 - loss: 0.048526037484407425 - acc:  100.000000%\n","Training - running batch 90/176 of epoch 17/20 - loss: 0.055803559720516205 - acc:  100.000000%\n","Training - running batch 91/176 of epoch 17/20 - loss: 0.15959952771663666 - acc:  98.437500%\n","Training - running batch 92/176 of epoch 17/20 - loss: 0.16514338552951813 - acc:  95.312500%\n","Training - running batch 93/176 of epoch 17/20 - loss: 0.06448496878147125 - acc:  98.437500%\n","Training - running batch 94/176 of epoch 17/20 - loss: 0.2309505194425583 - acc:  96.093750%\n","Training - running batch 95/176 of epoch 17/20 - loss: 0.17238382995128632 - acc:  95.312500%\n","Training - running batch 96/176 of epoch 17/20 - loss: 0.12973140180110931 - acc:  98.437500%\n","Training - running batch 97/176 of epoch 17/20 - loss: 0.11829668283462524 - acc:  98.437500%\n","Training - running batch 98/176 of epoch 17/20 - loss: 0.13309212028980255 - acc:  98.437500%\n","Training - running batch 99/176 of epoch 17/20 - loss: 0.17709720134735107 - acc:  96.093750%\n","Training - running batch 100/176 of epoch 17/20 - loss: 0.07126326858997345 - acc:  99.218750%\n","Training - running batch 101/176 of epoch 17/20 - loss: 0.10709898173809052 - acc:  98.437500%\n","Training - running batch 102/176 of epoch 17/20 - loss: 0.10140092670917511 - acc:  99.218750%\n","Training - running batch 103/176 of epoch 17/20 - loss: 0.07478991895914078 - acc:  99.218750%\n","Training - running batch 104/176 of epoch 17/20 - loss: 0.16153790056705475 - acc:  96.093750%\n","Training - running batch 105/176 of epoch 17/20 - loss: 0.1346430629491806 - acc:  98.437500%\n","Training - running batch 106/176 of epoch 17/20 - loss: 0.1623559147119522 - acc:  96.875000%\n","Training - running batch 107/176 of epoch 17/20 - loss: 0.21041899919509888 - acc:  93.750000%\n","Training - running batch 108/176 of epoch 17/20 - loss: 0.07760989665985107 - acc:  99.218750%\n","Training - running batch 109/176 of epoch 17/20 - loss: 0.058694612234830856 - acc:  99.218750%\n","Training - running batch 110/176 of epoch 17/20 - loss: 0.06686292588710785 - acc:  100.000000%\n","Training - running batch 111/176 of epoch 17/20 - loss: 0.0508720837533474 - acc:  98.437500%\n","Training - running batch 112/176 of epoch 17/20 - loss: 0.08886794745922089 - acc:  100.000000%\n","Training - running batch 113/176 of epoch 17/20 - loss: 0.04680971801280975 - acc:  100.000000%\n","Training - running batch 114/176 of epoch 17/20 - loss: 0.09857893735170364 - acc:  98.437500%\n","Training - running batch 115/176 of epoch 17/20 - loss: 0.14115764200687408 - acc:  95.312500%\n","Training - running batch 116/176 of epoch 17/20 - loss: 0.06184199079871178 - acc:  100.000000%\n","Training - running batch 117/176 of epoch 17/20 - loss: 0.0576571561396122 - acc:  100.000000%\n","Training - running batch 118/176 of epoch 17/20 - loss: 0.19092078506946564 - acc:  94.531250%\n","Training - running batch 119/176 of epoch 17/20 - loss: 0.1659230887889862 - acc:  96.875000%\n","Training - running batch 120/176 of epoch 17/20 - loss: 0.19385406374931335 - acc:  94.531250%\n","Training - running batch 121/176 of epoch 17/20 - loss: 0.09162130206823349 - acc:  98.437500%\n","Training - running batch 122/176 of epoch 17/20 - loss: 0.07017625123262405 - acc:  100.000000%\n","Training - running batch 123/176 of epoch 17/20 - loss: 0.1751532107591629 - acc:  96.875000%\n","Training - running batch 124/176 of epoch 17/20 - loss: 0.11968278884887695 - acc:  96.875000%\n","Training - running batch 125/176 of epoch 17/20 - loss: 0.04646304249763489 - acc:  100.000000%\n","Training - running batch 126/176 of epoch 17/20 - loss: 0.18830060958862305 - acc:  95.312500%\n","Training - running batch 127/176 of epoch 17/20 - loss: 0.04975765570998192 - acc:  100.000000%\n","Training - running batch 128/176 of epoch 17/20 - loss: 0.10739879310131073 - acc:  96.875000%\n","Training - running batch 129/176 of epoch 17/20 - loss: 0.21145819127559662 - acc:  94.531250%\n","Training - running batch 130/176 of epoch 17/20 - loss: 0.1818046271800995 - acc:  96.093750%\n","Training - running batch 131/176 of epoch 17/20 - loss: 0.14753358066082 - acc:  98.437500%\n","Training - running batch 132/176 of epoch 17/20 - loss: 0.10008350014686584 - acc:  99.218750%\n","Training - running batch 133/176 of epoch 17/20 - loss: 0.07332628965377808 - acc:  99.218750%\n","Training - running batch 134/176 of epoch 17/20 - loss: 0.05192539468407631 - acc:  100.000000%\n","Training - running batch 135/176 of epoch 17/20 - loss: 0.09705191850662231 - acc:  99.218750%\n","Training - running batch 136/176 of epoch 17/20 - loss: 0.11744501441717148 - acc:  99.218750%\n","Training - running batch 137/176 of epoch 17/20 - loss: 0.13412733376026154 - acc:  100.000000%\n","Training - running batch 138/176 of epoch 17/20 - loss: 0.2048308402299881 - acc:  96.093750%\n","Training - running batch 139/176 of epoch 17/20 - loss: 0.22777993977069855 - acc:  93.750000%\n","Training - running batch 140/176 of epoch 17/20 - loss: 0.0864356979727745 - acc:  98.437500%\n","Training - running batch 141/176 of epoch 17/20 - loss: 0.11851579695940018 - acc:  98.437500%\n","Training - running batch 142/176 of epoch 17/20 - loss: 0.19170331954956055 - acc:  95.312500%\n","Training - running batch 143/176 of epoch 17/20 - loss: 0.14943136274814606 - acc:  97.656250%\n","Training - running batch 144/176 of epoch 17/20 - loss: 0.07551095634698868 - acc:  99.218750%\n","Training - running batch 145/176 of epoch 17/20 - loss: 0.16143660247325897 - acc:  96.875000%\n","Training - running batch 146/176 of epoch 17/20 - loss: 0.07854855805635452 - acc:  98.437500%\n","Training - running batch 147/176 of epoch 17/20 - loss: 0.12282159924507141 - acc:  96.875000%\n","Training - running batch 148/176 of epoch 17/20 - loss: 0.13693922758102417 - acc:  98.437500%\n","Training - running batch 149/176 of epoch 17/20 - loss: 0.19100213050842285 - acc:  95.312500%\n","Training - running batch 150/176 of epoch 17/20 - loss: 0.13914187252521515 - acc:  98.437500%\n","Training - running batch 151/176 of epoch 17/20 - loss: 0.10249429941177368 - acc:  99.218750%\n","Training - running batch 152/176 of epoch 17/20 - loss: 0.12965185940265656 - acc:  96.875000%\n","Training - running batch 153/176 of epoch 17/20 - loss: 0.23777905106544495 - acc:  96.093750%\n","Training - running batch 154/176 of epoch 17/20 - loss: 0.09981880336999893 - acc:  100.000000%\n","Training - running batch 155/176 of epoch 17/20 - loss: 0.15867844223976135 - acc:  98.437500%\n","Training - running batch 156/176 of epoch 17/20 - loss: 0.14000707864761353 - acc:  98.437500%\n","Training - running batch 157/176 of epoch 17/20 - loss: 0.06136833131313324 - acc:  98.437500%\n","Training - running batch 158/176 of epoch 17/20 - loss: 0.06754955649375916 - acc:  100.000000%\n","Training - running batch 159/176 of epoch 17/20 - loss: 0.0739300549030304 - acc:  97.656250%\n","Training - running batch 160/176 of epoch 17/20 - loss: 0.11549017578363419 - acc:  97.656250%\n","Training - running batch 161/176 of epoch 17/20 - loss: 0.05380065739154816 - acc:  98.437500%\n","Training - running batch 162/176 of epoch 17/20 - loss: 0.10152966529130936 - acc:  96.875000%\n","Training - running batch 163/176 of epoch 17/20 - loss: 0.034628741443157196 - acc:  100.000000%\n","Training - running batch 164/176 of epoch 17/20 - loss: 0.13645723462104797 - acc:  98.437500%\n","Training - running batch 165/176 of epoch 17/20 - loss: 0.15516313910484314 - acc:  98.437500%\n","Training - running batch 166/176 of epoch 17/20 - loss: 0.0709124356508255 - acc:  100.000000%\n","Training - running batch 167/176 of epoch 17/20 - loss: 0.11365800350904465 - acc:  100.000000%\n","Training - running batch 168/176 of epoch 17/20 - loss: 0.11961120367050171 - acc:  98.437500%\n","Training - running batch 169/176 of epoch 17/20 - loss: 0.2601903975009918 - acc:  94.531250%\n","Training - running batch 170/176 of epoch 17/20 - loss: 0.050620000809431076 - acc:  100.000000%\n","Training - running batch 171/176 of epoch 17/20 - loss: 0.08222834020853043 - acc:  97.656250%\n","Training - running batch 172/176 of epoch 17/20 - loss: 0.0709436908364296 - acc:  100.000000%\n","Training - running batch 173/176 of epoch 17/20 - loss: 0.06346190720796585 - acc:  100.000000%\n","Training - running batch 174/176 of epoch 17/20 - loss: 0.1253637671470642 - acc:  99.218750%\n","Training - running batch 175/176 of epoch 17/20 - loss: 0.14554408192634583 - acc:  95.161290%\n","Testing - acc:  0.732844%\n","Training - running batch 0/176 of epoch 18/20 - loss: 0.09483259171247482 - acc:  98.437500%\n","Training - running batch 1/176 of epoch 18/20 - loss: 0.09318667650222778 - acc:  99.218750%\n","Training - running batch 2/176 of epoch 18/20 - loss: 0.12626439332962036 - acc:  97.656250%\n","Training - running batch 3/176 of epoch 18/20 - loss: 0.17419779300689697 - acc:  96.875000%\n","Training - running batch 4/176 of epoch 18/20 - loss: 0.17589934170246124 - acc:  95.312500%\n","Training - running batch 5/176 of epoch 18/20 - loss: 0.06670131534337997 - acc:  100.000000%\n","Training - running batch 6/176 of epoch 18/20 - loss: 0.11317720264196396 - acc:  100.000000%\n","Training - running batch 7/176 of epoch 18/20 - loss: 0.07508952915668488 - acc:  100.000000%\n","Training - running batch 8/176 of epoch 18/20 - loss: 0.05144897848367691 - acc:  98.437500%\n","Training - running batch 9/176 of epoch 18/20 - loss: 0.22307823598384857 - acc:  93.750000%\n","Training - running batch 10/176 of epoch 18/20 - loss: 0.05844506621360779 - acc:  98.437500%\n","Training - running batch 11/176 of epoch 18/20 - loss: 0.05875115096569061 - acc:  100.000000%\n","Training - running batch 12/176 of epoch 18/20 - loss: 0.11784565448760986 - acc:  100.000000%\n","Training - running batch 13/176 of epoch 18/20 - loss: 0.11846467852592468 - acc:  98.437500%\n","Training - running batch 14/176 of epoch 18/20 - loss: 0.18064245581626892 - acc:  96.093750%\n","Training - running batch 15/176 of epoch 18/20 - loss: 0.10007239133119583 - acc:  99.218750%\n","Training - running batch 16/176 of epoch 18/20 - loss: 0.059813376516103745 - acc:  98.437500%\n","Training - running batch 17/176 of epoch 18/20 - loss: 0.18011972308158875 - acc:  96.875000%\n","Training - running batch 18/176 of epoch 18/20 - loss: 0.09541353583335876 - acc:  98.437500%\n","Training - running batch 19/176 of epoch 18/20 - loss: 0.15058639645576477 - acc:  97.656250%\n","Training - running batch 20/176 of epoch 18/20 - loss: 0.07761967927217484 - acc:  100.000000%\n","Training - running batch 21/176 of epoch 18/20 - loss: 0.09580788016319275 - acc:  98.437500%\n","Training - running batch 22/176 of epoch 18/20 - loss: 0.04965142160654068 - acc:  99.218750%\n","Training - running batch 23/176 of epoch 18/20 - loss: 0.06101216375827789 - acc:  100.000000%\n","Training - running batch 24/176 of epoch 18/20 - loss: 0.10684452950954437 - acc:  99.218750%\n","Training - running batch 25/176 of epoch 18/20 - loss: 0.06027500703930855 - acc:  100.000000%\n","Training - running batch 26/176 of epoch 18/20 - loss: 0.07300537824630737 - acc:  100.000000%\n","Training - running batch 27/176 of epoch 18/20 - loss: 0.22835294902324677 - acc:  94.531250%\n","Training - running batch 28/176 of epoch 18/20 - loss: 0.07424617558717728 - acc:  98.437500%\n","Training - running batch 29/176 of epoch 18/20 - loss: 0.19491897523403168 - acc:  95.312500%\n","Training - running batch 30/176 of epoch 18/20 - loss: 0.10738831013441086 - acc:  100.000000%\n","Training - running batch 31/176 of epoch 18/20 - loss: 0.05896153301000595 - acc:  100.000000%\n","Training - running batch 32/176 of epoch 18/20 - loss: 0.10462259501218796 - acc:  98.437500%\n","Training - running batch 33/176 of epoch 18/20 - loss: 0.09047306329011917 - acc:  97.656250%\n","Training - running batch 34/176 of epoch 18/20 - loss: 0.042853180319070816 - acc:  100.000000%\n","Training - running batch 35/176 of epoch 18/20 - loss: 0.18207228183746338 - acc:  95.312500%\n","Training - running batch 36/176 of epoch 18/20 - loss: 0.13110938668251038 - acc:  96.875000%\n","Training - running batch 37/176 of epoch 18/20 - loss: 0.051918357610702515 - acc:  100.000000%\n","Training - running batch 38/176 of epoch 18/20 - loss: 0.14383068680763245 - acc:  96.093750%\n","Training - running batch 39/176 of epoch 18/20 - loss: 0.1135396659374237 - acc:  98.437500%\n","Training - running batch 40/176 of epoch 18/20 - loss: 0.1565127670764923 - acc:  98.437500%\n","Training - running batch 41/176 of epoch 18/20 - loss: 0.12247283011674881 - acc:  98.437500%\n","Training - running batch 42/176 of epoch 18/20 - loss: 0.08836188167333603 - acc:  100.000000%\n","Training - running batch 43/176 of epoch 18/20 - loss: 0.15214549005031586 - acc:  97.656250%\n","Training - running batch 44/176 of epoch 18/20 - loss: 0.06969344615936279 - acc:  98.437500%\n","Training - running batch 45/176 of epoch 18/20 - loss: 0.04384618252515793 - acc:  100.000000%\n","Training - running batch 46/176 of epoch 18/20 - loss: 0.0481402650475502 - acc:  100.000000%\n","Training - running batch 47/176 of epoch 18/20 - loss: 0.09993979334831238 - acc:  100.000000%\n","Training - running batch 48/176 of epoch 18/20 - loss: 0.12132175266742706 - acc:  100.000000%\n","Training - running batch 49/176 of epoch 18/20 - loss: 0.10388899594545364 - acc:  98.437500%\n","Training - running batch 50/176 of epoch 18/20 - loss: 0.060289740562438965 - acc:  97.656250%\n","Training - running batch 51/176 of epoch 18/20 - loss: 0.11774226278066635 - acc:  98.437500%\n","Training - running batch 52/176 of epoch 18/20 - loss: 0.13862276077270508 - acc:  96.093750%\n","Training - running batch 53/176 of epoch 18/20 - loss: 0.20318861305713654 - acc:  94.531250%\n","Training - running batch 54/176 of epoch 18/20 - loss: 0.23532477021217346 - acc:  93.750000%\n","Training - running batch 55/176 of epoch 18/20 - loss: 0.14406448602676392 - acc:  96.093750%\n","Training - running batch 56/176 of epoch 18/20 - loss: 0.12368758767843246 - acc:  98.437500%\n","Training - running batch 57/176 of epoch 18/20 - loss: 0.11440646648406982 - acc:  98.437500%\n","Training - running batch 58/176 of epoch 18/20 - loss: 0.07122322916984558 - acc:  100.000000%\n","Training - running batch 59/176 of epoch 18/20 - loss: 0.09201214462518692 - acc:  100.000000%\n","Training - running batch 60/176 of epoch 18/20 - loss: 0.18886496126651764 - acc:  96.093750%\n","Training - running batch 61/176 of epoch 18/20 - loss: 0.05922911688685417 - acc:  100.000000%\n","Training - running batch 62/176 of epoch 18/20 - loss: 0.08050765097141266 - acc:  100.000000%\n","Training - running batch 63/176 of epoch 18/20 - loss: 0.12825217843055725 - acc:  97.656250%\n","Training - running batch 64/176 of epoch 18/20 - loss: 0.04880838096141815 - acc:  100.000000%\n","Training - running batch 65/176 of epoch 18/20 - loss: 0.06194942444562912 - acc:  99.218750%\n","Training - running batch 66/176 of epoch 18/20 - loss: 0.04937128722667694 - acc:  100.000000%\n","Training - running batch 67/176 of epoch 18/20 - loss: 0.13268814980983734 - acc:  96.875000%\n","Training - running batch 68/176 of epoch 18/20 - loss: 0.030332686379551888 - acc:  100.000000%\n","Training - running batch 69/176 of epoch 18/20 - loss: 0.10099222511053085 - acc:  96.875000%\n","Training - running batch 70/176 of epoch 18/20 - loss: 0.16747261583805084 - acc:  98.437500%\n","Training - running batch 71/176 of epoch 18/20 - loss: 0.1537860929965973 - acc:  96.875000%\n","Training - running batch 72/176 of epoch 18/20 - loss: 0.11894166469573975 - acc:  99.218750%\n","Training - running batch 73/176 of epoch 18/20 - loss: 0.08580575883388519 - acc:  100.000000%\n","Training - running batch 74/176 of epoch 18/20 - loss: 0.13362476229667664 - acc:  96.093750%\n","Training - running batch 75/176 of epoch 18/20 - loss: 0.10053485631942749 - acc:  99.218750%\n","Training - running batch 76/176 of epoch 18/20 - loss: 0.16317740082740784 - acc:  96.093750%\n","Training - running batch 77/176 of epoch 18/20 - loss: 0.15638019144535065 - acc:  95.312500%\n","Training - running batch 78/176 of epoch 18/20 - loss: 0.14558106660842896 - acc:  97.656250%\n","Training - running batch 79/176 of epoch 18/20 - loss: 0.2549605965614319 - acc:  95.312500%\n","Training - running batch 80/176 of epoch 18/20 - loss: 0.06151992082595825 - acc:  98.437500%\n","Training - running batch 81/176 of epoch 18/20 - loss: 0.16945801675319672 - acc:  96.875000%\n","Training - running batch 82/176 of epoch 18/20 - loss: 0.1875058114528656 - acc:  96.093750%\n","Training - running batch 83/176 of epoch 18/20 - loss: 0.21660234034061432 - acc:  95.312500%\n","Training - running batch 84/176 of epoch 18/20 - loss: 0.12139461934566498 - acc:  97.656250%\n","Training - running batch 85/176 of epoch 18/20 - loss: 0.1197606548666954 - acc:  99.218750%\n","Training - running batch 86/176 of epoch 18/20 - loss: 0.07209983468055725 - acc:  98.437500%\n","Training - running batch 87/176 of epoch 18/20 - loss: 0.1933962106704712 - acc:  96.093750%\n","Training - running batch 88/176 of epoch 18/20 - loss: 0.1701827049255371 - acc:  96.875000%\n","Training - running batch 89/176 of epoch 18/20 - loss: 0.07053819298744202 - acc:  98.437500%\n","Training - running batch 90/176 of epoch 18/20 - loss: 0.09390002489089966 - acc:  97.656250%\n","Training - running batch 91/176 of epoch 18/20 - loss: 0.07441529631614685 - acc:  98.437500%\n","Training - running batch 92/176 of epoch 18/20 - loss: 0.1560424566268921 - acc:  96.875000%\n","Training - running batch 93/176 of epoch 18/20 - loss: 0.11645498871803284 - acc:  96.093750%\n","Training - running batch 94/176 of epoch 18/20 - loss: 0.13188336789608002 - acc:  97.656250%\n","Training - running batch 95/176 of epoch 18/20 - loss: 0.21531115472316742 - acc:  93.750000%\n","Training - running batch 96/176 of epoch 18/20 - loss: 0.0721266120672226 - acc:  99.218750%\n","Training - running batch 97/176 of epoch 18/20 - loss: 0.13168033957481384 - acc:  96.875000%\n","Training - running batch 98/176 of epoch 18/20 - loss: 0.10168662667274475 - acc:  96.875000%\n","Training - running batch 99/176 of epoch 18/20 - loss: 0.09706807881593704 - acc:  98.437500%\n","Training - running batch 100/176 of epoch 18/20 - loss: 0.10066957026720047 - acc:  100.000000%\n","Training - running batch 101/176 of epoch 18/20 - loss: 0.041265614330768585 - acc:  100.000000%\n","Training - running batch 102/176 of epoch 18/20 - loss: 0.10420890897512436 - acc:  98.437500%\n","Training - running batch 103/176 of epoch 18/20 - loss: 0.18999601900577545 - acc:  96.875000%\n","Training - running batch 104/176 of epoch 18/20 - loss: 0.06541023403406143 - acc:  100.000000%\n","Training - running batch 105/176 of epoch 18/20 - loss: 0.05376740172505379 - acc:  100.000000%\n","Training - running batch 106/176 of epoch 18/20 - loss: 0.06300748884677887 - acc:  100.000000%\n","Training - running batch 107/176 of epoch 18/20 - loss: 0.14870291948318481 - acc:  97.656250%\n","Training - running batch 108/176 of epoch 18/20 - loss: 0.09108294546604156 - acc:  99.218750%\n","Training - running batch 109/176 of epoch 18/20 - loss: 0.10032887756824493 - acc:  100.000000%\n","Training - running batch 110/176 of epoch 18/20 - loss: 0.07210481911897659 - acc:  99.218750%\n","Training - running batch 111/176 of epoch 18/20 - loss: 0.0952979326248169 - acc:  97.656250%\n","Training - running batch 112/176 of epoch 18/20 - loss: 0.1684529185295105 - acc:  97.656250%\n","Training - running batch 113/176 of epoch 18/20 - loss: 0.23691026866436005 - acc:  96.875000%\n","Training - running batch 114/176 of epoch 18/20 - loss: 0.0978701189160347 - acc:  99.218750%\n","Training - running batch 115/176 of epoch 18/20 - loss: 0.15242697298526764 - acc:  98.437500%\n","Training - running batch 116/176 of epoch 18/20 - loss: 0.08110573142766953 - acc:  97.656250%\n","Training - running batch 117/176 of epoch 18/20 - loss: 0.06894928216934204 - acc:  98.437500%\n","Training - running batch 118/176 of epoch 18/20 - loss: 0.10763940960168839 - acc:  97.656250%\n","Training - running batch 119/176 of epoch 18/20 - loss: 0.06475601345300674 - acc:  98.437500%\n","Training - running batch 120/176 of epoch 18/20 - loss: 0.25421658158302307 - acc:  92.187500%\n","Training - running batch 121/176 of epoch 18/20 - loss: 0.12505078315734863 - acc:  98.437500%\n","Training - running batch 122/176 of epoch 18/20 - loss: 0.1238693818449974 - acc:  96.875000%\n","Training - running batch 123/176 of epoch 18/20 - loss: 0.1021391898393631 - acc:  99.218750%\n","Training - running batch 124/176 of epoch 18/20 - loss: 0.05567045509815216 - acc:  100.000000%\n","Training - running batch 125/176 of epoch 18/20 - loss: 0.13205593824386597 - acc:  97.656250%\n","Training - running batch 126/176 of epoch 18/20 - loss: 0.1387297809123993 - acc:  95.312500%\n","Training - running batch 127/176 of epoch 18/20 - loss: 0.08768751472234726 - acc:  96.093750%\n","Training - running batch 128/176 of epoch 18/20 - loss: 0.10582038760185242 - acc:  100.000000%\n","Training - running batch 129/176 of epoch 18/20 - loss: 0.09085593372583389 - acc:  98.437500%\n","Training - running batch 130/176 of epoch 18/20 - loss: 0.10426421463489532 - acc:  98.437500%\n","Training - running batch 131/176 of epoch 18/20 - loss: 0.231599360704422 - acc:  96.875000%\n","Training - running batch 132/176 of epoch 18/20 - loss: 0.17073602974414825 - acc:  96.875000%\n","Training - running batch 133/176 of epoch 18/20 - loss: 0.0805603563785553 - acc:  98.437500%\n","Training - running batch 134/176 of epoch 18/20 - loss: 0.1462848037481308 - acc:  96.093750%\n","Training - running batch 135/176 of epoch 18/20 - loss: 0.14193011820316315 - acc:  94.531250%\n","Training - running batch 136/176 of epoch 18/20 - loss: 0.12714239954948425 - acc:  97.656250%\n","Training - running batch 137/176 of epoch 18/20 - loss: 0.07235479354858398 - acc:  98.437500%\n","Training - running batch 138/176 of epoch 18/20 - loss: 0.0889165997505188 - acc:  100.000000%\n","Training - running batch 139/176 of epoch 18/20 - loss: 0.07095754891633987 - acc:  100.000000%\n","Training - running batch 140/176 of epoch 18/20 - loss: 0.12813712656497955 - acc:  98.437500%\n","Training - running batch 141/176 of epoch 18/20 - loss: 0.13932716846466064 - acc:  96.875000%\n","Training - running batch 142/176 of epoch 18/20 - loss: 0.0871800035238266 - acc:  98.437500%\n","Training - running batch 143/176 of epoch 18/20 - loss: 0.11882482469081879 - acc:  96.875000%\n","Training - running batch 144/176 of epoch 18/20 - loss: 0.1155732050538063 - acc:  100.000000%\n","Training - running batch 145/176 of epoch 18/20 - loss: 0.12807311117649078 - acc:  99.218750%\n","Training - running batch 146/176 of epoch 18/20 - loss: 0.07805995643138885 - acc:  97.656250%\n","Training - running batch 147/176 of epoch 18/20 - loss: 0.11501467227935791 - acc:  97.656250%\n","Training - running batch 148/176 of epoch 18/20 - loss: 0.08683941513299942 - acc:  96.875000%\n","Training - running batch 149/176 of epoch 18/20 - loss: 0.05205153301358223 - acc:  100.000000%\n","Training - running batch 150/176 of epoch 18/20 - loss: 0.13419681787490845 - acc:  95.312500%\n","Training - running batch 151/176 of epoch 18/20 - loss: 0.13693593442440033 - acc:  96.093750%\n","Training - running batch 152/176 of epoch 18/20 - loss: 0.11181319504976273 - acc:  96.875000%\n","Training - running batch 153/176 of epoch 18/20 - loss: 0.06321651488542557 - acc:  98.437500%\n","Training - running batch 154/176 of epoch 18/20 - loss: 0.14000336825847626 - acc:  98.437500%\n","Training - running batch 155/176 of epoch 18/20 - loss: 0.1520237773656845 - acc:  96.875000%\n","Training - running batch 156/176 of epoch 18/20 - loss: 0.03333776816725731 - acc:  100.000000%\n","Training - running batch 157/176 of epoch 18/20 - loss: 0.0614948496222496 - acc:  100.000000%\n","Training - running batch 158/176 of epoch 18/20 - loss: 0.26677021384239197 - acc:  93.750000%\n","Training - running batch 159/176 of epoch 18/20 - loss: 0.31913110613822937 - acc:  91.406250%\n","Training - running batch 160/176 of epoch 18/20 - loss: 0.05304830148816109 - acc:  100.000000%\n","Training - running batch 161/176 of epoch 18/20 - loss: 0.053622834384441376 - acc:  100.000000%\n","Training - running batch 162/176 of epoch 18/20 - loss: 0.2231566458940506 - acc:  96.093750%\n","Training - running batch 163/176 of epoch 18/20 - loss: 0.21650230884552002 - acc:  94.531250%\n","Training - running batch 164/176 of epoch 18/20 - loss: 0.15254220366477966 - acc:  96.875000%\n","Training - running batch 165/176 of epoch 18/20 - loss: 0.10423149913549423 - acc:  96.875000%\n","Training - running batch 166/176 of epoch 18/20 - loss: 0.06326042115688324 - acc:  99.218750%\n","Training - running batch 167/176 of epoch 18/20 - loss: 0.08636336773633957 - acc:  100.000000%\n","Training - running batch 168/176 of epoch 18/20 - loss: 0.0594961941242218 - acc:  100.000000%\n","Training - running batch 169/176 of epoch 18/20 - loss: 0.09116262942552567 - acc:  98.437500%\n","Training - running batch 170/176 of epoch 18/20 - loss: 0.06361813843250275 - acc:  100.000000%\n","Training - running batch 171/176 of epoch 18/20 - loss: 0.12234841287136078 - acc:  96.093750%\n","Training - running batch 172/176 of epoch 18/20 - loss: 0.07590308040380478 - acc:  100.000000%\n","Training - running batch 173/176 of epoch 18/20 - loss: 0.037564124912023544 - acc:  100.000000%\n","Training - running batch 174/176 of epoch 18/20 - loss: 0.11325164884328842 - acc:  98.437500%\n","Training - running batch 175/176 of epoch 18/20 - loss: 0.1198258027434349 - acc:  95.161290%\n","Testing - acc:  0.730549%\n","Training - running batch 0/176 of epoch 19/20 - loss: 0.12114749848842621 - acc:  96.875000%\n","Training - running batch 1/176 of epoch 19/20 - loss: 0.20388850569725037 - acc:  94.531250%\n","Training - running batch 2/176 of epoch 19/20 - loss: 0.11086477339267731 - acc:  96.875000%\n","Training - running batch 3/176 of epoch 19/20 - loss: 0.18833032250404358 - acc:  93.750000%\n","Training - running batch 4/176 of epoch 19/20 - loss: 0.055121179670095444 - acc:  100.000000%\n","Training - running batch 5/176 of epoch 19/20 - loss: 0.1605004221200943 - acc:  98.437500%\n","Training - running batch 6/176 of epoch 19/20 - loss: 0.04879404231905937 - acc:  100.000000%\n","Training - running batch 7/176 of epoch 19/20 - loss: 0.1371132731437683 - acc:  97.656250%\n","Training - running batch 8/176 of epoch 19/20 - loss: 0.08314100652933121 - acc:  98.437500%\n","Training - running batch 9/176 of epoch 19/20 - loss: 0.05565587431192398 - acc:  100.000000%\n","Training - running batch 10/176 of epoch 19/20 - loss: 0.14225812256336212 - acc:  96.875000%\n","Training - running batch 11/176 of epoch 19/20 - loss: 0.1931067556142807 - acc:  97.656250%\n","Training - running batch 12/176 of epoch 19/20 - loss: 0.08096423000097275 - acc:  98.437500%\n","Training - running batch 13/176 of epoch 19/20 - loss: 0.14568877220153809 - acc:  96.093750%\n","Training - running batch 14/176 of epoch 19/20 - loss: 0.2126932144165039 - acc:  96.875000%\n","Training - running batch 15/176 of epoch 19/20 - loss: 0.07072523236274719 - acc:  99.218750%\n","Training - running batch 16/176 of epoch 19/20 - loss: 0.05514366179704666 - acc:  98.437500%\n","Training - running batch 17/176 of epoch 19/20 - loss: 0.11369393020868301 - acc:  99.218750%\n","Training - running batch 18/176 of epoch 19/20 - loss: 0.0596291646361351 - acc:  97.656250%\n","Training - running batch 19/176 of epoch 19/20 - loss: 0.06989678740501404 - acc:  100.000000%\n","Training - running batch 20/176 of epoch 19/20 - loss: 0.15881295502185822 - acc:  96.875000%\n","Training - running batch 21/176 of epoch 19/20 - loss: 0.05458340048789978 - acc:  100.000000%\n","Training - running batch 22/176 of epoch 19/20 - loss: 0.05940189212560654 - acc:  100.000000%\n","Training - running batch 23/176 of epoch 19/20 - loss: 0.13769374787807465 - acc:  97.656250%\n","Training - running batch 24/176 of epoch 19/20 - loss: 0.06066292151808739 - acc:  100.000000%\n","Training - running batch 25/176 of epoch 19/20 - loss: 0.1183682307600975 - acc:  98.437500%\n","Training - running batch 26/176 of epoch 19/20 - loss: 0.11129414290189743 - acc:  98.437500%\n","Training - running batch 27/176 of epoch 19/20 - loss: 0.0866999551653862 - acc:  100.000000%\n","Training - running batch 28/176 of epoch 19/20 - loss: 0.14651453495025635 - acc:  96.875000%\n","Training - running batch 29/176 of epoch 19/20 - loss: 0.1882420778274536 - acc:  96.093750%\n","Training - running batch 30/176 of epoch 19/20 - loss: 0.06813166290521622 - acc:  96.875000%\n","Training - running batch 31/176 of epoch 19/20 - loss: 0.10903685539960861 - acc:  98.437500%\n","Training - running batch 32/176 of epoch 19/20 - loss: 0.15538448095321655 - acc:  96.875000%\n","Training - running batch 33/176 of epoch 19/20 - loss: 0.10653615742921829 - acc:  100.000000%\n","Training - running batch 34/176 of epoch 19/20 - loss: 0.16194526851177216 - acc:  96.875000%\n","Training - running batch 35/176 of epoch 19/20 - loss: 0.14688919484615326 - acc:  97.656250%\n","Training - running batch 36/176 of epoch 19/20 - loss: 0.07107160985469818 - acc:  100.000000%\n","Training - running batch 37/176 of epoch 19/20 - loss: 0.18519602715969086 - acc:  92.968750%\n","Training - running batch 38/176 of epoch 19/20 - loss: 0.08090273290872574 - acc:  99.218750%\n","Training - running batch 39/176 of epoch 19/20 - loss: 0.12090975046157837 - acc:  97.656250%\n","Training - running batch 40/176 of epoch 19/20 - loss: 0.12332530319690704 - acc:  96.875000%\n","Training - running batch 41/176 of epoch 19/20 - loss: 0.09166304767131805 - acc:  99.218750%\n","Training - running batch 42/176 of epoch 19/20 - loss: 0.10203089565038681 - acc:  97.656250%\n","Training - running batch 43/176 of epoch 19/20 - loss: 0.07383348047733307 - acc:  99.218750%\n","Training - running batch 44/176 of epoch 19/20 - loss: 0.04565658047795296 - acc:  100.000000%\n","Training - running batch 45/176 of epoch 19/20 - loss: 0.09267125278711319 - acc:  97.656250%\n","Training - running batch 46/176 of epoch 19/20 - loss: 0.19525204598903656 - acc:  96.875000%\n","Training - running batch 47/176 of epoch 19/20 - loss: 0.16315138339996338 - acc:  98.437500%\n","Training - running batch 48/176 of epoch 19/20 - loss: 0.13877160847187042 - acc:  96.875000%\n","Training - running batch 49/176 of epoch 19/20 - loss: 0.10863883048295975 - acc:  98.437500%\n","Training - running batch 50/176 of epoch 19/20 - loss: 0.05890529975295067 - acc:  100.000000%\n","Training - running batch 51/176 of epoch 19/20 - loss: 0.05853480473160744 - acc:  100.000000%\n","Training - running batch 52/176 of epoch 19/20 - loss: 0.0993841290473938 - acc:  98.437500%\n","Training - running batch 53/176 of epoch 19/20 - loss: 0.09055383503437042 - acc:  100.000000%\n","Training - running batch 54/176 of epoch 19/20 - loss: 0.11005305498838425 - acc:  96.875000%\n","Training - running batch 55/176 of epoch 19/20 - loss: 0.12029370665550232 - acc:  98.437500%\n","Training - running batch 56/176 of epoch 19/20 - loss: 0.20851746201515198 - acc:  96.875000%\n","Training - running batch 57/176 of epoch 19/20 - loss: 0.07904867082834244 - acc:  96.875000%\n","Training - running batch 58/176 of epoch 19/20 - loss: 0.13677740097045898 - acc:  96.875000%\n","Training - running batch 59/176 of epoch 19/20 - loss: 0.13879580795764923 - acc:  96.875000%\n","Training - running batch 60/176 of epoch 19/20 - loss: 0.07393347471952438 - acc:  98.437500%\n","Training - running batch 61/176 of epoch 19/20 - loss: 0.18236733973026276 - acc:  96.875000%\n","Training - running batch 62/176 of epoch 19/20 - loss: 0.0908941924571991 - acc:  97.656250%\n","Training - running batch 63/176 of epoch 19/20 - loss: 0.10721350461244583 - acc:  99.218750%\n","Training - running batch 64/176 of epoch 19/20 - loss: 0.13019660115242004 - acc:  95.312500%\n","Training - running batch 65/176 of epoch 19/20 - loss: 0.056187067180871964 - acc:  100.000000%\n","Training - running batch 66/176 of epoch 19/20 - loss: 0.06686441600322723 - acc:  99.218750%\n","Training - running batch 67/176 of epoch 19/20 - loss: 0.072838194668293 - acc:  100.000000%\n","Training - running batch 68/176 of epoch 19/20 - loss: 0.09455551207065582 - acc:  100.000000%\n","Training - running batch 69/176 of epoch 19/20 - loss: 0.1157367154955864 - acc:  98.437500%\n","Training - running batch 70/176 of epoch 19/20 - loss: 0.07609722763299942 - acc:  100.000000%\n","Training - running batch 71/176 of epoch 19/20 - loss: 0.049175675958395004 - acc:  100.000000%\n","Training - running batch 72/176 of epoch 19/20 - loss: 0.04968776926398277 - acc:  100.000000%\n","Training - running batch 73/176 of epoch 19/20 - loss: 0.13444459438323975 - acc:  98.437500%\n","Training - running batch 74/176 of epoch 19/20 - loss: 0.10837370902299881 - acc:  97.656250%\n","Training - running batch 75/176 of epoch 19/20 - loss: 0.04012418910861015 - acc:  100.000000%\n","Training - running batch 76/176 of epoch 19/20 - loss: 0.1274413913488388 - acc:  98.437500%\n","Training - running batch 77/176 of epoch 19/20 - loss: 0.14355716109275818 - acc:  98.437500%\n","Training - running batch 78/176 of epoch 19/20 - loss: 0.22816304862499237 - acc:  91.406250%\n","Training - running batch 79/176 of epoch 19/20 - loss: 0.07287987321615219 - acc:  98.437500%\n","Training - running batch 80/176 of epoch 19/20 - loss: 0.05822835490107536 - acc:  99.218750%\n","Training - running batch 81/176 of epoch 19/20 - loss: 0.08018087595701218 - acc:  100.000000%\n","Training - running batch 82/176 of epoch 19/20 - loss: 0.1679745465517044 - acc:  97.656250%\n","Training - running batch 83/176 of epoch 19/20 - loss: 0.08582902699708939 - acc:  96.875000%\n","Training - running batch 84/176 of epoch 19/20 - loss: 0.060278210788965225 - acc:  99.218750%\n","Training - running batch 85/176 of epoch 19/20 - loss: 0.0853637158870697 - acc:  98.437500%\n","Training - running batch 86/176 of epoch 19/20 - loss: 0.12019199132919312 - acc:  95.312500%\n","Training - running batch 87/176 of epoch 19/20 - loss: 0.1018909215927124 - acc:  99.218750%\n","Training - running batch 88/176 of epoch 19/20 - loss: 0.1131293922662735 - acc:  96.875000%\n","Training - running batch 89/176 of epoch 19/20 - loss: 0.0980679914355278 - acc:  100.000000%\n","Training - running batch 90/176 of epoch 19/20 - loss: 0.16768445074558258 - acc:  96.875000%\n","Training - running batch 91/176 of epoch 19/20 - loss: 0.09557180106639862 - acc:  97.656250%\n","Training - running batch 92/176 of epoch 19/20 - loss: 0.06703650951385498 - acc:  98.437500%\n","Training - running batch 93/176 of epoch 19/20 - loss: 0.1010776236653328 - acc:  99.218750%\n","Training - running batch 94/176 of epoch 19/20 - loss: 0.1659112125635147 - acc:  96.875000%\n","Training - running batch 95/176 of epoch 19/20 - loss: 0.08579273521900177 - acc:  100.000000%\n","Training - running batch 96/176 of epoch 19/20 - loss: 0.052810993045568466 - acc:  98.437500%\n","Training - running batch 97/176 of epoch 19/20 - loss: 0.08384283632040024 - acc:  98.437500%\n","Training - running batch 98/176 of epoch 19/20 - loss: 0.06820403784513474 - acc:  100.000000%\n","Training - running batch 99/176 of epoch 19/20 - loss: 0.0992128849029541 - acc:  99.218750%\n","Training - running batch 100/176 of epoch 19/20 - loss: 0.0709502324461937 - acc:  97.656250%\n","Training - running batch 101/176 of epoch 19/20 - loss: 0.049869269132614136 - acc:  100.000000%\n","Training - running batch 102/176 of epoch 19/20 - loss: 0.06479642540216446 - acc:  98.437500%\n","Training - running batch 103/176 of epoch 19/20 - loss: 0.0477544441819191 - acc:  100.000000%\n","Training - running batch 104/176 of epoch 19/20 - loss: 0.13027657568454742 - acc:  99.218750%\n","Training - running batch 105/176 of epoch 19/20 - loss: 0.09921342879533768 - acc:  100.000000%\n","Training - running batch 106/176 of epoch 19/20 - loss: 0.05373983830213547 - acc:  100.000000%\n","Training - running batch 107/176 of epoch 19/20 - loss: 0.0670212060213089 - acc:  100.000000%\n","Training - running batch 108/176 of epoch 19/20 - loss: 0.25599098205566406 - acc:  92.968750%\n","Training - running batch 109/176 of epoch 19/20 - loss: 0.36022597551345825 - acc:  92.968750%\n","Training - running batch 110/176 of epoch 19/20 - loss: 0.09428736567497253 - acc:  100.000000%\n","Training - running batch 111/176 of epoch 19/20 - loss: 0.04328000545501709 - acc:  100.000000%\n","Training - running batch 112/176 of epoch 19/20 - loss: 0.08274704217910767 - acc:  98.437500%\n","Training - running batch 113/176 of epoch 19/20 - loss: 0.19432666897773743 - acc:  95.312500%\n","Training - running batch 114/176 of epoch 19/20 - loss: 0.037481002509593964 - acc:  100.000000%\n","Training - running batch 115/176 of epoch 19/20 - loss: 0.15584048628807068 - acc:  95.312500%\n","Training - running batch 116/176 of epoch 19/20 - loss: 0.13077376782894135 - acc:  96.875000%\n","Training - running batch 117/176 of epoch 19/20 - loss: 0.14641225337982178 - acc:  98.437500%\n","Training - running batch 118/176 of epoch 19/20 - loss: 0.14955031871795654 - acc:  93.750000%\n","Training - running batch 119/176 of epoch 19/20 - loss: 0.1165294274687767 - acc:  98.437500%\n","Training - running batch 120/176 of epoch 19/20 - loss: 0.203231543302536 - acc:  95.312500%\n","Training - running batch 121/176 of epoch 19/20 - loss: 0.11534798890352249 - acc:  96.875000%\n","Training - running batch 122/176 of epoch 19/20 - loss: 0.08405642211437225 - acc:  98.437500%\n","Training - running batch 123/176 of epoch 19/20 - loss: 0.044841740280389786 - acc:  100.000000%\n","Training - running batch 124/176 of epoch 19/20 - loss: 0.09108653664588928 - acc:  100.000000%\n","Training - running batch 125/176 of epoch 19/20 - loss: 0.21457067131996155 - acc:  94.531250%\n","Training - running batch 126/176 of epoch 19/20 - loss: 0.11133097857236862 - acc:  100.000000%\n","Training - running batch 127/176 of epoch 19/20 - loss: 0.07549538463354111 - acc:  96.875000%\n","Training - running batch 128/176 of epoch 19/20 - loss: 0.0977732241153717 - acc:  97.656250%\n","Training - running batch 129/176 of epoch 19/20 - loss: 0.16299794614315033 - acc:  98.437500%\n","Training - running batch 130/176 of epoch 19/20 - loss: 0.14825847744941711 - acc:  98.437500%\n","Training - running batch 131/176 of epoch 19/20 - loss: 0.09070244431495667 - acc:  97.656250%\n","Training - running batch 132/176 of epoch 19/20 - loss: 0.094393290579319 - acc:  100.000000%\n","Training - running batch 133/176 of epoch 19/20 - loss: 0.2098332941532135 - acc:  96.875000%\n","Training - running batch 134/176 of epoch 19/20 - loss: 0.18963244557380676 - acc:  96.093750%\n","Training - running batch 135/176 of epoch 19/20 - loss: 0.1805470734834671 - acc:  93.750000%\n","Training - running batch 136/176 of epoch 19/20 - loss: 0.2087830752134323 - acc:  96.875000%\n","Training - running batch 137/176 of epoch 19/20 - loss: 0.1179472953081131 - acc:  96.875000%\n","Training - running batch 138/176 of epoch 19/20 - loss: 0.27942052483558655 - acc:  93.750000%\n","Training - running batch 139/176 of epoch 19/20 - loss: 0.07119452208280563 - acc:  100.000000%\n","Training - running batch 140/176 of epoch 19/20 - loss: 0.07719317078590393 - acc:  98.437500%\n","Training - running batch 141/176 of epoch 19/20 - loss: 0.13686715066432953 - acc:  96.875000%\n","Training - running batch 142/176 of epoch 19/20 - loss: 0.14015814661979675 - acc:  96.093750%\n","Training - running batch 143/176 of epoch 19/20 - loss: 0.06959646195173264 - acc:  100.000000%\n","Training - running batch 144/176 of epoch 19/20 - loss: 0.042557794600725174 - acc:  100.000000%\n","Training - running batch 145/176 of epoch 19/20 - loss: 0.1339135617017746 - acc:  98.437500%\n","Training - running batch 146/176 of epoch 19/20 - loss: 0.10064052790403366 - acc:  96.875000%\n","Training - running batch 147/176 of epoch 19/20 - loss: 0.07658585906028748 - acc:  98.437500%\n","Training - running batch 148/176 of epoch 19/20 - loss: 0.036414504051208496 - acc:  100.000000%\n","Training - running batch 149/176 of epoch 19/20 - loss: 0.13954214751720428 - acc:  97.656250%\n","Training - running batch 150/176 of epoch 19/20 - loss: 0.04976402595639229 - acc:  100.000000%\n","Training - running batch 151/176 of epoch 19/20 - loss: 0.1465531885623932 - acc:  96.875000%\n","Training - running batch 152/176 of epoch 19/20 - loss: 0.09324164688587189 - acc:  100.000000%\n","Training - running batch 153/176 of epoch 19/20 - loss: 0.0825929269194603 - acc:  99.218750%\n","Training - running batch 154/176 of epoch 19/20 - loss: 0.17152941226959229 - acc:  96.093750%\n","Training - running batch 155/176 of epoch 19/20 - loss: 0.0730658769607544 - acc:  100.000000%\n","Training - running batch 156/176 of epoch 19/20 - loss: 0.10810645669698715 - acc:  98.437500%\n","Training - running batch 157/176 of epoch 19/20 - loss: 0.0730331614613533 - acc:  98.437500%\n","Training - running batch 158/176 of epoch 19/20 - loss: 0.19697324931621552 - acc:  94.531250%\n","Training - running batch 159/176 of epoch 19/20 - loss: 0.1013759970664978 - acc:  98.437500%\n","Training - running batch 160/176 of epoch 19/20 - loss: 0.08073655515909195 - acc:  99.218750%\n","Training - running batch 161/176 of epoch 19/20 - loss: 0.10889751464128494 - acc:  99.218750%\n","Training - running batch 162/176 of epoch 19/20 - loss: 0.13475550711154938 - acc:  98.437500%\n","Training - running batch 163/176 of epoch 19/20 - loss: 0.13676197826862335 - acc:  97.656250%\n","Training - running batch 164/176 of epoch 19/20 - loss: 0.1386549025774002 - acc:  95.312500%\n","Training - running batch 165/176 of epoch 19/20 - loss: 0.1595587134361267 - acc:  98.437500%\n","Training - running batch 166/176 of epoch 19/20 - loss: 0.1140446588397026 - acc:  98.437500%\n","Training - running batch 167/176 of epoch 19/20 - loss: 0.07605520635843277 - acc:  100.000000%\n","Training - running batch 168/176 of epoch 19/20 - loss: 0.1386277973651886 - acc:  97.656250%\n","Training - running batch 169/176 of epoch 19/20 - loss: 0.17464759945869446 - acc:  95.312500%\n","Training - running batch 170/176 of epoch 19/20 - loss: 0.0723370835185051 - acc:  99.218750%\n","Training - running batch 171/176 of epoch 19/20 - loss: 0.05313597992062569 - acc:  99.218750%\n","Training - running batch 172/176 of epoch 19/20 - loss: 0.059675030410289764 - acc:  98.437500%\n","Training - running batch 173/176 of epoch 19/20 - loss: 0.07888032495975494 - acc:  98.437500%\n","Training - running batch 174/176 of epoch 19/20 - loss: 0.2339109182357788 - acc:  92.968750%\n","Training - running batch 175/176 of epoch 19/20 - loss: 0.310493141412735 - acc:  90.322581%\n","Testing - acc:  0.731467%\n","Best test 0.7328436970710754, corresponding test 0.7328436970710754 - best test: 0.7328436970710754, best epoch: 17\n","Model saved at ./model_domain_adaption_last.pth\n"]}],"source":["\n","\n","def get_model(model_name, pretrained=True, **kwargs):\n","    if model_name == 'resnet':\n","        model_class = resnet18\n","    elif model_name == 'alexnet':\n","        model_class = alex\n","    else:\n","        raise ValueError(f\"Unsupported model name: {model_name}\")\n","\n","    return model_class(pretrained=pretrained, **kwargs)\n","\n","#Accelerate the training\n","torch.backends.cudnn.benchmark = True\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","\n","\n","print(\"Target domain: {}\".format('Real world'))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize the model\n","n_classes = 65\n","model = get_model('resnet', pretrained=True, classes=n_classes)\n","model = model.to(device)\n","\n","# Data loaders\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),\n","    transforms.RandomGrayscale(0.1),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","val_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize(size=(224,224)),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","target_dataset = datasets.ImageFolder(\n","    f'./OfficeHomeDataset_10072016/Real World',\n","    transform=val_transform)\n","target_loader = torch.utils.data.DataLoader(target_dataset , batch_size=64, shuffle=True)\n","\n","concatenated_datasets = []\n","total_size = 0\n","\n","for name in [\"Art\",\"Clipart\",\"Product\"]:\n","    dataset = datasets.ImageFolder(\n","        f'./OfficeHomeDataset_10072016/{name}',\n","        transform=train_transform\n","    )\n","    total_size += len(dataset)\n","    concatenated_datasets.append(dataset)\n","\n","combined_dataset = ConcatDataset(concatenated_datasets)\n","indices = list(range(total_size))\n","random.shuffle(indices)\n","random_subset = Subset(combined_dataset, indices)\n","source_loader = DataLoader(random_subset, batch_size=64, shuffle=True)\n","\n","\n","\n","# Optimizer and scheduler\n","\n","params = model.parameters()\n","\n","optimizer = optim.SGD(params, weight_decay=.0005, momentum=.9, nesterov=False, lr=0.01)\n","#optimizer = optim.Adam(params, lr=lr)\n","step_size = int(20 * .8)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size)\n","\n","print(\"Step size: %d\" % step_size)\n","\n","# Training and testing loop\n","results = {\"test\": torch.zeros(20)}\n","criterion = nn.CrossEntropyLoss()\n","\n","writer = SummaryWriter('test')\n","global_step = 0\n","\n","for now_epoch in range(20):\n","    scheduler.step()\n","    lr = scheduler.get_lr()[0]\n","    writer.add_scalar('Learning Rate', lr, now_epoch)\n","\n","    # Training\n","    model.train()\n","\n","\n","    for it, batch in enumerate(source_loader):\n","        data, labels = batch\n","\n","        data, labels = data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","\n","        data_fliped = torch.flip(data, (3,)).detach().clone()\n","        data = torch.cat((data, data_fliped))\n","        labels_labels_flip = torch.cat((labels, labels))\n","\n","        class_logit = model(data, labels_labels_flip, True, now_epoch)\n","        class_loss = criterion(class_logit, labels_labels_flip)\n","        _, cls_pred = class_logit.max(dim=1)\n","        loss = class_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        class_loss_train = class_loss.item()\n","        accuracy = torch.sum(cls_pred == labels_labels_flip.data).item() / data.shape[0]\n","        writer.add_scalar('Loss/Train', class_loss_train, global_step)\n","        writer.add_scalar('Accuracy/Train', accuracy, global_step)\n","        global_step += 1\n","\n","        print(f\"Training - running batch {it}/{len(source_loader)} of epoch {now_epoch}/{20} - \"\n","                f\"loss: {class_loss_train} - acc: {accuracy * 100 : 2f}%\")\n","        del loss, class_loss, class_logit\n","\n","    # Testing\n","    model.eval()\n","    with torch.no_grad():\n","        class_correct = 0\n","        total = len(target_loader.dataset)\n","        for it, batch in enumerate(target_loader):\n","            data, labels = batch\n","\n","            data, labels = data.to(device), labels.to(device)\n","\n","            class_logit = model(data, labels, False)\n","            _, cls_pred = class_logit.max(dim=1)\n","\n","            class_correct += torch.sum(cls_pred == labels.data)\n","\n","        class_acc = float(class_correct) / total\n","        writer.add_scalar('Test/Accuracy', class_acc, now_epoch)\n","        print(f\"Testing - acc: {class_acc : 2f}%\")\n","        results['test'][now_epoch] = class_acc\n","\n","# Output results\n","test_res = results[\"test\"]\n","idx_best = test_res.argmax()\n","print(f\"Best test {test_res.max()}, corresponding test {test_res[idx_best]} - best test: {test_res.max()}, best epoch: {idx_best}\")\n","\n","# Save the model\n","model_path = './model_domain_adaption_last.pth'\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved at {model_path}\")"]},{"cell_type":"markdown","metadata":{"id":"HP2DYbiXXVru"},"source":["# With domain generalization PACS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdExPGdfXVru","outputId":"a2b49e01-dd7d-491c-e3b4-7846d0e5a791"},"outputs":[{"name":"stdout","output_type":"stream","text":["Target domain: sketch\n","Step size: 16\n"]},{"name":"stderr","output_type":"stream","text":["/home/xilanhua12138/anaconda3/envs/DomainAdaption/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","/home/xilanhua12138/anaconda3/envs/DomainAdaption/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","/home/xilanhua12138/anaconda3/envs/DomainAdaption/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Training - running batch 0/95 of epoch 0/20 - loss: 2.280043840408325 - acc:  5.468750%\n","Training - running batch 1/95 of epoch 0/20 - loss: 2.0619845390319824 - acc:  9.375000%\n","Training - running batch 2/95 of epoch 0/20 - loss: 2.008608102798462 - acc:  14.062500%\n","Training - running batch 3/95 of epoch 0/20 - loss: 1.894229531288147 - acc:  27.343750%\n","Training - running batch 4/95 of epoch 0/20 - loss: 3.0388033390045166 - acc:  7.812500%\n","Training - running batch 5/95 of epoch 0/20 - loss: 2.8647422790527344 - acc:  19.531250%\n","Training - running batch 6/95 of epoch 0/20 - loss: 1.616131067276001 - acc:  41.406250%\n","Training - running batch 7/95 of epoch 0/20 - loss: 2.3550338745117188 - acc:  35.156250%\n","Training - running batch 8/95 of epoch 0/20 - loss: 2.2441415786743164 - acc:  28.906250%\n","Training - running batch 9/95 of epoch 0/20 - loss: 2.136320114135742 - acc:  28.906250%\n","Training - running batch 10/95 of epoch 0/20 - loss: 2.051337718963623 - acc:  35.937500%\n","Training - running batch 11/95 of epoch 0/20 - loss: 0.988692045211792 - acc:  74.218750%\n","Training - running batch 12/95 of epoch 0/20 - loss: 1.9690569639205933 - acc:  40.625000%\n","Training - running batch 13/95 of epoch 0/20 - loss: 0.8501448035240173 - acc:  76.562500%\n","Training - running batch 14/95 of epoch 0/20 - loss: 0.9364045262336731 - acc:  73.437500%\n","Training - running batch 15/95 of epoch 0/20 - loss: 1.7108253240585327 - acc:  49.218750%\n","Training - running batch 16/95 of epoch 0/20 - loss: 0.9245408177375793 - acc:  73.437500%\n","Training - running batch 17/95 of epoch 0/20 - loss: 1.0256927013397217 - acc:  65.625000%\n","Training - running batch 18/95 of epoch 0/20 - loss: 1.7390271425247192 - acc:  54.687500%\n","Training - running batch 19/95 of epoch 0/20 - loss: 1.6299002170562744 - acc:  54.687500%\n","Training - running batch 20/95 of epoch 0/20 - loss: 0.7250503301620483 - acc:  78.906250%\n","Training - running batch 21/95 of epoch 0/20 - loss: 1.6580368280410767 - acc:  48.437500%\n","Training - running batch 22/95 of epoch 0/20 - loss: 1.400128722190857 - acc:  59.375000%\n","Training - running batch 23/95 of epoch 0/20 - loss: 1.7470176219940186 - acc:  46.875000%\n","Training - running batch 24/95 of epoch 0/20 - loss: 0.8571476340293884 - acc:  75.000000%\n","Training - running batch 25/95 of epoch 0/20 - loss: 0.7788065075874329 - acc:  77.343750%\n","Training - running batch 26/95 of epoch 0/20 - loss: 0.7882298827171326 - acc:  75.000000%\n","Training - running batch 27/95 of epoch 0/20 - loss: 1.6038728952407837 - acc:  50.000000%\n","Training - running batch 28/95 of epoch 0/20 - loss: 1.4196527004241943 - acc:  55.468750%\n","Training - running batch 29/95 of epoch 0/20 - loss: 1.3876134157180786 - acc:  53.125000%\n","Training - running batch 30/95 of epoch 0/20 - loss: 1.3200936317443848 - acc:  57.031250%\n","Training - running batch 31/95 of epoch 0/20 - loss: 0.8296984434127808 - acc:  76.562500%\n","Training - running batch 32/95 of epoch 0/20 - loss: 1.5580992698669434 - acc:  46.875000%\n","Training - running batch 33/95 of epoch 0/20 - loss: 1.2556248903274536 - acc:  53.906250%\n","Training - running batch 34/95 of epoch 0/20 - loss: 0.6301703453063965 - acc:  83.593750%\n","Training - running batch 35/95 of epoch 0/20 - loss: 1.0675487518310547 - acc:  53.906250%\n","Training - running batch 36/95 of epoch 0/20 - loss: 1.1900386810302734 - acc:  53.906250%\n","Training - running batch 37/95 of epoch 0/20 - loss: 0.6080334782600403 - acc:  78.906250%\n","Training - running batch 38/95 of epoch 0/20 - loss: 1.1724542379379272 - acc:  50.000000%\n","Training - running batch 39/95 of epoch 0/20 - loss: 0.7393494248390198 - acc:  78.125000%\n","Training - running batch 40/95 of epoch 0/20 - loss: 1.0378239154815674 - acc:  57.031250%\n","Training - running batch 41/95 of epoch 0/20 - loss: 0.5960358381271362 - acc:  80.468750%\n","Training - running batch 42/95 of epoch 0/20 - loss: 0.48795121908187866 - acc:  86.718750%\n","Training - running batch 43/95 of epoch 0/20 - loss: 1.5842790603637695 - acc:  41.406250%\n","Training - running batch 44/95 of epoch 0/20 - loss: 0.9871580004692078 - acc:  60.156250%\n","Training - running batch 45/95 of epoch 0/20 - loss: 1.039250373840332 - acc:  59.375000%\n","Training - running batch 46/95 of epoch 0/20 - loss: 0.8609206676483154 - acc:  62.500000%\n","Training - running batch 47/95 of epoch 0/20 - loss: 0.5402334332466125 - acc:  85.937500%\n","Training - running batch 48/95 of epoch 0/20 - loss: 0.44970881938934326 - acc:  92.187500%\n","Training - running batch 49/95 of epoch 0/20 - loss: 0.9961987137794495 - acc:  59.375000%\n","Training - running batch 50/95 of epoch 0/20 - loss: 0.6316982507705688 - acc:  79.687500%\n","Training - running batch 51/95 of epoch 0/20 - loss: 0.6388779878616333 - acc:  81.250000%\n","Training - running batch 52/95 of epoch 0/20 - loss: 0.7689321637153625 - acc:  70.312500%\n","Training - running batch 53/95 of epoch 0/20 - loss: 0.7900875210762024 - acc:  63.281250%\n","Training - running batch 54/95 of epoch 0/20 - loss: 1.0481075048446655 - acc:  57.031250%\n","Training - running batch 55/95 of epoch 0/20 - loss: 0.7788395285606384 - acc:  66.406250%\n","Training - running batch 56/95 of epoch 0/20 - loss: 0.902098536491394 - acc:  62.500000%\n","Training - running batch 57/95 of epoch 0/20 - loss: 0.7360372543334961 - acc:  69.531250%\n","Training - running batch 58/95 of epoch 0/20 - loss: 0.6961537003517151 - acc:  75.781250%\n","Training - running batch 59/95 of epoch 0/20 - loss: 0.6263980269432068 - acc:  81.250000%\n","Training - running batch 60/95 of epoch 0/20 - loss: 0.5248319506645203 - acc:  88.281250%\n","Training - running batch 61/95 of epoch 0/20 - loss: 0.5607704520225525 - acc:  85.937500%\n","Training - running batch 62/95 of epoch 0/20 - loss: 0.7463222742080688 - acc:  66.406250%\n","Training - running batch 63/95 of epoch 0/20 - loss: 0.692233145236969 - acc:  81.250000%\n","Training - running batch 64/95 of epoch 0/20 - loss: 0.8360273838043213 - acc:  63.281250%\n","Training - running batch 65/95 of epoch 0/20 - loss: 0.6299040913581848 - acc:  82.812500%\n","Training - running batch 66/95 of epoch 0/20 - loss: 0.7499336004257202 - acc:  69.531250%\n","Training - running batch 67/95 of epoch 0/20 - loss: 0.6046653985977173 - acc:  81.250000%\n","Training - running batch 68/95 of epoch 0/20 - loss: 0.6693394184112549 - acc:  81.250000%\n","Training - running batch 69/95 of epoch 0/20 - loss: 0.7525383830070496 - acc:  71.093750%\n","Training - running batch 70/95 of epoch 0/20 - loss: 0.6671806573867798 - acc:  79.687500%\n","Training - running batch 71/95 of epoch 0/20 - loss: 0.7601718306541443 - acc:  72.656250%\n","Training - running batch 72/95 of epoch 0/20 - loss: 0.7813723087310791 - acc:  64.843750%\n","Training - running batch 73/95 of epoch 0/20 - loss: 1.0028961896896362 - acc:  59.375000%\n","Training - running batch 74/95 of epoch 0/20 - loss: 0.673939049243927 - acc:  68.750000%\n","Training - running batch 75/95 of epoch 0/20 - loss: 0.6103494167327881 - acc:  85.156250%\n","Training - running batch 76/95 of epoch 0/20 - loss: 0.7208418846130371 - acc:  69.531250%\n","Training - running batch 77/95 of epoch 0/20 - loss: 0.8141303658485413 - acc:  65.625000%\n","Training - running batch 78/95 of epoch 0/20 - loss: 0.6063971519470215 - acc:  75.000000%\n","Training - running batch 79/95 of epoch 0/20 - loss: 0.7049683332443237 - acc:  71.875000%\n","Training - running batch 80/95 of epoch 0/20 - loss: 0.6297513246536255 - acc:  71.875000%\n","Training - running batch 81/95 of epoch 0/20 - loss: 0.5269239544868469 - acc:  82.812500%\n","Training - running batch 82/95 of epoch 0/20 - loss: 0.8043423891067505 - acc:  64.062500%\n","Training - running batch 83/95 of epoch 0/20 - loss: 0.3851180672645569 - acc:  94.531250%\n","Training - running batch 84/95 of epoch 0/20 - loss: 0.5917381048202515 - acc:  75.781250%\n","Training - running batch 85/95 of epoch 0/20 - loss: 0.46438834071159363 - acc:  88.281250%\n","Training - running batch 86/95 of epoch 0/20 - loss: 0.5556471347808838 - acc:  75.781250%\n","Training - running batch 87/95 of epoch 0/20 - loss: 0.5975641012191772 - acc:  73.437500%\n","Training - running batch 88/95 of epoch 0/20 - loss: 0.8875210285186768 - acc:  66.406250%\n","Training - running batch 89/95 of epoch 0/20 - loss: 0.5280979871749878 - acc:  85.156250%\n","Training - running batch 90/95 of epoch 0/20 - loss: 0.8974804282188416 - acc:  80.468750%\n","Training - running batch 91/95 of epoch 0/20 - loss: 0.46979543566703796 - acc:  83.593750%\n","Training - running batch 92/95 of epoch 0/20 - loss: 0.7317714691162109 - acc:  67.187500%\n","Training - running batch 93/95 of epoch 0/20 - loss: 0.39766937494277954 - acc:  91.406250%\n","Training - running batch 94/95 of epoch 0/20 - loss: 0.6259357333183289 - acc:  73.913043%\n","Testing - acc:  0.732756%\n","Training - running batch 0/95 of epoch 1/20 - loss: 0.6237539052963257 - acc:  76.562500%\n","Training - running batch 1/95 of epoch 1/20 - loss: 0.6120739579200745 - acc:  76.562500%\n","Training - running batch 2/95 of epoch 1/20 - loss: 0.5908747315406799 - acc:  82.031250%\n","Training - running batch 3/95 of epoch 1/20 - loss: 0.5794872045516968 - acc:  71.093750%\n","Training - running batch 4/95 of epoch 1/20 - loss: 0.5073663592338562 - acc:  80.468750%\n","Training - running batch 5/95 of epoch 1/20 - loss: 0.625634491443634 - acc:  73.437500%\n","Training - running batch 6/95 of epoch 1/20 - loss: 0.5795466303825378 - acc:  83.593750%\n","Training - running batch 7/95 of epoch 1/20 - loss: 0.44676029682159424 - acc:  79.687500%\n","Training - running batch 8/95 of epoch 1/20 - loss: 0.4363793730735779 - acc:  89.843750%\n","Training - running batch 9/95 of epoch 1/20 - loss: 0.8029618263244629 - acc:  60.937500%\n","Training - running batch 10/95 of epoch 1/20 - loss: 0.6220825910568237 - acc:  79.687500%\n","Training - running batch 11/95 of epoch 1/20 - loss: 0.6907685995101929 - acc:  74.218750%\n","Training - running batch 12/95 of epoch 1/20 - loss: 0.7988958358764648 - acc:  68.750000%\n","Training - running batch 13/95 of epoch 1/20 - loss: 0.4476032853126526 - acc:  83.593750%\n","Training - running batch 14/95 of epoch 1/20 - loss: 0.4367797374725342 - acc:  89.062500%\n","Training - running batch 15/95 of epoch 1/20 - loss: 0.5610376596450806 - acc:  81.250000%\n","Training - running batch 16/95 of epoch 1/20 - loss: 0.5005300045013428 - acc:  87.500000%\n","Training - running batch 17/95 of epoch 1/20 - loss: 0.6269869208335876 - acc:  77.343750%\n","Training - running batch 18/95 of epoch 1/20 - loss: 0.4196142256259918 - acc:  85.156250%\n","Training - running batch 19/95 of epoch 1/20 - loss: 0.4235251247882843 - acc:  89.062500%\n","Training - running batch 20/95 of epoch 1/20 - loss: 0.5143386125564575 - acc:  76.562500%\n","Training - running batch 21/95 of epoch 1/20 - loss: 0.4885488450527191 - acc:  74.218750%\n","Training - running batch 22/95 of epoch 1/20 - loss: 0.44398826360702515 - acc:  89.062500%\n","Training - running batch 23/95 of epoch 1/20 - loss: 0.49182042479515076 - acc:  78.125000%\n","Training - running batch 24/95 of epoch 1/20 - loss: 0.6437323093414307 - acc:  66.406250%\n","Training - running batch 25/95 of epoch 1/20 - loss: 0.4515177011489868 - acc:  84.375000%\n","Training - running batch 26/95 of epoch 1/20 - loss: 0.5740827322006226 - acc:  77.343750%\n","Training - running batch 27/95 of epoch 1/20 - loss: 0.45752519369125366 - acc:  90.625000%\n","Training - running batch 28/95 of epoch 1/20 - loss: 0.4410931169986725 - acc:  90.625000%\n","Training - running batch 29/95 of epoch 1/20 - loss: 0.38894015550613403 - acc:  89.843750%\n","Training - running batch 30/95 of epoch 1/20 - loss: 0.7535160183906555 - acc:  70.312500%\n","Training - running batch 31/95 of epoch 1/20 - loss: 0.5975921154022217 - acc:  75.000000%\n","Training - running batch 32/95 of epoch 1/20 - loss: 0.4637715220451355 - acc:  86.718750%\n","Training - running batch 33/95 of epoch 1/20 - loss: 0.3352273106575012 - acc:  95.312500%\n","Training - running batch 34/95 of epoch 1/20 - loss: 0.5337424874305725 - acc:  83.593750%\n","Training - running batch 35/95 of epoch 1/20 - loss: 0.4900446832180023 - acc:  83.593750%\n","Training - running batch 36/95 of epoch 1/20 - loss: 0.45944643020629883 - acc:  87.500000%\n","Training - running batch 37/95 of epoch 1/20 - loss: 0.5890140533447266 - acc:  82.031250%\n","Training - running batch 38/95 of epoch 1/20 - loss: 0.7405320405960083 - acc:  67.968750%\n","Training - running batch 39/95 of epoch 1/20 - loss: 0.6930623650550842 - acc:  82.031250%\n","Training - running batch 40/95 of epoch 1/20 - loss: 0.4290444254875183 - acc:  86.718750%\n","Training - running batch 41/95 of epoch 1/20 - loss: 0.667776882648468 - acc:  74.218750%\n","Training - running batch 42/95 of epoch 1/20 - loss: 0.41974732279777527 - acc:  89.062500%\n","Training - running batch 43/95 of epoch 1/20 - loss: 0.528732419013977 - acc:  75.781250%\n","Training - running batch 44/95 of epoch 1/20 - loss: 0.4138285517692566 - acc:  89.843750%\n","Training - running batch 45/95 of epoch 1/20 - loss: 0.3944922089576721 - acc:  84.375000%\n","Training - running batch 46/95 of epoch 1/20 - loss: 0.615824818611145 - acc:  74.218750%\n","Training - running batch 47/95 of epoch 1/20 - loss: 0.4255189299583435 - acc:  89.062500%\n","Training - running batch 48/95 of epoch 1/20 - loss: 0.5648320317268372 - acc:  80.468750%\n","Training - running batch 49/95 of epoch 1/20 - loss: 0.5359593629837036 - acc:  85.156250%\n","Training - running batch 50/95 of epoch 1/20 - loss: 0.4470409154891968 - acc:  82.031250%\n","Training - running batch 51/95 of epoch 1/20 - loss: 0.4875071942806244 - acc:  78.125000%\n","Training - running batch 52/95 of epoch 1/20 - loss: 0.3724029064178467 - acc:  82.031250%\n","Training - running batch 53/95 of epoch 1/20 - loss: 0.3985269367694855 - acc:  85.937500%\n","Training - running batch 54/95 of epoch 1/20 - loss: 0.37870699167251587 - acc:  89.062500%\n","Training - running batch 55/95 of epoch 1/20 - loss: 0.37349897623062134 - acc:  89.843750%\n","Training - running batch 56/95 of epoch 1/20 - loss: 0.34965822100639343 - acc:  92.968750%\n","Training - running batch 57/95 of epoch 1/20 - loss: 0.4270397424697876 - acc:  80.468750%\n","Training - running batch 58/95 of epoch 1/20 - loss: 0.659710705280304 - acc:  79.687500%\n","Training - running batch 59/95 of epoch 1/20 - loss: 0.2792326807975769 - acc:  94.531250%\n","Training - running batch 60/95 of epoch 1/20 - loss: 0.456807404756546 - acc:  78.125000%\n","Training - running batch 61/95 of epoch 1/20 - loss: 0.4590917229652405 - acc:  79.687500%\n","Training - running batch 62/95 of epoch 1/20 - loss: 0.3389507830142975 - acc:  91.406250%\n","Training - running batch 63/95 of epoch 1/20 - loss: 0.42183029651641846 - acc:  85.937500%\n","Training - running batch 64/95 of epoch 1/20 - loss: 0.39633652567863464 - acc:  86.718750%\n"]}],"source":["\n","\n","def get_model(model_name, pretrained=True, **kwargs):\n","    if model_name == 'resnet':\n","        model_class = resnet18\n","    elif model_name == 'alexnet':\n","        model_class = alex\n","    else:\n","        raise ValueError(f\"Unsupported model name: {model_name}\")\n","\n","    return model_class(pretrained=pretrained, **kwargs)\n","\n","#Accelerate the training\n","torch.backends.cudnn.benchmark = True\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","\n","\n","print(\"Target domain: {}\".format('sketch'))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize the model\n","n_classes = 7\n","model = get_model('resnet', pretrained=True, classes=n_classes)\n","model = model.to(device)\n","\n","# Data loaders\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),\n","    transforms.RandomGrayscale(0.1),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","val_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize(size=(224,224)),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","target_dataset = datasets.ImageFolder(\n","    f'./Domain_Generalization/PACS/kfold/sketch',\n","    transform=val_transform)\n","target_loader = torch.utils.data.DataLoader(target_dataset , batch_size=64, shuffle=True)\n","\n","concatenated_datasets = []\n","total_size = 0\n","\n","for name in ['art_painting', 'cartoon', 'photo']:\n","    dataset = datasets.ImageFolder(\n","        f'./Domain_Generalization/PACS/kfold/{name}',\n","        transform=train_transform\n","    )\n","    total_size += len(dataset)\n","    concatenated_datasets.append(dataset)\n","\n","combined_dataset = ConcatDataset(concatenated_datasets)\n","indices = list(range(total_size))\n","random.shuffle(indices)\n","random_subset = Subset(combined_dataset, indices)\n","source_loader = DataLoader(random_subset, batch_size=64, shuffle=True)\n","\n","\n","\n","# Optimizer and scheduler\n","\n","params = model.parameters()\n","\n","optimizer = optim.SGD(params, weight_decay=.0005, momentum=.9, nesterov=False, lr=0.01)\n","#optimizer = optim.Adam(params, lr=lr)\n","step_size = int(20 * .8)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size)\n","\n","print(\"Step size: %d\" % step_size)\n","\n","# Training and testing loop\n","results = {\"test\": torch.zeros(20)}\n","criterion = nn.CrossEntropyLoss()\n","\n","writer = SummaryWriter('test_PACS')\n","global_step = 0\n","\n","for now_epoch in range(20):\n","    scheduler.step()\n","    lr = scheduler.get_lr()[0]\n","    writer.add_scalar('Learning Rate', lr, now_epoch)\n","\n","    # Training\n","    model.train()\n","\n","\n","    for it, batch in enumerate(source_loader):\n","        data, labels = batch\n","\n","        data, labels = data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","\n","        data_fliped = torch.flip(data, (3,)).detach().clone()\n","        data = torch.cat((data, data_fliped))\n","        labels_labels_flip = torch.cat((labels, labels))\n","\n","        class_logit = model(data, labels_labels_flip, True, now_epoch)\n","        class_loss = criterion(class_logit, labels_labels_flip)\n","        _, cls_pred = class_logit.max(dim=1)\n","        loss = class_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        class_loss_train = class_loss.item()\n","        accuracy = torch.sum(cls_pred == labels_labels_flip.data).item() / data.shape[0]\n","        writer.add_scalar('Loss/Train', class_loss_train, global_step)\n","        writer.add_scalar('Accuracy/Train', accuracy, global_step)\n","        global_step += 1\n","\n","        print(f\"Training - running batch {it}/{len(source_loader)} of epoch {now_epoch}/{20} - \"\n","                f\"loss: {class_loss_train} - acc: {accuracy * 100 : 2f}%\")\n","        del loss, class_loss, class_logit\n","\n","    # Testing\n","    model.eval()\n","    with torch.no_grad():\n","        class_correct = 0\n","        total = len(target_loader.dataset)\n","        for it, batch in enumerate(target_loader):\n","            data, labels = batch\n","\n","            data, labels = data.to(device), labels.to(device)\n","\n","            class_logit = model(data, labels, False)\n","            _, cls_pred = class_logit.max(dim=1)\n","\n","            class_correct += torch.sum(cls_pred == labels.data)\n","\n","        class_acc = float(class_correct) / total\n","        writer.add_scalar('Test/Accuracy', class_acc, now_epoch)\n","        print(f\"Testing - acc: {class_acc : 2f}%\")\n","        results['test'][now_epoch] = class_acc\n","\n","# Output results\n","test_res = results[\"test\"]\n","idx_best = test_res.argmax()\n","print(f\"Best test {test_res.max()}, corresponding test {test_res[idx_best]} - best test: {test_res.max()}, best epoch: {idx_best}\")\n","\n","# Save the model\n","model_path = './model_PACS_domain_adaption_last.pth'\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved at {model_path}\")"]},{"cell_type":"markdown","metadata":{"id":"1YbYZLbQXVrv"},"source":["# With domain generalization 16 class imagenet dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-i2XmPCoXVrv"},"outputs":[],"source":["\n","\n","def get_model(model_name, pretrained=True, **kwargs):\n","    if model_name == 'resnet':\n","        model_class = resnet18\n","    elif model_name == 'alexnet':\n","        model_class = alex\n","    else:\n","        raise ValueError(f\"Unsupported model name: {model_name}\")\n","\n","    return model_class(pretrained=pretrained, **kwargs)\n","\n","#Accelerate the training\n","torch.backends.cudnn.benchmark = True\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","\n","print(\"Target domain: {}\".format('edge'))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize the model\n","n_classes = 7\n","model = get_model('resnet', pretrained=True, classes=n_classes)\n","model = model.to(device)\n","\n","# Data loaders\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),\n","    transforms.RandomGrayscale(0.1),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","val_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize(size=(224,224)),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","target_dataset = datasets.ImageFolder(\n","    f'./Domain_Generalization16_class_imagenet_datasets/edge',\n","    transform=val_transform)\n","target_loader = torch.utils.data.DataLoader(target_dataset , batch_size=64, shuffle=True)\n","\n","concatenated_datasets = []\n","total_size = 0\n","\n","for name in ['silhouette', 'cue-conflict']:\n","    dataset = datasets.ImageFolder(\n","        f'./Domain_Generalization/16_class_imagenet_datasets/{name}',\n","        transform=train_transform\n","    )\n","    total_size += len(dataset)\n","    concatenated_datasets.append(dataset)\n","\n","combined_dataset = ConcatDataset(concatenated_datasets)\n","indices = list(range(total_size))\n","random.shuffle(indices)\n","random_subset = Subset(combined_dataset, indices)\n","source_loader = DataLoader(random_subset, batch_size=64, shuffle=True)\n","\n","\n","\n","# Optimizer and scheduler\n","\n","params = model.parameters()\n","\n","optimizer = optim.SGD(params, weight_decay=.0005, momentum=.9, nesterov=False, lr=0.01)\n","#optimizer = optim.Adam(params, lr=lr)\n","step_size = int(20 * .8)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size)\n","\n","print(\"Step size: %d\" % step_size)\n","\n","# Training and testing loop\n","results = {\"test\": torch.zeros(20)}\n","criterion = nn.CrossEntropyLoss()\n","\n","writer = SummaryWriter('test_PACS')\n","global_step = 0\n","\n","for now_epoch in range(20):\n","    scheduler.step()\n","    lr = scheduler.get_lr()[0]\n","    writer.add_scalar('Learning Rate', lr, now_epoch)\n","\n","    # Training\n","    model.train()\n","\n","\n","    for it, batch in enumerate(source_loader):\n","        data, labels = batch\n","\n","        data, labels = data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","\n","        data_fliped = torch.flip(data, (3,)).detach().clone()\n","        data = torch.cat((data, data_fliped))\n","        labels_labels_flip = torch.cat((labels, labels))\n","\n","        class_logit = model(data, labels_labels_flip, True, now_epoch)\n","        class_loss = criterion(class_logit, labels_labels_flip)\n","        _, cls_pred = class_logit.max(dim=1)\n","        loss = class_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        class_loss_train = class_loss.item()\n","        accuracy = torch.sum(cls_pred == labels_labels_flip.data).item() / data.shape[0]\n","        writer.add_scalar('Loss/Train', class_loss_train, global_step)\n","        writer.add_scalar('Accuracy/Train', accuracy, global_step)\n","        global_step += 1\n","\n","        print(f\"Training - running batch {it}/{len(source_loader)} of epoch {now_epoch}/{20} - \"\n","                f\"loss: {class_loss_train} - acc: {accuracy * 100 : 2f}%\")\n","        del loss, class_loss, class_logit\n","\n","    # Testing\n","    model.eval()\n","    with torch.no_grad():\n","        class_correct = 0\n","        total = len(target_loader.dataset)\n","        for it, batch in enumerate(target_loader):\n","            data, labels = batch\n","\n","            data, labels = data.to(device), labels.to(device)\n","\n","            class_logit = model(data, labels, False)\n","            _, cls_pred = class_logit.max(dim=1)\n","\n","            class_correct += torch.sum(cls_pred == labels.data)\n","\n","        class_acc = float(class_correct) / total\n","        writer.add_scalar('Test/Accuracy', class_acc, now_epoch)\n","        print(f\"Testing - acc: {class_acc : 2f}%\")\n","        results['test'][now_epoch] = class_acc\n","\n","# Output results\n","test_res = results[\"test\"]\n","idx_best = test_res.argmax()\n","print(f\"Best test {test_res.max()}, corresponding test {test_res[idx_best]} - best test: {test_res.max()}, best epoch: {idx_best}\")\n","\n","# Save the model\n","model_path = './model_PACS_domain_adaption_last.pth'\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved at {model_path}\")"]}],"metadata":{"kernelspec":{"display_name":"DomainAdaption","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}